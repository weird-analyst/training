{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utvRlOqdczIA"
   },
   "source": [
    "# Region Superresolution Code\n",
    "\n",
    "This python notebook runs the complete process for a dynamic updation starting from abundance population, and graph finetuning.\n",
    "\n",
    "Code Flow:\n",
    "- Input Parameters: input_file.csv\n",
    "- Output: subregion_i_j.csv containing all 2km x 2km elemental abundances for that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hjw9WClocq8m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gdown rasterio shapely torch_geometric gdown torch_optimizer -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YayY4877cztS",
    "outputId": "1bcd0d6a-3c72-4615-955f-71e368befae2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import rasterio\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import threading\n",
    "import pickle\n",
    "import threading\n",
    "import csv\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch_optimizer as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from scipy.spatial.distance import cdist\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def current_time():\n",
    "    return datetime.datetime.now().strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UEm5v4I5ESqe"
   },
   "outputs": [],
   "source": [
    "# from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L47Yku5Pfngk",
    "outputId": "f346a909-2993-4dea-c2df-997d7eb25280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:11:18 Imports done. Using cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f\"[INFO] {current_time()} Imports done. Using {device}\")\n",
    "os.makedirs('regions', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkuarsfebmcH",
    "outputId": "71e704f8-86e0-4805-ecdd-e618d02b50e5"
   },
   "outputs": [],
   "source": [
    "# # All mare files\n",
    "# !gdown 1Ig5nXZqwscWYRLWgD22a76N5AsIDF2cv\n",
    "# !gdown 1RgGrRBntdAb6aMf7mvAgD23WunY2uqv8\n",
    "# !gdown 1LqYxaY08nyZqGlK0Udd28MzekwzwLsCz\n",
    "# # !gdown 1794ulttX9D46Onumwvg6ToBJCGd9M05L\n",
    "# !gdown 15DUqZp716VV60jBM8V0CCg2oQdlBXlDU\n",
    "# !gdown 1Ltm2PjQvXCQ-NiJjARWrJGsr74AxWv3g\n",
    "# !gdown 1agqOartoVDRJ65yG_gxmtF6Z6oyk7Xd4\n",
    "# !gdown 1ul-A8zHvUUOl62F0fzDqm8wL49GNuDiL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dv4BQqWXfgN8",
    "outputId": "60184cf2-fdaf-4a79-e769-4414eafeddfa"
   },
   "outputs": [],
   "source": [
    "# # All region files\n",
    "# !gdown --folder --output ./regions/ 1O9Lhg54pWAxzRc4OuITKIKOVlftkHEFV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPTpd5Xih5wN",
    "outputId": "e332f641-1bb0-457f-80f4-e89b51e55a5c"
   },
   "outputs": [],
   "source": [
    "# !gdown --folder --output ./regions/ 1Vea75zfr8pns5SnMf3Uxseqsd822KqnU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pb_I9Dlsh5n1",
    "outputId": "e0a4af62-1fee-4fc2-d6ec-504dd8741264"
   },
   "outputs": [],
   "source": [
    "# !gdown --folder --output ./regions/ 1G4hAxJ_cvsEmBWBreA2b8t69L42NyFIm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAnPCn_bh5hO",
    "outputId": "3229d282-54c7-4137-e55c-e01020c42a3a"
   },
   "outputs": [],
   "source": [
    "# !gdown --folder --output ./regions/ 1shlmJDzid368w8XvAW8Q1in49SHhenDH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udIS3wBb6yZu",
    "outputId": "f6805030-4642-46cf-e4eb-3cf80bee4870"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Download completed\n",
      "Retrieving folder contents\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "# Masks Folder\n",
    "os.makedirs('drive/MyDrive/ISRO_SuperResolution', exist_ok=True)\n",
    "os.makedirs('drive/MyDrive/ISRO_SuperResolution/masks', exist_ok=True)\n",
    "!gdown --folder --output ./drive/MyDrive/ISRO_SuperResolution/ 1WJAinvVegAQdi7QoMK9us37QPF7otWSj\n",
    "\n",
    "# Models Folder\n",
    "os.makedirs('drive/MyDrive/ISRO_SuperResolution/models', exist_ok=True)\n",
    "!gdown --folder --output ./drive/MyDrive/ISRO_SuperResolution/ 10EpU9D8L_IECDYLn3GrEtCjV1Htjxevb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jthvq-KufBcN"
   },
   "source": [
    "# Part 1: Some functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4GUbC-kRe-GL"
   },
   "outputs": [],
   "source": [
    "MOON_RADIUS_KM = 1737.4 # Approximate radius of Moon in kilometers\n",
    "\n",
    "# Lat Long to Pixel --> Converts latitude longitude to pixel coordinate in the image array\n",
    "def lat_long_to_pixel(lat, lon, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Converts latitude and longitude to pixel coordinates in the image.\n",
    "    Assumes the image is georeferenced from (-180W, 90N) to (180E, -90S).\n",
    "    \"\"\"\n",
    "    x = min(int((lon + 180) / 360 * img_width), img_width - 125)\n",
    "    y = min(int(((90 - lat) / 180) * img_height), img_height - 125)\n",
    "    return x, y\n",
    "\n",
    "# Function to calculate degrees from kilometers at the equator\n",
    "def km_to_degrees(km):\n",
    "    return km / (2 * np.pi * MOON_RADIUS_KM) * 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJ0Xk3VACXH2",
    "outputId": "d040392d-fe32-4246-d86f-9e8002a676b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 21:24:43 Done importing geodataframe for highland-mare classification\n"
     ]
    }
   ],
   "source": [
    "# Load the Mare Areas Shapefile\n",
    "shapefile_path = './LROC_GLOBAL_MARE_180.SHP'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf['region_type'] = gdf['MARE_NAME'].apply(lambda x: 1 if pd.notnull(x) else 2)\n",
    "gdf.sindex  # Creates a spatial index if not already present\n",
    "print(f\"[INFO] {current_time()} Done importing geodataframe for highland-mare classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vM0-oYpze-Lj"
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "LATITUDE_RANGE = (90, -90)\n",
    "LONGITUDE_RANGE = (-180, 180)\n",
    "NUM_SUBREGIONS = 64\n",
    "SUBREGIONS_PER_ROW = 8  # Assume the regions are divided into 8x8 grid\n",
    "SQUARE_SIZE_KM = 2      # Each square's size in kilometers\n",
    "# Calculate the size of each subregion in degrees in terms of equator\n",
    "LAT_PER_REGION = ((LATITUDE_RANGE[1] - LATITUDE_RANGE[0]) / SUBREGIONS_PER_ROW)\n",
    "LON_PER_REGION = ((LONGITUDE_RANGE[1] - LONGITUDE_RANGE[0]) / SUBREGIONS_PER_ROW)\n",
    "square_size_deg = km_to_degrees(SQUARE_SIZE_KM)\n",
    "num_squares_lat = abs(int(LAT_PER_REGION // square_size_deg))\n",
    "num_squares_lon = abs(int(LON_PER_REGION // square_size_deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4J8TJop5e-Qr"
   },
   "outputs": [],
   "source": [
    "elements = ['Fe', 'Ti', 'Ca', 'Si', 'Al', 'Mg', 'Na', 'O']\n",
    "mareOrHighland = ['mareOrHighland']\n",
    "topFeatures = [0, 1846, 1808, 1813, 1146, 1378, 923, 1237, 1558, 37, 1574, 1117, 103, 505, 550, 1734, 1785,\n",
    "               881, 1030, 1820, 1978, 792, 1323, 51, 1714, 691, 978, 1746, 1499, 1183, 1160, 1288, 371, 985,\n",
    "               34, 1696, 1101, 469, 1406, 133, 703, 1679, 258, 857, 1245, 914, 184, 157, 1988, 1641, 947,\n",
    "               1847, 1953, 2007, 787, 129, 793, 188, 163, 1262, 800, 1131, 1390, 66, 700, 590, 662, 916,\n",
    "               1538, 1673, 995, 1424, 139, 652, 959, 1869, 228, 1293, 1105, 1457, 2015, 692, 149, 1958,\n",
    "               647, 1530, 1228, 930, 567, 1003, 46, 1341, 1045, 1560, 741, 1995, 522, 1728, 1298, 783,\n",
    "               1778, 1077, 640, 1774, 226, 1694, 285, 969, 97, 1863, 578, 558, 780, 813, 397, 643, 696,\n",
    "               1026, 434, 559, 1699, 1195, 251, 534, 555, 1555, 1676, 403, 1373, 577, 762, 912, 1611,\n",
    "               943, 278, 1135, 1584, 1207, 323, 186, 1076, 1470, 1564, 952, 221, 1184, 419, 478, 880,\n",
    "               1276, 1938, 982, 1159, 116, 395, 1936, 1926, 980, 729, 524, 1290, 252, 1670, 264, 1727,\n",
    "               1083, 412, 398, 1155, 814, 688, 1865, 126, 561, 1835, 1372, 1154, 716, 362, 216, 1534,\n",
    "               320, 463, 866, 932, 843, 311, 672, 170, 1218, 869, 1665, 975, 1144, 110, 1946, 1691,\n",
    "               1698, 759, 761, 53, 1111, 1141, 1109, 457, 573, 2011, 1593, 25, 360, 650, 997, 1431,\n",
    "               347, 769, 427, 704, 1587, 1522, 262, 715, 746, 772, 1650, 354, 1458, 106, 840, 585,\n",
    "               353, 1110, 818, 1878, 422, 543, 637, 571, 1852, 826, 361, 1442, 1243, 1922, 656, 95,\n",
    "               1244, 1556, 1009, 1966, 1552, 456, 1463, 1363, 808, 1326, 481, 1468, 407, 2029, 1687,\n",
    "               974, 1898, 162, 917, 576, 1187, 1327, 1708, 1726, 57, 390, 1553, 1595, 710, 152, 91,\n",
    "               659, 1975, 273, 156, 701, 777, 1118, 1319, 105, 26, 1972, 1093, 1220, 833, 1776, 366,\n",
    "               306, 498, 1368, 31, 918, 1639, 1236, 1797]\n",
    "wCount = len(topFeatures)\n",
    "\n",
    "headers = ['lat_center', 'lon_center'] + elements + mareOrHighland + [f'w_{i}' for i in range(1, wCount + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "j63FWL0Ye-WQ"
   },
   "outputs": [],
   "source": [
    "def classify_points(lat, lon_list):\n",
    "    \"\"\"\n",
    "    Classifies a continuous set of points along the same latitude.\n",
    "    lat: latitude of all points\n",
    "    lon_list: list of longitudes\n",
    "    \"\"\"\n",
    "    points = [Point(lon, lat) for lon in lon_list]\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=points, crs=gdf.crs)\n",
    "\n",
    "    # Perform spatial join\n",
    "    joined = gpd.sjoin(points_gdf, gdf[['geometry', 'region_type']], how='left', predicate='within')\n",
    "    classifications = joined['region_type'].fillna(2).tolist()  # Default to 2 (Highland)\n",
    "\n",
    "    return classifications\n",
    "\n",
    "def isMareOrHighland(lat, lon):\n",
    "    '''\n",
    "        Returns if the given coordinate belongs to a mare or highland region\n",
    "        1 - Mare\n",
    "        2 - Highland\n",
    "    '''\n",
    "    # Create a GeoDataFrame for the point of interest\n",
    "    point = Point(lon, lat)  # Make sure lon, lat are in the correct order (lon, lat)\n",
    "    gdf_point = gpd.GeoDataFrame(geometry=[point], crs=gdf.crs)\n",
    "\n",
    "    # Check if the point lies within any of the polygons (maria regions)\n",
    "    is_maria = gdf['geometry'].apply(lambda x: x.contains(point)).any()\n",
    "\n",
    "    if is_maria:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is1peIOanC5_"
   },
   "source": [
    "# Part 2: File Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSVr3BEonHVn"
   },
   "source": [
    "This part exposes a function Part2(dataframe) responsible for reading the supplied abundances dataframe, solving the optimisation condition to get the abundances of the 8 subpoints in each rectangle. This is then populated in the subregion file.\n",
    "\n",
    "It returns region wise number of updated points, number of updated points in each subregion of the region and the indices where each csv file is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uHKltA2gk68p"
   },
   "outputs": [],
   "source": [
    "from shapely import Point\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "y0XWk8Tik64v"
   },
   "outputs": [],
   "source": [
    "lat_per_region = (LATITUDE_RANGE[0] - LATITUDE_RANGE[1]) / SUBREGIONS_PER_ROW\n",
    "lon_per_region = (LONGITUDE_RANGE[1] - LONGITUDE_RANGE[0]) / SUBREGIONS_PER_ROW\n",
    "regions = 0\n",
    "updatedRegions = np.zeros((SUBREGIONS_PER_ROW, SUBREGIONS_PER_ROW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sFhACft-n_zj"
   },
   "outputs": [],
   "source": [
    "def find_region_indices(lat, lon):\n",
    "    \"\"\"\n",
    "    Determine which of the 64 regions a given latitude and longitude belongs to.\n",
    "\n",
    "    Parameters:\n",
    "    - lat: Latitude coordinate (90 to -90)\n",
    "    - lon: Longitude coordinate (-180 to 180)\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of (row_index, column_index) for the region (0-7, 0-7)\n",
    "    \"\"\"\n",
    "    # Calculate row and column indices\n",
    "    row_index = min(7, abs(int((LATITUDE_RANGE[0] - lat) / lat_per_region)))\n",
    "    col_index = min(7, abs(int((lon - LONGITUDE_RANGE[0]) / lon_per_region)))\n",
    "\n",
    "    return row_index, col_index\n",
    "\n",
    "def find_subregion_indices(lat, lon):\n",
    "    \"\"\"\n",
    "    Determines all of the 78 overlapping subregions of a region a given latitude and longitude belongs to.\n",
    "\n",
    "    Parameters:\n",
    "    - lat: Latitude coordinate (90 to -90)\n",
    "    - lon: Longitude coordinate (-180 to 180)\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples [(r_i, c_i)] where r_i belongs to (0 - 5) and c_i belongs to (0 - 12)\n",
    "    \"\"\"\n",
    "\n",
    "    # Constants for the grid and chunk divisions\n",
    "    LAT_GRID_SIZE = lat_per_region\n",
    "    LON_GRID_SIZE = lon_per_region\n",
    "    LAT_CHUNK_SIZE = LAT_GRID_SIZE / 342  # Degrees per chunk in latitude\n",
    "    LON_CHUNK_SIZE = LON_GRID_SIZE / 682  # Degrees per chunk in longitude\n",
    "\n",
    "    # Subregion starting points for latitude and longitude\n",
    "    LAT_SUBREGION_STARTS = [0, 50, 100, 150, 200, 242]\n",
    "    LON_SUBREGION_STARTS = [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 582]\n",
    "    LAT_SUBREGION_ENDS = [start + 100 for start in LAT_SUBREGION_STARTS]\n",
    "    LON_SUBREGION_ENDS = [start + 100 for start in LON_SUBREGION_STARTS]\n",
    "\n",
    "    # Step 1: Identify the grid cell\n",
    "    grid_row = min(7, abs(int((LATITUDE_RANGE[0] - lat) / lat_per_region)))\n",
    "    grid_col = min(7, abs(int((lon - LONGITUDE_RANGE[0]) / lon_per_region)))\n",
    "\n",
    "    # Step 2: Relative position within the grid cell\n",
    "    rel_lat = (90 - lat) % lat_per_region\n",
    "    rel_lon = (lon + 180) % lon_per_region\n",
    "\n",
    "    # Step 3: Map to chunk coordinates within the 342x682 grid\n",
    "    chunk_row = int(rel_lat // LAT_CHUNK_SIZE)\n",
    "    chunk_col = int(rel_lon // LON_CHUNK_SIZE)\n",
    "\n",
    "    # Step 4: Determine overlapping subregions\n",
    "    subregion_indices = []\n",
    "    for r_idx, (start_lat, end_lat) in enumerate(zip(LAT_SUBREGION_STARTS, LAT_SUBREGION_ENDS)):\n",
    "        if start_lat <= chunk_row < end_lat:\n",
    "            for c_idx, (start_lon, end_lon) in enumerate(zip(LON_SUBREGION_STARTS, LON_SUBREGION_ENDS)):\n",
    "                if start_lon <= chunk_col < end_lon:\n",
    "                    subregion_indices.append((r_idx, c_idx))\n",
    "\n",
    "    return grid_row, grid_col, subregion_indices\n",
    "\n",
    "def find_and_update_subregion_indices(lat, lon, updatedRegions, updatedSubregions):\n",
    "    row_index, col_index, subregion_indices = find_subregion_indices(lat, lon)\n",
    "\n",
    "    updatedRegions[row_index][col_index] += 1\n",
    "    for tup in subregion_indices:\n",
    "        updatedSubregions[row_index][col_index][tup[0]][tup[1]] += 1\n",
    "\n",
    "    return row_index, col_index, updatedRegions, updatedSubregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "V3H93i6wn_w9"
   },
   "outputs": [],
   "source": [
    "def calculate_abundances(A, labels, counter):\n",
    "    \"\"\"\n",
    "    Calculate abundances using the optimization method\n",
    "\n",
    "    Parameters:\n",
    "    - A: Input tensor of initial abundances (8x1)\n",
    "    - labels: Labels for 8 regions (1 for Mare, otherwise Highland)\n",
    "\n",
    "    Returns:\n",
    "    - Calculated abundances matrix (C)\n",
    "    \"\"\"\n",
    "    # print(f\"[DEBUG] Calculating abundances for label : {labels} : done : {counter}\")\n",
    "    # Highland and Mare reference abundances\n",
    "    # [\"Fe\", \"Ti\", \"Ca\", \"Si\", \"Al\", \"Mg\", \"Na\", \"O\"]\n",
    "    Ah = torch.tensor([2.58, 0.66, 13.15, 22.41, 13.65, 0.18, 1.37, 45], dtype=torch.float32).reshape(8, 1)\n",
    "    Am = torch.tensor([9.72, 4.37, 7.00, 18.76, 7.40, 7.23, 1.37, 45], dtype=torch.float32).reshape(8, 1)\n",
    "\n",
    "    # Matrix D (8x8): each column is Ah or Am based on labels\n",
    "    D = torch.zeros(8, 8, dtype=torch.float32)\n",
    "    for i, label in enumerate(labels):\n",
    "        D[:, i] = Ah.squeeze() if label == 2 else Am.squeeze()\n",
    "\n",
    "    # Regularization parameter\n",
    "    lambda_reg = 1.0\n",
    "    lambda_nonneg = 2.0  # Penalty for negative values\n",
    "\n",
    "    # Define Lagrangian Function\n",
    "    def lagrangian(B_flat):\n",
    "        B = B_flat.reshape(1, 8)  # Reshape B into 1x8\n",
    "        C = A @ B                # Compute C = A * B (8x8)\n",
    "        loss = torch.norm(C - D, p='fro') ** 2\n",
    "        constraint = (torch.mean(C, dim=1) - A.squeeze()) ** 2\n",
    "        non_negativity_penalty = torch.sum(torch.relu(-C))\n",
    "        lagrangian_value = loss + lambda_reg * torch.sum(constraint) + lambda_nonneg * non_negativity_penalty\n",
    "        return lagrangian_value\n",
    "\n",
    "    B = torch.rand(1, 8, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([B], lr=0.01)\n",
    "\n",
    "    # Gradient Descent Loop\n",
    "    num_epochs = 80\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = lagrangian(B)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    # Compute final C (with ReLU to ensure non-negativity)\n",
    "    C = torch.relu(A @ B.detach())\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iHQZu7-Kn_t-"
   },
   "outputs": [],
   "source": [
    "def interpolate_subregion_center(vertices_lat, vertices_lon, i, divide_along_left_right):\n",
    "    \"\"\"\n",
    "    Interpolate the center coordinates for a specific subregion along the longer side.\n",
    "\n",
    "    Parameters:\n",
    "    - vertices_lat: List of 4 vertex latitudes in clockwise order.\n",
    "    - vertices_lon: List of 4 vertex longitudes in clockwise order.\n",
    "    - i: Subregion index (0-7).\n",
    "    - divide_along_left_right: Boolean, True if dividing along left-right sides, False if top-bottom.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of (subregion_center_lat, subregion_center_lon).\n",
    "    \"\"\"\n",
    "    def wrap_longitude(lon):\n",
    "        \"\"\"Wrap longitude to [-180, 180].\"\"\"\n",
    "        return (lon + 180) % 360 - 180\n",
    "\n",
    "    if divide_along_left_right:  # Divide along the left-right longer sides\n",
    "        left_lat_interp = vertices_lat[0] + (i + 0.5) * (vertices_lat[3] - vertices_lat[0]) / 8\n",
    "        left_lon_interp = vertices_lon[0] + (i + 0.5) * (wrap_longitude(vertices_lon[3] - vertices_lon[0])) / 8\n",
    "\n",
    "        right_lat_interp = vertices_lat[1] + (i + 0.5) * (vertices_lat[2] - vertices_lat[1]) / 8\n",
    "        right_lon_interp = vertices_lon[1] + (i + 0.5) * (wrap_longitude(vertices_lon[2] - vertices_lon[1])) / 8\n",
    "\n",
    "        subregion_center_lat = (left_lat_interp + right_lat_interp) / 2\n",
    "        subregion_center_lon = wrap_longitude((left_lon_interp + right_lon_interp) / 2)\n",
    "\n",
    "    else:  # Divide along the top-bottom shorter sides\n",
    "        top_lat_interp = vertices_lat[0] + (i + 0.5) * (vertices_lat[1] - vertices_lat[0]) / 8\n",
    "        top_lon_interp = vertices_lon[0] + (i + 0.5) * (wrap_longitude(vertices_lon[1] - vertices_lon[0])) / 8\n",
    "\n",
    "        bottom_lat_interp = vertices_lat[3] + (i + 0.5) * (vertices_lat[2] - vertices_lat[3]) / 8\n",
    "        bottom_lon_interp = vertices_lon[3] + (i + 0.5) * (wrap_longitude(vertices_lon[2] - vertices_lon[3])) / 8\n",
    "\n",
    "        subregion_center_lat = (top_lat_interp + bottom_lat_interp) / 2\n",
    "        subregion_center_lon = wrap_longitude((top_lon_interp + bottom_lon_interp) / 2)\n",
    "\n",
    "    return subregion_center_lat, subregion_center_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZtn2p1Gn_rR"
   },
   "outputs": [],
   "source": [
    "def group_subregions(vertices_lat, vertices_lon):\n",
    "    \"\"\"\n",
    "    Groups subregions based on the longer side of the quadrilateral.\n",
    "\n",
    "    Parameters:\n",
    "    - vertices_lat: List of latitudes of the region's vertices.\n",
    "    - vertices_lon: List of longitudes of the region's vertices.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with subregion indices as keys and center coordinates as values.\n",
    "    \"\"\"\n",
    "    # Calculate great-circle distances for each pair of sides\n",
    "    def great_circle_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate the great-circle distance between two points.\"\"\"\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        delta_lon = np.abs(lon2 - lon1)\n",
    "        delta_lon = np.minimum(delta_lon, 2 * np.pi - delta_lon)\n",
    "        central_angle = np.arccos(np.sin(lat1) * np.sin(lat2) + np.cos(lat1) * np.cos(lat2) * np.cos(delta_lon))\n",
    "        return central_angle * MOON_RADIUS_KM  # Moon's radius in kilometers\n",
    "\n",
    "    # Compute lengths of opposite sides\n",
    "    left_length = great_circle_distance(vertices_lat[0], vertices_lon[0], vertices_lat[3], vertices_lon[3])\n",
    "    right_length = great_circle_distance(vertices_lat[1], vertices_lon[1], vertices_lat[2], vertices_lon[2])\n",
    "    top_length = great_circle_distance(vertices_lat[0], vertices_lon[0], vertices_lat[1], vertices_lon[1])\n",
    "    bottom_length = great_circle_distance(vertices_lat[3], vertices_lon[3], vertices_lat[2], vertices_lon[2])\n",
    "\n",
    "    # Determine whether to divide along left-right (longer) or top-bottom (shorter)\n",
    "    divide_along_left_right = (left_length + right_length) >= (top_length + bottom_length)\n",
    "\n",
    "    # Compute subregion centers\n",
    "    subregion_centers = {}\n",
    "    for i in range(8):\n",
    "        subregion_center_lat, subregion_center_lon = interpolate_subregion_center(vertices_lat, vertices_lon, i, divide_along_left_right)\n",
    "        subregion_centers[i] = (subregion_center_lat, subregion_center_lon)\n",
    "\n",
    "    return subregion_centers\n",
    "\n",
    "def find_grid_indices_in_subregion(lat, lon, subregion_row, subregion_col):\n",
    "    \"\"\"\n",
    "    Calculate the grid indices for a given latitude and longitude within a subregion.\n",
    "\n",
    "    Parameters:\n",
    "    - lat: Latitude of the point.\n",
    "    - lon: Longitude of the point.\n",
    "    - subregion_row: Row index of the subregion.\n",
    "    - subregion_col: Column index of the subregion.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple (square_row, square_col) representing the grid indices.\n",
    "    \"\"\"\n",
    "    # Calculate the start of the subregion\n",
    "    subregion_lat_start = LATITUDE_RANGE[0] - subregion_row * lat_per_region\n",
    "    subregion_lon_start = LONGITUDE_RANGE[0] + subregion_col * lon_per_region\n",
    "    # Calculate the grid square within the subregion\n",
    "    square_row = abs(int((lat - subregion_lat_start) // square_size_deg))\n",
    "    square_col = abs(int((lon - subregion_lon_start) // square_size_deg))\n",
    "\n",
    "    if not (0 <= square_row < num_squares_lat) or not (0 <= square_col < num_squares_lon):\n",
    "        # If point is out of the subregion's grid, find the nearest grid point\n",
    "        square_row = max(0, min(square_row, num_squares_lat))\n",
    "        square_col = max(0, min(square_col, num_squares_lon))\n",
    "\n",
    "    # Calculate the exact row index\n",
    "    row_index = square_row * num_squares_lon + square_col\n",
    "\n",
    "    return row_index\n",
    "\n",
    "def save_subregion(subregion_row, subregion_col, points):\n",
    "    print(f\"Processing subregion: ({subregion_row}, {subregion_col})\")\n",
    "\n",
    "    local_indices = []\n",
    "\n",
    "    # Define the file name for the subregion\n",
    "    csv_filename = f\"./regions/ISRO_RegionData{subregion_row - subregion_row%2}{1+subregion_row - subregion_row%2}/subregion_{subregion_row}_{subregion_col}.csv\"\n",
    "    print(f\"[INFO] {current_time()} Processing file: {csv_filename}\")\n",
    "\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    dtype_spec = {\n",
    "        'lat_center': 'float',\n",
    "        'lon_center': 'float',\n",
    "        'Fe': 'float',\n",
    "        'Ti': 'float',\n",
    "        'Ca': 'float',\n",
    "        'Si': 'float',\n",
    "        'Al': 'float',\n",
    "        'Mg': 'float',\n",
    "        'Na': 'float',\n",
    "        'O': 'float',\n",
    "    }\n",
    "    df = df.astype(dtype_spec)\n",
    "    elements = ['Fe', 'Ti', 'Ca', 'Si', 'Al', 'Mg', 'Na', 'O']\n",
    "\n",
    "    for point in points:\n",
    "        # Find the closest grid index\n",
    "        grid_index = find_grid_indices_in_subregion(\n",
    "            point['x_center'], point['y_center'], subregion_row, subregion_col\n",
    "        )\n",
    "        if grid_index not in df.index:\n",
    "            continue\n",
    "        existing_values = df.loc[grid_index, elements]\n",
    "\n",
    "        local_indices.append(grid_index)\n",
    "\n",
    "        if existing_values.sum() == 0:\n",
    "            # Direct assignment if all values are zero\n",
    "            df.loc[grid_index, elements] = [\n",
    "                point['Fe'], point['Ti'], point['Ca'],\n",
    "                point['Si'], point['Al'], point['Mg'], point['Na'], point['O']]\n",
    "            #print(f\"[INFO] {current_time()} Updated values at index {grid_index} in file {csv_filename} at {point['x_center'], point['y_center']}\")\n",
    "        else:\n",
    "            # Average with new values if non-zero\n",
    "            new_values = pd.Series({\n",
    "                'Fe': point['Fe'],\n",
    "                'Ti': point['Ti'],\n",
    "                'Ca': point['Ca'],\n",
    "                'Si': point['Si'],\n",
    "                'Al': point['Al'],\n",
    "                'Mg': point['Mg'],\n",
    "                'Na': point['Na'],\n",
    "                'O': point['O'],\n",
    "            })\n",
    "            # df.loc[grid_index, elements] = (\n",
    "            #     (existing_values + new_values) / 2\n",
    "            # )\n",
    "            df.loc[grid_index, elements] = new_values\n",
    "            #print(f\"[INFO] {current_time()} Averaged values at index {grid_index} in file {csv_filename} at {point['x_center'], point['y_center']}\")\n",
    "\n",
    "    # Save the updated CSV file\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"[INFO] {current_time()} Saved updates to file: {csv_filename}\")\n",
    "\n",
    "    return local_indices\n",
    "\n",
    "def update_csv_files(subregion_data, indices):\n",
    "    \"\"\"\n",
    "    Update existing CSV files for all subregions by populating the closest node\n",
    "    to the subregion's center using grid-based calculation. Updates are based on:\n",
    "    - Direct update if existing values are zero.\n",
    "    - Averaging if existing values are non-zero.\n",
    "\n",
    "    Parameters:\n",
    "    - subregion_data: Dictionary with subregion keys and collected points as values.\n",
    "\n",
    "    Returns:\n",
    "    - indices: An 8x8 array of lists containing row_indices of each updation\n",
    "    \"\"\"\n",
    "\n",
    "    # # for (subregion_row, subregion_col), points in tqdm(subregion_data.items(), desc=\"Processing Subregions\"):\n",
    "    # with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    #     futures = [\n",
    "    #         executor.submit(process_subregion, subregion_row, subregion_col, points)\n",
    "    #         for (subregion_row, subregion_col), points in subregion_data.items()\n",
    "    #     ]\n",
    "\n",
    "    #     for future in tqdm(futures, desc=\"Processing Subregions\"):\n",
    "    #         future.result()  # Handle exceptions if any.\n",
    "\n",
    "    subregion_tasks = list(subregion_data.items())  # Convert to list to get total count for progress tracking\n",
    "    with ThreadPoolExecutor(max_workers = 2) as executor:\n",
    "        # Initialize progress bar\n",
    "        with tqdm(total=len(subregion_tasks), desc=\"Updating CSV Files\") as pbar:\n",
    "            futures = {\n",
    "                executor.submit(save_subregion, subregion_row, subregion_col, points):\n",
    "                (subregion_row, subregion_col)\n",
    "                for (subregion_row, subregion_col), points in subregion_tasks\n",
    "            }\n",
    "\n",
    "            # Update progress bar as each task completes\n",
    "            for future in as_completed(futures):\n",
    "                subregion_row, subregion_col = futures[future]\n",
    "                try:\n",
    "                    local_indices = future.result()\n",
    "                    indices[subregion_row][subregion_col].extend(local_indices)\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Subregion ({subregion_row}, {subregion_col}): {e}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AKfwaLDRpZnw"
   },
   "outputs": [],
   "source": [
    "def process_data_regions(data_regions, batch_size, updatedRegions, updatedSubregions, indices):\n",
    "    \"\"\"\n",
    "    Process data regions with optimized subregion segregation and abundance calculation.\n",
    "\n",
    "    Parameters:\n",
    "    - data_regions: Original dataset containing 4-vertex coordinates and element abundances\n",
    "    - batch_size: Number of data regions to process in one batch.\n",
    "    - updatedRegions: a numpy array of shape 8, 8 to store number of enteries added in each region\n",
    "    - updatedSubregions: a numpy array of shape 8, 8, 6, 13 to store number of enteries added in each subregion\n",
    "    - indices: a numpy array of shape shape 8, 8 containing lists to store indices of updation\n",
    "\n",
    "    This function collects data for subregions and updates CSVs after batch processing.\n",
    "    \"\"\"\n",
    "\n",
    "    bools = input('Is it old data?')\n",
    "\n",
    "    for batch_start in range(0, len(data_regions), batch_size):\n",
    "        batch_regions = data_regions[batch_start:batch_start + batch_size]\n",
    "\n",
    "        # Dictionary to store subregion data\n",
    "        subregion_data = defaultdict(list)\n",
    "\n",
    "        # Collect data for the entire batch\n",
    "        # for counter, region in batch_regions.iterrows():\n",
    "        for counter, region in tqdm(batch_regions.iterrows(), total=len(batch_regions), desc=\"Calculating abundances\"):\n",
    "            # Extract vertex coordinates\n",
    "            vertices_lat = [\n",
    "                region['V0_lat'], region['V1_lat'],\n",
    "                region['V2_lat'], region['V3_lat']\n",
    "            ]\n",
    "            vertices_lon = [\n",
    "                region['V0_lon'], region['V1_lon'],\n",
    "                region['V2_lon'], region['V3_lon']\n",
    "            ]\n",
    "\n",
    "            # Group subregions\n",
    "            subregion_centers = group_subregions(vertices_lat, vertices_lon)\n",
    "\n",
    "            # Prepare initial abundances (8x1 matrix)\n",
    "            initial_abundances = torch.tensor([\n",
    "                region['Fe'], region['Ti'], region['Ca'],\n",
    "                region['Si'], region['Al'], region['Mg'], region['Na'], region['O']\n",
    "            ], dtype=torch.float32).reshape(8, 1)\n",
    "\n",
    "            # Determine labels for all 8 subregions\n",
    "            labels = [\n",
    "                isMareOrHighland(subregion_center_lat, subregion_center_lon)\n",
    "                for subregion_center_lat, subregion_center_lon in subregion_centers.values()\n",
    "            ]\n",
    "\n",
    "            if bools == 'yes':\n",
    "                TI = np.random.normal(np.mean([4.37 if label == 1 else 0.66 for label in labels]), 1)\n",
    "                region['Ti'] = TI\n",
    "                initial_abundances[1] = TI\n",
    "                CA = 100 - initial_abundances.sum().item()\n",
    "                region['Ca'] = CA\n",
    "                initial_abundances[2] = CA\n",
    "\n",
    "            # Calculate optimized abundances for all 8 subregions (8x8 matrix)\n",
    "            optimized_abundances = calculate_abundances(initial_abundances, labels, counter)\n",
    "\n",
    "            # Store information for all subregions\n",
    "            for i, (subregion_center_lat, subregion_center_lon) in subregion_centers.items():\n",
    "                subregion_row, subregion_col, updatedRegions, updatedSubregions = find_and_update_subregion_indices(subregion_center_lat, subregion_center_lon, updatedRegions, updatedSubregions)\n",
    "                subregion_key = (subregion_row, subregion_col)\n",
    "\n",
    "                subregion_data[subregion_key].append({\n",
    "                    'x_center': subregion_center_lat,\n",
    "                    'y_center': subregion_center_lon,\n",
    "                    'Fe': optimized_abundances[0, i].item(),\n",
    "                    'Ti': optimized_abundances[1, i].item(),\n",
    "                    'Ca': optimized_abundances[2, i].item(),\n",
    "                    'Si': optimized_abundances[3, i].item(),\n",
    "                    'Al': optimized_abundances[4, i].item(),\n",
    "                    'Mg': optimized_abundances[5, i].item(),\n",
    "                    'Na': optimized_abundances[6, i].item(),\n",
    "                    'O': optimized_abundances[7, i].item(),\n",
    "                })\n",
    "\n",
    "        for subregion_key, entries in subregion_data.items():\n",
    "            print(f\"Subregion '{subregion_key}' has {len(entries)} entries in batch {batch_start//batch_size}\")\n",
    "\n",
    "        # Write all collected data to CSVs outside the main loop\n",
    "        indices = update_csv_files(subregion_data, indices)\n",
    "\n",
    "        return updatedRegions, updatedSubregions, indices\n",
    "\n",
    "def preprocess(data):\n",
    "    # Clean column names by stripping leading/trailing whitespace\n",
    "    data.columns = data.columns.str.strip()\n",
    "    # Step 1: Extract only the weight columns for elements\n",
    "    weight_columns = [col for col in data.columns if col.endswith('_WT')]\n",
    "    lats = ['V0_LATITUDE', 'V0_LONGITUDE', 'V1_LATITUDE', 'V1_LONGITUDE', 'V2_LATITUDE', 'V2_LONGITUDE', 'V3_LATITUDE', 'V3_LONGITUDE']\n",
    "    weights_data = data[weight_columns + lats].copy()\n",
    "    # Step 2: Add fixed columns for \"Na\" and \"O\"\n",
    "    weights_data['Na'] = 1.375\n",
    "    weights_data['O'] = 45\n",
    "    weights_data['Ti'] = 0\n",
    "    weights_data['Ca'] = 0\n",
    "    # Resulting dataset\n",
    "    weights_data.head()\n",
    "    weights_data.rename(columns={\n",
    "        'V0_LATITUDE': 'lat0', 'V0_LONGITUDE': 'lon0',\n",
    "        'V1_LATITUDE': 'lat1', 'V1_LONGITUDE': 'lon1',\n",
    "        'V2_LATITUDE': 'lat2', 'V2_LONGITUDE': 'lon2',\n",
    "        'V3_LATITUDE': 'lat3', 'V3_LONGITUDE': 'lon3',\n",
    "        'MG_WT': 'Mg', 'AL_WT': 'Al', 'SI_WT': 'Si',\n",
    "        'FE_WT': 'Fe'\n",
    "    }, inplace=True)\n",
    "    # Assuming `df` is your dataframe\n",
    "    new_column_order = [\n",
    "        'lat0', 'lon0', 'lat1', 'lon1', 'lat2', 'lon2', 'lat3', 'lon3',\n",
    "        'Fe', 'Ti', 'Ca', 'Si', 'Al', 'Mg', 'Na', 'O'\n",
    "    ]\n",
    "    weights_data = weights_data[new_column_order]\n",
    "    print(weights_data.head())\n",
    "    # exit()\n",
    "    return weights_data\n",
    "\n",
    "def RegionProcessor2(file_path, batch_size, updatedRegions, updatedSubregions, indices):\n",
    "    \"\"\"\n",
    "    Function to process a single data region file.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"[INFO] {current_time()} Processing file: {file_path}\")\n",
    "\n",
    "    # Read the data region file\n",
    "    data_regions = pd.read_csv(file_path)\n",
    "\n",
    "    data_regions = preprocess(data_regions)\n",
    "\n",
    "    # Adjust headers to match expected format\n",
    "    data_regions.rename(columns={\n",
    "        'lat0': 'V0_lat', 'lon0': 'V0_lon',\n",
    "        'lat1': 'V1_lat', 'lon1': 'V1_lon',\n",
    "        'lat2': 'V2_lat', 'lon2': 'V2_lon',\n",
    "        'lat3': 'V3_lat', 'lon3': 'V3_lon',\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Process the data in batches\n",
    "    updatedRegions, updatedSubregions, indices = process_data_regions(data_regions, batch_size, updatedRegions, updatedSubregions, indices)\n",
    "\n",
    "    print(updatedRegions)\n",
    "\n",
    "    return updatedRegions, updatedSubregions, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wUB5eLl2pZg4"
   },
   "outputs": [],
   "source": [
    "# List of data region files\n",
    "def Part2(file_name):\n",
    "    batch_size = 3000\n",
    "\n",
    "    updatedRegions = np.zeros((8, 8))\n",
    "    updatedSubregions = np.zeros((8, 8, 6, 13))\n",
    "    # Create an 8x8 array with empty lists\n",
    "    indices = np.empty((8, 8), dtype=object)\n",
    "    # Initialize each element to be an empty list\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            indices[i, j] = []\n",
    "\n",
    "    return RegionProcessor2(file_name, batch_size, updatedRegions, updatedSubregions, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb1OfV-OBer3"
   },
   "source": [
    "Now for each input file, I have saved all enteries in the correct subregion file. I just need to process each input file now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX7YCl25CNn4"
   },
   "source": [
    "# Part 3: CSV to Subgraphs\n",
    "This exposes a function (Part3), which given any i, j, iteration_number will create the subgraphs of all the 78 subsubregions and save them as a .pt file in the folder: ./graphs/subregion_i_j/subregion_i_j_x_y.pt, and also masks for each subsubregion listing the rows where original abundances are known.\n",
    "\n",
    "This can be used in an iteration loop in Part 5 to train all graphs for a subregion (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WyXrbgGgn_oV"
   },
   "outputs": [],
   "source": [
    "# These parameters control graph construction and sliding window behavior\n",
    "ALPHA = 1                 # Spatial distance weight factor\n",
    "BETA = 1.1                # Feature distance weight factor\n",
    "K = 100                   # Maximum number of nearest neighbors to connect for each node\n",
    "BATCH_SIZE = 1000         # Number of nodes processed in parallel batches\n",
    "LAT_STEP = 682            # Number of longitude entries per latitude block\n",
    "WINDOW_SIZE = 100         # Size of sliding window\n",
    "STRIDE = 50               # Step size between sliding windows\n",
    "LAT_SIZE = 342            # Total latitude grid size\n",
    "LON_SIZE = 682            # Total longitude grid size\n",
    "OccupancyMatrix = np.zeros((6, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MMwG12Yqn_mD"
   },
   "outputs": [],
   "source": [
    "def haversine_gpu(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate great-circle distances between geographical points using Haversine formula on GPU.\n",
    "\n",
    "    Args:\n",
    "        lat1 (torch.Tensor): Latitude of first point(s)\n",
    "        lon1 (torch.Tensor): Longitude of first point(s)\n",
    "        lat2 (torch.Tensor): Latitude of second point(s)\n",
    "        lon2 (torch.Tensor): Longitude of second point(s)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Distances between points in kilometers\n",
    "    \"\"\"\n",
    "    R = MOON_RADIUS_KM  # mooon radius in kilometers\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    lat1, lat2 = torch.deg2rad(lat1), torch.deg2rad(lat2)\n",
    "    lon1, lon2 = torch.deg2rad(lon1), torch.deg2rad(lon2)\n",
    "\n",
    "    # Haversine formula computation\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = torch.sin(dlat / 2)**2 + torch.cos(lat1) * torch.cos(lat2) * torch.sin(dlon / 2)**2\n",
    "    return 2 * R * torch.arcsin(torch.sqrt(a))\n",
    "\n",
    "def compute_spatial_distances_gpu(batch_lat, batch_lon, all_lat, all_lon):\n",
    "    \"\"\"\n",
    "    Compute and normalize spatial distance matrix between a batch of points and all points.\n",
    "\n",
    "    Args:\n",
    "        batch_lat (torch.Tensor): Latitudes of the current batch\n",
    "        batch_lon (torch.Tensor): Longitudes of the current batch\n",
    "        all_lat (torch.Tensor): Latitudes of all points\n",
    "        all_lon (torch.Tensor): Longitudes of all points\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized spatial distance matrix\n",
    "    \"\"\"\n",
    "    distances = haversine_gpu(batch_lat.unsqueeze(1), batch_lon.unsqueeze(1), all_lat, all_lon)\n",
    "    min_val = distances.min()\n",
    "    max_val = distances.max()\n",
    "    normalized_distances = (distances - min_val) / (max_val - min_val)\n",
    "\n",
    "    return normalized_distances\n",
    "\n",
    "def compute_feature_distances_gpu(batch_w, all_w):\n",
    "    \"\"\"\n",
    "    Compute and normalize Euclidean feature distances between a batch of nodes and all nodes.\n",
    "\n",
    "    Args:\n",
    "        batch_w (torch.Tensor): Feature vectors of the current batch\n",
    "        all_w (torch.Tensor): Feature vectors of all nodes\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized feature distance matrix\n",
    "    \"\"\"\n",
    "    distances = torch.cdist(batch_w, all_w, p=2)\n",
    "    min_val = distances.min()\n",
    "    max_val = distances.max()\n",
    "    normalized_distances = (distances - min_val) / (max_val - min_val)\n",
    "\n",
    "    return normalized_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "hGX8rrF0n_jZ"
   },
   "outputs": [],
   "source": [
    "def compute_edge_weights_gpu(Dspatial, Dfeature, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute edge weights based on spatial and feature distances.\n",
    "\n",
    "    Uses an exponential decay function: exp(-α * spatial_dist - β * feature_dist)\n",
    "\n",
    "    Args:\n",
    "        Dspatial (torch.Tensor): Spatial distance matrix\n",
    "        Dfeature (torch.Tensor): Feature distance matrix\n",
    "        alpha (float): Spatial distance weight factor\n",
    "        beta (float): Feature distance weight factor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Edge weight matrix\n",
    "    \"\"\"\n",
    "    return torch.exp(-alpha * Dspatial - beta * Dfeature)\n",
    "\n",
    "def calculate_sliding_windows(window_size=WINDOW_SIZE, stride=STRIDE):\n",
    "    \"\"\"\n",
    "    Calculate sliding window positions for a grid of specific dimensions.\n",
    "\n",
    "    Args:\n",
    "        window_size (int): Size of each sliding window\n",
    "        stride (int): Step size between windows\n",
    "\n",
    "    Returns:\n",
    "        tuple: Latitude and longitude positions and sizes\n",
    "    \"\"\"\n",
    "    def get_windows(num_windows, actual_size):\n",
    "        positions = []\n",
    "        sizes = []\n",
    "\n",
    "        # Calculate number of full strides\n",
    "        num_full_strides = num_windows - 1\n",
    "\n",
    "        # For all windows except the last one\n",
    "        for i in range(num_full_strides):\n",
    "            positions.append(i * stride)\n",
    "            sizes.append(window_size)\n",
    "\n",
    "        # Handle the last window to ensure coverage of actual data size\n",
    "        last_start = actual_size - window_size\n",
    "        positions.append(last_start)\n",
    "        sizes.append(window_size)\n",
    "\n",
    "        return positions, sizes\n",
    "\n",
    "    # 6 windows in latitude direction (5 strides of 50 + last window)\n",
    "    lat_positions, lat_sizes = get_windows(6, LAT_SIZE)\n",
    "\n",
    "    # 12 windows in longitude direction (11 strides of 50 + last window)\n",
    "    lon_positions, lon_sizes = get_windows(13, LON_SIZE)\n",
    "\n",
    "    # print(f\"[INFO] {current_time()} Latitude positions: {lat_positions}\")\n",
    "    # print(f\"[INFO] {current_time()} Longitude positions: {lon_positions}\")\n",
    "\n",
    "    return lat_positions, lon_positions, lat_sizes, lon_sizes\n",
    "\n",
    "def process_subgraph(df, subregion_row, subregion_col, lat_start, lon_start, lat_size, lon_size, itr_no):\n",
    "    \"\"\"\n",
    "    Process a subgraph within a specified window of the lunar region.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Full subregion dataframe\n",
    "        subregion_row (int): Subregion row index\n",
    "        subregion_col (int): Subregion column index\n",
    "        lat_start (int): Starting latitude index\n",
    "        lon_start (int): Starting longitude index\n",
    "        lat_size (int): Latitude window size\n",
    "        lon_size (int): Longitude window size\n",
    "\n",
    "    Returns:\n",
    "        str: Path where the graph is saved\n",
    "    \"\"\"\n",
    "    # global OccupancyMatrix\n",
    "\n",
    "    # Calculate indices for the subgraph\n",
    "    indices = []\n",
    "    for i in range(lat_size):\n",
    "        row_start = (lat_start + i) * LAT_STEP + lon_start\n",
    "        indices.extend(range(row_start, row_start + lon_size))\n",
    "\n",
    "    row_idx = int(np.ceil(float(lat_start)/STRIDE))\n",
    "    col_idx = int(np.ceil(float(lon_start)/STRIDE))\n",
    "    print(f\"[INFO] {current_time()} Started processing subregion {subregion_row} {subregion_col}, subgraph id: {row_idx} {col_idx}\")\n",
    "\n",
    "    # Subset the dataframe\n",
    "    sub_df = df.iloc[indices].copy()\n",
    "\n",
    "    # Extract features for the subgraph\n",
    "    latitudes = sub_df['lat_center'].values\n",
    "    longitudes = sub_df['lon_center'].values\n",
    "    mareOrHighland = sub_df['mareOrHighland'].values\n",
    "    w_vectors = sub_df[[f'w_{i}' for i in range(1, wCount + 1)]].values\n",
    "    element_columns = elements\n",
    "    element_compositions = sub_df[element_columns].values\n",
    "    if itr_no == 1:\n",
    "        updates = sub_df['updated'].values\n",
    "        updates_tensor = torch.tensor(updates, dtype=torch.bool).to(device)\n",
    "\n",
    "    # non_zero_rows = np.sum((sub_df[element_columns].sum(axis=1) > 0).values)\n",
    "    # total_rows = len(sub_df)\n",
    "    # percentage_non_zero = (non_zero_rows / total_rows) * 100\n",
    "    # OccupancyMatrix[row_idx, col_idx] = percentage_non_zero\n",
    "\n",
    "    # Move data to GPU and preprocess\n",
    "    latitudes_tensor = torch.tensor(latitudes, dtype=torch.float32).to(device)\n",
    "    longitudes_tensor = torch.tensor(longitudes, dtype=torch.float32).to(device)\n",
    "    mareOrHighland_tensor = torch.tensor(mareOrHighland, dtype=int).to(device)\n",
    "\n",
    "    scaler_w = StandardScaler()\n",
    "    w_vectors_tensor = torch.tensor(scaler_w.fit_transform(w_vectors), dtype=torch.float32).to(device)\n",
    "    element_compositions_tensor = torch.tensor(element_compositions, dtype=torch.float32).to(device)\n",
    "\n",
    "    # if (itr_no == 1):\n",
    "    #     # Need to compute element mask only in the first iteration, otherwise load it\n",
    "    #     element_mask_tensor = torch.tensor((sub_df[element_columns].sum(axis=1) > 0).values, dtype=torch.bool).to(device)\n",
    "    #     os.makedirs(f'./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row}_{subregion_col}', exist_ok=True)\n",
    "    #     torch.save(element_mask_tensor, f'./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row}_{subregion_col}/mask_tensor_{subregion_row}_{subregion_col}_{row_idx}_{col_idx}.pt')\n",
    "    #     print(f\"[INFO] {current_time()} Created mask for subgraph: {row_idx}, {col_idx}\")\n",
    "    # else :\n",
    "    #     element_mask_tensor = torch.load(f'masks/masks_subregion_{subregion_row}_{subregion_col}/mask_tensor_{subregion_row}_{subregion_col}_{row_idx}_{col_idx}.pt', weights_only = True)\n",
    "    #     print(f\"[INFO] {current_time()} Loaded mask for subgraph : {row_idx}, {col_idx}\")\n",
    "\n",
    "    os.makedirs(f'./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row}_{subregion_col}', exist_ok=True)\n",
    "    if os.path.isfile(f'./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row}_{subregion_col}/mask_tensor_{subregion_row}_{subregion_col}_{row_idx}_{col_idx}.pt'):\n",
    "        # So mask already exists, need to update if iteration is 1\n",
    "        element_mask_tensor = torch.load(f'./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row}_{subregion_col}/mask_tensor_{subregion_row}_{subregion_col}_{row_idx}_{col_idx}.pt', weights_only = True)\n",
    "        # I need to update the tensor using the updated column in the dataframe\n",
    "        print(f\"[INFO] {current_time()} Loaded mask for subgraph : {row_idx}, {col_idx}\")\n",
    "        if itr_no == 1:\n",
    "            element_mask_tensor = torch.logical_or(element_mask_tensor, updates_tensor)\n",
    "            torch.save(element_mask_tensor, f'./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row}_{subregion_col}/mask_tensor_{subregion_row}_{subregion_col}_{row_idx}_{col_idx}.pt')\n",
    "            print(f\"[INFO] {current_time()} Updated and saved new mask for subgraph : {row_idx}, {col_idx} in iteration {itr_no}\")\n",
    "    else:\n",
    "        # Need to compute element mask if it does not exist\n",
    "        # Mask only needs to be computed in the first iteration only using updates tensor\n",
    "        if itr_no == 1:\n",
    "            element_mask_tensor = updates_tensor\n",
    "            # element_mask_tensor = torch.tensor((sub_df[element_columns].sum(axis=1) > 0).values, dtype=torch.bool).to(device)\n",
    "            os.makedirs(f'./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row}_{subregion_col}', exist_ok=True)\n",
    "            torch.save(element_mask_tensor, f'./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row}_{subregion_col}/mask_tensor_{subregion_row}_{subregion_col}_{row_idx}_{col_idx}.pt')\n",
    "            print(f\"[INFO] {current_time()} Created and saved mask for subgraph: {row_idx}, {col_idx}\")\n",
    "        else:\n",
    "            # So for this subregion mask was not created in the first iteration and it does not even exist\n",
    "            # Thus the entire mask will be zero, it will just be a temporary mask and will not be saved\n",
    "            element_mask_tensor = torch.tensor(np.zeros(len(sub_df)), dtype=torch.bool).to(device)\n",
    "            print(f\"[INFO] {current_time()} Created temporary zero mask for subgraph: {row_idx}, {col_idx}\")\n",
    "\n",
    "\n",
    "    # print(f\"[INFO] {current_time()} Extracted features from dataframe, Subgraph id: {int(np.ceil(float(lat_start)/STRIDE))} {int(np.ceil(float(lon_start)/STRIDE))}\")\n",
    "\n",
    "    # Process graph edges\n",
    "    edge_index = []\n",
    "    edge_weights = []\n",
    "    num_nodes = len(latitudes)\n",
    "\n",
    "    # Iterate over batches to compute edge weights efficiently\n",
    "    for batch_start in range(0, num_nodes, BATCH_SIZE):\n",
    "        # print(f\"[INFO] {current_time()} Starting batch {batch_start}, Subgraph id: {int(np.ceil(float(lat_start)/STRIDE))} {int(np.ceil(float(lon_start)/STRIDE))}\")\n",
    "\n",
    "        batch_lat = latitudes_tensor[batch_start:batch_start+BATCH_SIZE]\n",
    "        batch_lon = longitudes_tensor[batch_start:batch_start+BATCH_SIZE]\n",
    "        batch_w = w_vectors_tensor[batch_start:batch_start+BATCH_SIZE]\n",
    "\n",
    "        # print(f\"[DEBUG] batch_lat shape: {batch_lat.shape}\")\n",
    "        # print(f\"[DEBUG] batch_lon shape: {batch_lon.shape}\")\n",
    "        # print(f\"[DEBUG] batch_w shape: {batch_w.shape}\")\n",
    "\n",
    "        # Compute distances\n",
    "        Dspatial = compute_spatial_distances_gpu(batch_lat, batch_lon, latitudes_tensor, longitudes_tensor)\n",
    "        Dfeature = compute_feature_distances_gpu(batch_w, w_vectors_tensor)\n",
    "        weights = compute_edge_weights_gpu(Dspatial, Dfeature, ALPHA, BETA)\n",
    "\n",
    "        # Debugging: Check distance matrix shapes\n",
    "        # print(f\"[DEBUG] Dspatial shape: {Dspatial.shape}\")\n",
    "        # print(f\"[DEBUG] Dfeature shape: {Dfeature.shape}\")\n",
    "        # print(f\"[DEBUG] weights shape: {weights.shape}\")\n",
    "        # print(f\"[INFO] {current_time()} Computed distances {batch_start}, Subgraph id: {int(np.ceil(float(lat_start)/STRIDE))} {int(np.ceil(float(lon_start)/STRIDE))}\")\n",
    "\n",
    "        # Select top-k neighbors for all nodes in the batch at once\n",
    "        top_k_values, top_k_indices = torch.topk(weights, K+1, dim=1, largest=True)\n",
    "        top_k_values = top_k_values[:,1:]\n",
    "        top_k_indices = top_k_indices[:,1:]\n",
    "\n",
    "        # Flatten and append edge indices and weights in one go\n",
    "        batch_indices = torch.arange(batch_start, batch_start + weights.size(0), device=weights.device).unsqueeze(1).repeat(1, K+1).flatten()\n",
    "        top_k_indices_flat = top_k_indices.flatten()\n",
    "        top_k_values_flat = top_k_values.flatten()\n",
    "\n",
    "        # Append edge indices and weights in a vectorized manner\n",
    "        edge_index.extend(zip(batch_indices.tolist(), top_k_indices_flat.tolist()))\n",
    "        edge_weights.extend(top_k_values_flat.tolist())\n",
    "\n",
    "    # Convert to tensors\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "\n",
    "    # Prepare node features\n",
    "    node_features = torch.hstack((\n",
    "        w_vectors_tensor,\n",
    "        mareOrHighland_tensor.reshape(-1, 1),\n",
    "        latitudes_tensor.reshape(-1, 1),\n",
    "        longitudes_tensor.reshape(-1, 1),\n",
    "    ))\n",
    "\n",
    "    # Create PyTorch Geometric Data object\n",
    "    graph = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_weights,\n",
    "        y=element_compositions_tensor,\n",
    "        mask=element_mask_tensor,\n",
    "    )\n",
    "\n",
    "    # Add metadata for tracking and analysis\n",
    "    graph.metadata = {\n",
    "        \"node_features_shape\": node_features.shape,\n",
    "        \"edge_index_shape\": edge_index.shape,\n",
    "        \"edge_attr_shape\": edge_weights.shape,\n",
    "        \"element_compositions_shape\": element_compositions_tensor.shape,\n",
    "        \"element_mask_shape\": element_mask_tensor.shape,\n",
    "        \"subregion_indices\": {\n",
    "            \"row\": subregion_row,\n",
    "            \"col\": subregion_col,\n",
    "            \"subgraph_row\": row_idx,\n",
    "            \"subgraph_col\": col_idx\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save graph\n",
    "    graphs_dir = f\"./graphs/graphs_subregion_{subregion_row}_{subregion_col}\"\n",
    "    os.makedirs(\"./graphs/\", exist_ok=True)\n",
    "    os.makedirs(graphs_dir, exist_ok=True)\n",
    "\n",
    "    # Define save path using new directory structure\n",
    "    save_path = os.path.join(\n",
    "        graphs_dir,\n",
    "        f\"subgraph_{subregion_row}_{subregion_col}_{row_idx}_{col_idx}.pt\"\n",
    "    )\n",
    "\n",
    "    # Save graph\n",
    "    torch.save(graph, save_path)\n",
    "\n",
    "    print(f\"[INFO] {current_time()} Saved subgraph to {save_path}\")\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "O0xyXze0EdpB"
   },
   "outputs": [],
   "source": [
    "def Part3(subregion_row, subregion_col, iteration_number, updatedIndices, updatedSubregions):\n",
    "    # updatedIndices is a list of csv indices where abundances are added by the file being currently processed\n",
    "    # updatedSubregions is an array of size 6x13 denoting number of updates in each subregion\n",
    "    # In iteration 1, we do not need to generate graphs for subregions where updatedSubregions is 0 and also we need to update/generate the mask\n",
    "\n",
    "    # Construct filename and verify file exists\n",
    "    fileName = f\"./regions/ISRO_RegionData{subregion_row - subregion_row%2}{1+subregion_row - subregion_row%2}/subregion_{subregion_row}_{subregion_col}.csv\"\n",
    "    if not os.path.isfile(fileName):\n",
    "        print(f\"[ERROR] File (subregion_{subregion_row}_{subregion_col}.csv) does not exist. Exiting...\")\n",
    "        exit()\n",
    "\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(fileName)\n",
    "    print(f\"[INFO] {current_time()} Dataframe Read. Size = {df.memory_usage(deep=True).sum()/(1024*1024):6f} MB\")\n",
    "\n",
    "    lat_positions, lon_positions, lat_sizes, lon_sizes = calculate_sliding_windows()\n",
    "\n",
    "    # Add column for updating mask with updated indexes\n",
    "    if iteration_number == 1:\n",
    "        df['updated'] = np.where(df.index.isin(updatedIndices), 1, 0)\n",
    "\n",
    "    # Compute total number of subgraphs\n",
    "    total_graphs = len(lat_positions) * len(lon_positions)\n",
    "    print(f\"[INFO] {current_time()} Will generate {total_graphs} subgraphs ({len(lat_positions)} rows x {len(lon_positions)} columns)\")\n",
    "\n",
    "    # for i_index, lat_start in enumerate(lat_positions):\n",
    "    #     for j_index, lon_start in enumerate(lon_positions):\n",
    "    #         save_path = process_subgraph(\n",
    "    #             df, k, p, lat_start, lon_start,\n",
    "    #             lat_sizes[i_index], lon_sizes[j_index], itr\n",
    "    #         )\n",
    "    #         print(f\"[INFO] {current_time()} Saved subgraph to {save_path}\")\n",
    "\n",
    "    # Define thread pool and process data\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for j_index, lon_start in enumerate(lon_positions):\n",
    "            futures = []\n",
    "            for i_index, lat_start in enumerate(lat_positions):\n",
    "                if iteration_number == 1:\n",
    "                    if updatedSubregions[i_index][j_index] == 0:\n",
    "                        print(f\"Skipping subgraph {i_index} {j_index} for Region {subregion_row}{subregion_col}: No updated entries\")\n",
    "                        continue\n",
    "                # Submit each task to the thread pool\n",
    "                print(f\"[INFO] {current_time()} Generating subgraph {i_index} {j_index} for Region {subregion_row} {subregion_col}\")\n",
    "                futures.append(executor.submit(process_subgraph, df, subregion_row, subregion_col, lat_start, lon_start, lat_sizes[i_index], lon_sizes[j_index], iteration_number))\n",
    "\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "    # os.makedirs('density', exist_ok=True)\n",
    "    # with open(f'density/OccupancyMatrix_{k}_{p}.pkl', 'wb') as f:\n",
    "    #     pickle.dump(OccupancyMatrix, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZID676PM38D"
   },
   "source": [
    "# Part 4: Train all Subgraphs\n",
    "This exposes a function (Part4), which given any i, j, iteration_number will load and train the combined model in an advanced mini batch fashion on all the 78 subgraphs and also delete those now redundant subgraphs and update the csv file with the calculated abundances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "I29CkDFGFvcH"
   },
   "outputs": [],
   "source": [
    "num_targets = len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fCxh0y0rFvZO"
   },
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads = 1, edge_dim = 1)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, out_channels, heads = 1, edge_dim = 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "KQjQrfvzFvWW"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, out_channels, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "azprra6NFvRs"
   },
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, gnn_in_channels, gnn_hidden_channels, gnn_out_channels,\n",
    "        cnn_in_channels, cnn_out_channels, fusion_hidden_channels, num_elements\n",
    "    ):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.gnn = GNNModel(gnn_in_channels, gnn_hidden_channels, gnn_out_channels)\n",
    "        self.cnn = CNNModel(cnn_in_channels, cnn_out_channels)\n",
    "        self.fc1 = nn.Linear(gnn_out_channels + cnn_out_channels, fusion_hidden_channels)\n",
    "        self.fc2 = nn.Linear(fusion_hidden_channels, num_elements)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # GNN forward\n",
    "        x_gnn = self.gnn(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "        grid_size = (100, 100)\n",
    "        grid = torch.zeros((1, data.x.size(1), grid_size[0], grid_size[1]), device = data.x.device)\n",
    "        coords = data.x[:, -2:]\n",
    "        latitudes, longitudes = coords[:, 0], coords[:, 1]\n",
    "\n",
    "        lat_indices = ((latitudes - latitudes.max()) / (latitudes.min() - latitudes.max()) * (grid_size[0] - 1)).int()\n",
    "        lon_indices = ((longitudes - longitudes.min()) / (longitudes.max() - longitudes.min()) * (grid_size[1] - 1)).int()\n",
    "\n",
    "        lat_indices = torch.clamp(lat_indices, 0, grid_size[0] - 1)\n",
    "        lon_indices = torch.clamp(lon_indices, 0, grid_size[1] - 1)\n",
    "\n",
    "        grid[0, :, lat_indices, lon_indices] = data.x.t()\n",
    "\n",
    "        # Pass through CNN\n",
    "        x_cnn = self.cnn(grid)\n",
    "\n",
    "        # Compute lat_indices and lon_indices directly for the reduced grid\n",
    "        lat_indices = ((latitudes - latitudes.max()) / (latitudes.min() - latitudes.max()) * (x_cnn.shape[2] - 1)).int()\n",
    "        lon_indices = ((longitudes - longitudes.min()) / (longitudes.max() - longitudes.min()) * (x_cnn.shape[3] - 1)).int()\n",
    "\n",
    "        # Clamp the indices to ensure they are within valid bounds\n",
    "        lat_indices = torch.clamp(lat_indices, 0, x_cnn.shape[2] - 1)\n",
    "        lon_indices = torch.clamp(lon_indices, 0, x_cnn.shape[3] - 1)\n",
    "\n",
    "        x_cnn = x_cnn[0, :, lat_indices, lon_indices].t()\n",
    "\n",
    "        # Combine GNN and CNN outputs\n",
    "        x_combined = torch.cat((x_gnn, x_cnn), dim=1)\n",
    "        x = F.relu(self.fc1(x_combined))\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "cB9iqqGiQljv"
   },
   "outputs": [],
   "source": [
    "# TASK 4: Loss Function\n",
    "mse = nn.MSELoss(reduction='none')\n",
    "def masked_mse_loss(predictions, targets, mask, element_weights):\n",
    "    \"\"\"\n",
    "    Computes Weighted MSE loss only where the mask is True.\n",
    "\n",
    "    Args:\n",
    "        predictions: Predicted values of shape (num_nodes, 8).\n",
    "        targets: Ground truth values of shape (num_nodes, 8).\n",
    "        mask: Boolean mask indicating known values of shape (num_nodes, 8).\n",
    "\n",
    "    Returns:\n",
    "        Weighted MSE loss computed only for the known (masked) values.\n",
    "    \"\"\"\n",
    "    loss = mse(predictions, targets)*element_weights\n",
    "    masked_loss = loss * mask.unsqueeze(1)  # Apply mask\n",
    "    return masked_loss.sum() / mask.sum()\n",
    "\n",
    "def feature_similarity_loss(predictions, features, edge_index):\n",
    "    \"\"\"\n",
    "    Penalizes abundance differences for nodes with similar features.\n",
    "\n",
    "    Args:\n",
    "        predictions: Predicted abundances (num_nodes, num_elements).\n",
    "        features: Node feature vectors (num_nodes, feature_dim).\n",
    "        edge_index: Edge indices (2, num_edges).\n",
    "\n",
    "    Returns:\n",
    "        Feature similarity loss.\n",
    "    \"\"\"\n",
    "    src, dest = edge_index  # Source and destination nodes\n",
    "    diff_predictions = predictions[src] - predictions[dest]  # Abundance differences\n",
    "    diff_features = features[src] - features[dest]  # Feature differences\n",
    "    # normalized_diff_feature = (diff_features - diff_features.min()) / (diff_features.max() - diff_features.min() + 1e-8)  # Avoid division by zero\n",
    "\n",
    "    # Normalize per edge, not globally across the whole tensor\n",
    "    diff_features_norm = torch.norm(diff_features, dim=1, keepdim=True)\n",
    "    normalized_diff_feature = diff_features / (diff_features_norm + 1e-8)\n",
    "\n",
    "    weights = torch.exp(-torch.norm(normalized_diff_feature, dim=1))  # Similarity weight (higher for similar features)\n",
    "    return (weights.unsqueeze(1) * diff_predictions**2).mean()\n",
    "\n",
    "def logarithmic_loss(predictions, targets, mask, element_weights):\n",
    "    \"\"\"\n",
    "    Computes the logarithmic loss with masking and element-wise weighting.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions (torch.Tensor): Predicted values (batch_size x num_elements).\n",
    "    - targets (torch.Tensor): True values (batch_size x num_elements).\n",
    "    - mask (torch.Tensor): Binary mask (batch_size,) to indicate valid samples.\n",
    "    - element_weights (torch.Tensor, optional): Weights for each element (num_elements,).\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Computed loss (scalar).\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    log_diff = (torch.log(predictions + epsilon) - torch.log(targets + epsilon))\n",
    "    log_loss = (log_diff ** 2) * element_weights  # Square the difference\n",
    "    masked_log_loss = log_loss*mask.unsqueeze(1)\n",
    "    return (masked_log_loss.sum()) / (mask.sum() + epsilon)    \n",
    "\n",
    "def spatial_similarity_loss(predictions, edge_index, lat_lon):\n",
    "    \"\"\"\n",
    "    Penalizes abundance differences for spatially close nodes using Euclidean distance.\n",
    "\n",
    "    Args:\n",
    "        predictions: Predicted abundances (num_nodes, num_elements).\n",
    "        edge_index: Edge indices (2, num_edges).\n",
    "        lat_lon: Tensor containing latitudes and longitudes for each node (num_nodes, 2).\n",
    "\n",
    "    Returns:\n",
    "        Spatial similarity loss.\n",
    "    \"\"\"\n",
    "\n",
    "    src, dest = edge_index  # Source and destination nodes\n",
    "    lat_lon_src = lat_lon[src]  # Get latitudes and longitudes for source nodes\n",
    "    lat_lon_dest = lat_lon[dest]  # Get latitudes and longitudes for destination nodes\n",
    "\n",
    "    distance = torch.norm(lat_lon_src - lat_lon_dest, dim=1)\n",
    "    normalized_distance = (distance - distance.min()) / (distance.max() - distance.min() + 1e-8)  # Avoid division by zero\n",
    "\n",
    "    diff_predictions = predictions[src] - predictions[dest]  # (num_edges, num_elements)\n",
    "    weights = torch.exp(-normalized_distance)\n",
    "    spatial_loss = (weights.unsqueeze(1) * diff_predictions**2).mean()  # (num_edges, num_elements)\n",
    "\n",
    "    return spatial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hOMJFtDbQlhE"
   },
   "outputs": [],
   "source": [
    "def combined_loss(mode, predictions, targets, mask, w_features, edge_index, element_weights, lat_lon, metadata, lambda_reg=0.1):\n",
    "    # Calculated only for known points for subgraphs where mask is present\n",
    "    # subregion_row_loss = metadata['subregion_indices']['row']\n",
    "    # subregion_col_loss = metadata['subregion_indices']['col']\n",
    "    # subregion_srow = metadata['subregion_indices']['subgraph_row']\n",
    "    # subregion_scol = metadata['subregion_indices']['subgraph_col']\n",
    "\n",
    "    if mode == 1:\n",
    "        mse_loss = masked_mse_loss(predictions, targets, mask, element_weights)\n",
    "        # log_loss = logarithmic_loss(predictions, targets, mask, element_weights)\n",
    "\n",
    "        return mse_loss\n",
    "    elif mode == 2:\n",
    "        feature_sim_loss = feature_similarity_loss(predictions, w_features, edge_index)\n",
    "        spatial_loss = spatial_similarity_loss(predictions, edge_index, lat_lon)\n",
    "        mse_loss = masked_mse_loss(predictions, targets, mask, element_weights)\n",
    "        log_loss = logarithmic_loss(predictions, targets, mask, element_weights)\n",
    "\n",
    "        return mse_loss + lambda_reg*log_loss + feature_sim_loss + spatial_loss\n",
    "    elif mode == 3:\n",
    "        feature_sim_loss = feature_similarity_loss(predictions, w_features, edge_index)\n",
    "        spatial_loss = spatial_similarity_loss(predictions, edge_index, lat_lon)\n",
    "    \n",
    "        return feature_sim_loss + spatial_loss\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # # Calculated for all points\n",
    "    # feature_sim_loss = feature_similarity_loss(predictions, w_features, edge_index)\n",
    "    # spatial_loss = spatial_similarity_loss(predictions, edge_index, lat_lon)\n",
    "\n",
    "    # if os.path.isfile(f\"./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row_loss}_{subregion_col_loss}/mask_tensor_{subregion_row_loss}_{subregion_col_loss}_{subregion_srow}_{subregion_scol}.pt\"):\n",
    "    #     mse_loss = masked_mse_loss(predictions, targets, mask, element_weights)\n",
    "    #     log_loss = logarithmic_loss(predictions, targets, mask, element_weights)\n",
    "    #     print(feature_sim_loss.item(), spatial_loss.item(), mse_loss.item(), log_loss.item())\n",
    "    #     return mse_loss + lambda_reg*log_loss + lambda_reg*feature_sim_loss + lambda_reg*spatial_loss\n",
    "    #     # return mse_loss + lambda_reg*log_loss\n",
    "    \n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=20):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "\n",
    "    def early_stop(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        elif self.best_loss > loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nSCimuLQleS"
   },
   "outputs": [],
   "source": [
    "def save_model(epoch, checkpoint_dir, model):\n",
    "    model_save_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"[INFO] {current_time()} Model saved at {model_save_path}\")\n",
    "\n",
    "def train_combined(data, model, optimizer, epochs, element_weights, lr):\n",
    "    model.train()\n",
    "    torch.backends.cudnn.benchmark = True  # For consistent GPU performance\n",
    "    torch.backends.cudnn.deterministic = False  # Slightly faster, less reproducible\n",
    "    early = EarlyStopper()\n",
    "\n",
    "    # I will use a dual training approach,\n",
    "    # if the mask file is present, i.e. the subregion has some known enteries\n",
    "    # Then initially I am only going to optimise the mse loss (mode = 1)\n",
    "    # After which I am going to train on all the 4 losses (mode = 2)\n",
    "    # If no mask is present, then trained only using the first 2 losses (mode = 3)\n",
    "    # Thus the loss calculation has a parameter mode = (1, 2, 3) and returns a list of Pytorch loss objects\n",
    "\n",
    "    subregion_row_loss = data.metadata['subregion_indices']['row']\n",
    "    subregion_col_loss = data.metadata['subregion_indices']['col']\n",
    "    subregion_srow = data.metadata['subregion_indices']['subgraph_row']\n",
    "    subregion_scol = data.metadata['subregion_indices']['subgraph_col']\n",
    "    totalEpochs = int(epochs*(3/2))\n",
    "\n",
    "    if os.path.isfile(f\"./drive/MyDrive/ISRO_SuperResolution/masks/masks_subregion_{subregion_row_loss}_{subregion_col_loss}/mask_tensor_{subregion_row_loss}_{subregion_col_loss}_{subregion_srow}_{subregion_scol}.pt\"):\n",
    "        # Mask is present, train for epochs using mse loss\n",
    "        with tqdm(range(totalEpochs), desc=\"Training... (mask +nt)\") as pbar:\n",
    "            mode = 1\n",
    "            scheduler = lr_scheduler.CosineAnnealingLR(optimizer, ((totalEpochs//2)+1), eta_min=0.0001)\n",
    "\n",
    "            for epoch in range(totalEpochs//2):\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                output = model(data)\n",
    "\n",
    "                loss = combined_loss(mode, output, data.y, data.mask, data.x[:, :300], data.edge_index, element_weights, data.x[:, -2:], data.metadata)\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.set_postfix_str(f\"{loss.item():4f}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "                if early.early_stop(loss.item()):\n",
    "                    break\n",
    "\n",
    "                if epoch != (epochs - 2):\n",
    "                    scheduler.step()\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            mode = 2\n",
    "            scheduler = lr_scheduler.CosineAnnealingLR(optimizer, (totalEpochs - (epochs//2) + 1), eta_min=0.0001)\n",
    "            \n",
    "            for epoch in range(totalEpochs - (totalEpochs//2)):\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                output = model(data)\n",
    "\n",
    "                loss = combined_loss(mode, output, data.y, data.mask, data.x[:, :300], data.edge_index, element_weights, data.x[:, -2:], data.metadata)\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.set_postfix_str(f\"{loss.item():4f}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "                if epoch != (totalEpochs-2):\n",
    "                    scheduler.step()\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "    else:\n",
    "        mode = 3\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, (epochs + 1), eta_min=0.0001)\n",
    "\n",
    "        with tqdm(range(epochs), desc=\"Training... (mask -nt)\") as pbar:\n",
    "            for epoch in range(epochs):\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                output = model(data)\n",
    "\n",
    "                loss = combined_loss(mode, output, data.y, data.mask, data.x[:, :300], data.edge_index, element_weights, data.x[:, -2:], data.metadata)\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.set_postfix_str(f\"{loss.item():4f}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "                if early.early_stop(loss.item()):\n",
    "                    break\n",
    "\n",
    "                if epoch != (epochs-2):\n",
    "                    scheduler.step()\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    #     # print(f'[INFO] Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "CmjUzj9CRP18"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gnn_in, gnn_hidden, gnn_out = 303, 128, 36\n",
    "cnn_in, cnn_out = 303, 36\n",
    "fusion_hidden, num_targets = 64, num_targets\n",
    "# num_epochs = 100\n",
    "element_weights = torch.tensor([2, 1, 1, 3, 3, 3, 1, 1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "O4LoGhnBRPy5"
   },
   "outputs": [],
   "source": [
    "def train_graph(graph_data, model, optimizer, num_epochs = 100, lr = 0.001):\n",
    "    graph_data = graph_data.to(device)\n",
    "    train_combined(data=graph_data, model=model, optimizer=optimizer, epochs=num_epochs, element_weights= element_weights, lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "pTVH6ojNRPv4"
   },
   "outputs": [],
   "source": [
    "def find_grid_indices_in_subregion2(lat, lon, subregion_row, subregion_col):\n",
    "    \"\"\"\n",
    "    Calculate the grid indices for a given latitude and longitude within a subregion.\n",
    "\n",
    "    Parameters:\n",
    "    - lat: Latitude of the point.\n",
    "    - lon: Longitude of the point.\n",
    "    - subregion_row: Row index of the subregion.\n",
    "    - subregion_col: Column index of the subregion.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple (square_row, square_col) representing the grid indices.\n",
    "    \"\"\"\n",
    "    # Calculate the start of the subregion\n",
    "    subregion_lat_start = LATITUDE_RANGE[0] - subregion_row * lat_per_region\n",
    "    subregion_lon_start = LONGITUDE_RANGE[0] + subregion_col * lon_per_region\n",
    "\n",
    "    # Calculate the grid square within the subregion\n",
    "    square_row = abs(int((lat - subregion_lat_start) // square_size_deg))\n",
    "    square_col = abs(int((lon - subregion_lon_start) // square_size_deg))\n",
    "\n",
    "    if not (0 <= square_row < num_squares_lat) or not (0 <= square_col < num_squares_lon):\n",
    "        # If point is out of the subregion's grid, find the nearest grid point\n",
    "        square_row = max(0, min(square_row, num_squares_lat - 1))\n",
    "        square_col = max(0, min(square_col, num_squares_lon - 1))\n",
    "\n",
    "    # Calculate the exact row index\n",
    "    row_index = square_row * num_squares_lon + square_col\n",
    "\n",
    "    return row_index\n",
    "\n",
    "def find_grid_indices_in_subregion2_vectorized(latitudes, longitudes, subregion_row, subregion_col):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of grid indices for given latitudes and longitudes within a subregion using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    - latitudes: Numpy Array of latitudes.\n",
    "    - longitudes: Numpy Array of longitudes.\n",
    "    - subregion_row: Row index of the subregion.\n",
    "    - subregion_col: Column index of the subregion.\n",
    "\n",
    "    Returns:\n",
    "    - Numpy Array of row indices for the grid positions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the start of the subregion\n",
    "    subregion_lat_start = LATITUDE_RANGE[0] - subregion_row * lat_per_region\n",
    "    subregion_lon_start = LONGITUDE_RANGE[0] + subregion_col * lon_per_region\n",
    "\n",
    "    # # Calculate the grid squares within the subregion\n",
    "    # square_rows = torch.abs(((latitudes - subregion_lat_start) // square_size_deg).to(torch.int))\n",
    "    # square_cols = torch.abs(((longitudes - subregion_lon_start) // square_size_deg).to(torch.int))\n",
    "\n",
    "    # Calculate the grid squares within the subregion\n",
    "    square_rows = np.abs(((latitudes - subregion_lat_start) // square_size_deg).astype(int))\n",
    "    square_cols = np.abs(((longitudes - subregion_lon_start) // square_size_deg).astype(int))\n",
    "\n",
    "    # # Clamp indices to valid grid ranges\n",
    "    # square_rows = torch.clamp(square_rows, 0, num_squares_lat - 1)\n",
    "    # square_cols = torch.clamp(square_cols, 0, num_squares_lon - 1)\n",
    "\n",
    "    # Clamp indices to valid grid ranges\n",
    "    square_rows = np.clip(square_rows, 0, num_squares_lat - 1)\n",
    "    square_cols = np.clip(square_cols, 0, num_squares_lon - 1)\n",
    "\n",
    "    # Calculate the exact row indices\n",
    "    row_indices = square_rows * num_squares_lon + square_cols\n",
    "\n",
    "    return row_indices\n",
    "\n",
    "def save_predictions(d, o, graph_data, graph_output):\n",
    "    \"\"\"\n",
    "    Optimized save_predictions function using vectorized grid index computation.\n",
    "    This function takes as argument a graph_data object and iterates through all rows of the data.\n",
    "    It uses the index to get the abundance vector from the model output on the graph_data.\n",
    "    It then figures out the index position in the csv file\n",
    "    Then saves the data if the csv file is initially empty\n",
    "    Then closes the file at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the CSV once\n",
    "    df_path = f'./regions/ISRO_RegionData{d - d%2}{1 + d - d%2}/subregion_{d}_{o}.csv'\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    metadata = graph_data.metadata\n",
    "    subregion_row = metadata['subregion_indices']['row']\n",
    "    subregion_col = metadata['subregion_indices']['col']\n",
    "    w = 0.25\n",
    "\n",
    "    assert subregion_row == d, \"[ERROR] Metadata row does not match d\"\n",
    "    assert subregion_col == o, \"[ERROR] Metadata col does not match o\"\n",
    "\n",
    "    # Extract latitudes, longitudes, mask, and predictions as numpy arrays\n",
    "    latitudes = graph_data.x[:, -2].cpu().numpy()\n",
    "    longitudes = graph_data.x[:, -1].cpu().numpy()\n",
    "    masks = graph_data.mask.cpu().numpy()\n",
    "    predictions = graph_output.cpu().numpy()\n",
    "\n",
    "    # Compute grid indices using the vectorized function\n",
    "    df_indices = find_grid_indices_in_subregion2_vectorized(\n",
    "        latitudes, \n",
    "        longitudes, \n",
    "        subregion_row, \n",
    "        subregion_col\n",
    "    )\n",
    "\n",
    "    # Filter valid rows (where data needs to be imputed) where mask is 0\n",
    "    valid_rows = masks == 0\n",
    "    df_indices = df_indices[valid_rows]\n",
    "    predictions = predictions[valid_rows]\n",
    "\n",
    "    # Extract the relevant rows from the DataFrame for processing\n",
    "    df_elements = df.loc[df_indices, elements].values\n",
    "\n",
    "    # Create a mask to identify rows with non-zero elements\n",
    "    nonzero_mask = (df_elements != 0).any(axis=1)\n",
    "\n",
    "    # Weighted update for rows with non-zero elements\n",
    "    df_elements[nonzero_mask] = (w * df_elements[nonzero_mask]) + ((1 - w) * predictions[nonzero_mask])\n",
    "\n",
    "    # Direct update for rows with all-zero elements\n",
    "    df_elements[~nonzero_mask] = predictions[~nonzero_mask]\n",
    "\n",
    "    # Write the updated values back to the DataFrame\n",
    "    df.loc[df_indices, elements] = df_elements\n",
    "\n",
    "    # # Apply updates to the DataFrame\n",
    "    # for idx, prediction in zip(df_indices.tolist(), predictions.cpu().tolist()):\n",
    "    #     if (df.loc[idx, elements] != 0).any():\n",
    "    #         # Weighted update\n",
    "    #         df.loc[idx, elements] = (w * df.loc[idx, elements]) + ((1 - w) * prediction)\n",
    "    #     else:\n",
    "    #         # Direct update\n",
    "    #         df.loc[idx, elements] = prediction\n",
    "\n",
    "    # Save the updated DataFrame\n",
    "    df.to_csv(df_path, index=False)\n",
    "    print(f\"[INFO] {current_time()} Saved predictions to {df_path} for subregion {graph_data.metadata[\"subregion_indices\"][\"subgraph_row\"]} {graph_data.metadata[\"subregion_indices\"][\"subgraph_col\"]}\")\n",
    "\n",
    "\n",
    "Part4executor = ThreadPoolExecutor(max_workers=1)  # Adjust max_workers based on available resources\n",
    "\n",
    "def Part4(x, y, iteration_number):\n",
    "    pattern = r\"subgraph_(\\d+)_(\\d+)_(\\d+)_(\\d+)\\.pt\"\n",
    "\n",
    "    # I need to iterate through all the x, y files in the directory graphs/subregion_i_j/subgraph_i_j_x_y.pt\n",
    "    directory = f\"./graphs/graphs_subregion_{x}_{y}/\"\n",
    "\n",
    "    # Directory to save the model checkpoints\n",
    "    # checkpoint_dir = './model_checkpoints'\n",
    "    # os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    # os.makedirs(f'{checkpoint_dir}/subregion_{x}_{y}/', exist_ok=True)\n",
    "    # os.makedirs(f'{checkpoint_dir}/subregion_{x}_{y}/{iteration_number}/', exist_ok=True)\n",
    "\n",
    "    model = CombinedModel(gnn_in, gnn_hidden, gnn_out, cnn_in, cnn_out, fusion_hidden, num_targets).to(device)\n",
    "\n",
    "    os.makedirs(f'./drive/MyDrive/ISRO_SuperResolution/models', exist_ok=True)\n",
    "    if os.path.isfile(f'./drive/MyDrive/ISRO_SuperResolution/models/{x}_{y}.pth'):\n",
    "        model.load_state_dict(torch.load(f\"./drive/MyDrive/ISRO_SuperResolution/models/{x}_{y}.pth\"))\n",
    "        num_epochs = 100 # Only need to finetune later\n",
    "        lr = 0.0005\n",
    "    else:\n",
    "        num_epochs = 200\n",
    "        lr = 0.001\n",
    "\n",
    "    # optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    optimizer = optim.Yogi(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "    # # Load initial occupancy matrix\n",
    "    # pickle_file_path = f'density/OccupancyMatrix_{x}_{y}.pkl'  # Change this to your file path\n",
    "    # with open(pickle_file_path, 'rb') as f:\n",
    "    #     Occupancy = pickle.load(f)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pt\"):\n",
    "            torch.cuda.empty_cache()\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Extract i, j, x, and y using the regular expression\n",
    "            match = re.match(pattern, filename)\n",
    "            if not match:\n",
    "                print(f\"[ERROR] Filename {filename} does not match expected pattern. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            i2, j2, x1, y1 = map(int, match.groups())\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            i2 = int(i2)\n",
    "            j2 = int(j2)\n",
    "            del match\n",
    "\n",
    "            print(f\"[INFO] {current_time()} Starting\", filename)\n",
    "\n",
    "            assert i2 == x, \"[ERROR] File Number row index do not match\"\n",
    "            assert j2 == y, \"[ERROR] File Number col index do not match\"\n",
    "\n",
    "            # Now I need to do the following, if iteration number is 1. Then I need to ignore those subgraphs (x, y)\n",
    "            # s.t. the amount of data in that subgraph is less than 10%\n",
    "            # This data is stored in the following pickle file: ./density/OccupanyMatrix_i_j.pkl\n",
    "            # which is loaded in the Occupancy Variable\n",
    "            # # A-S Logic\n",
    "            # if iteration_number == 1:\n",
    "            #     if(Occupancy[0, 0] < 10):\n",
    "            #         continue\n",
    "\n",
    "            # save_directory = f'{checkpoint_dir}/subregion_{x}_{y}/{iteration_number}/{x1}_{y1}/'\n",
    "            # os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "            graph_data = torch.load(file_path)\n",
    "\n",
    "            # For each data object file I need to train the Graph using that data\n",
    "            train_graph(graph_data, model, optimizer, num_epochs, lr)\n",
    "\n",
    "            print(f\"[INFO] {current_time()} Trained model on {filename}. Now saving predictions.\")\n",
    "\n",
    "            # # Finally for that graph I need to open the region csv file and place where data is not avalaible\n",
    "            # save_predictions(x, y, model, graph_data)\n",
    "\n",
    "            # Compute the output on the CPU and pass it to save_predictions\n",
    "            graph_output = model(graph_data).cpu().detach()\n",
    "            _ = Part4executor.submit(save_predictions, x, y, graph_data, graph_output)\n",
    "\n",
    "            os.remove(file_path)\n",
    "\n",
    "            print(f\"[INFO] {current_time()} Processed and deleted {filename}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.save(model.state_dict(), f\"./drive/MyDrive/ISRO_SuperResolution/models/{x}_{y}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n1Foo4ZWgMi"
   },
   "source": [
    "# Part 5: Final\n",
    "This part uses Part3 and Part4 function in an iterative loop to get the final abundances csv. It also does some final processing to make it easier for mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ptV6ObVmQgXl"
   },
   "outputs": [],
   "source": [
    "def HandleRegion(i, j, num_iterations, updatedIndices, updatedSubregions):\n",
    "  \"\"\"\n",
    "  This function is responsible for handling all update compuations of a region (out of the 64 regions) indexed by i, j\n",
    "  \"\"\"\n",
    "  for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"[INFO] {current_time()} Starting Iteration {iteration} for region {i} {j}\")\n",
    "\n",
    "    print(f\"[INFO] {current_time()} Running PART 3 (iter: {iteration})\")\n",
    "    Part3(i, j, iteration, updatedIndices, updatedSubregions) # Note that updatedIndices is a list of indices in the csv\n",
    "    # updatedSubregions is a 6x13 matrix of number of enteries added in each subregion\n",
    "\n",
    "    print(f\"[INFO] {current_time()} Running PART 4 (iter: {iteration})\")\n",
    "    Part4(i, j, iteration)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "toProcess = [(1,3), (2,3), (3,3), (2,4), (4,3), (3,4)]\n",
    "\n",
    "# Final function\n",
    "def ProcessDataP1(file_name, num_iteration):\n",
    "  \"\"\"\n",
    "  This will implement the dynamic nature required\n",
    "  Arguments:\n",
    "    - df:\n",
    "        - rows: observations\n",
    "        - headers: lat0, lon0, lat1, lon1, lat2, lon2, lat3, lon3, 'Fe', 'Ti', 'Ca', 'Si', 'Al', 'Mg', 'Na', 'O'\n",
    "    - num_iterations:\n",
    "        - Number of iterations to finetune the output on\n",
    "\n",
    "  Functionality:\n",
    "    - Updates the mapping csv file with the new data\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "  Directory Structure Assumptions:\n",
    "    - root\n",
    "        | - regions/subregion_i_j.csv, for all i, j in [1 ... 8]: This is the global information bank which is continously updated\n",
    "        | - LROC_GLOBAL_MARE_180.DBF\n",
    "        | - LROC_GLOBAL_MARE_180.PRJ\n",
    "        | - LROC_GLOBAL_MARE_180.SHP\n",
    "        | - LROC_GLOBAL_MARE_180.SHP.XML\n",
    "        | - LROC_GLOBAL_MARE_180.SHX\n",
    "        | - LROC_GLOBAL_MARE_README.TXT\n",
    "  \"\"\"\n",
    "\n",
    "  # First thing it will do is call PART2 function with arguments: dataframe\n",
    "  # PART2 will update CSV files with the new enteries\n",
    "  # and return number of new rows in each region (64 regions)\n",
    "  # and for each region number of new data points in each subgraph\n",
    "  # and also the row indices of the subregion_i_j.csv file where values are updated\n",
    "  print(f\"[INFO] {current_time()} Starting...\")\n",
    "  print(f\"[INFO] {current_time()} Calling Part2 to update GIB\")\n",
    "  updatedRegions, updatedSubregions, indices = Part2(file_name)\n",
    "\n",
    "  np.savez_compressed(f'PART2Output.npz', \n",
    "                      updatedSubregions=updatedSubregions, \n",
    "                      updatedRegions=updatedRegions, \n",
    "                      indices=indices)\n",
    "  print(f\"[INFO] {current_time()} Successfully updated GIB. UpdatedRegions: {updatedRegions}, UpdatedSubregions shape: {updatedSubregions.shape}, UpdatedIndices shape: {indices.shape}\")\n",
    "\n",
    "  return updatedRegions, updatedSubregions, indices\n",
    "\n",
    "def ProcessDataP2(num_iteration, updatedRegions, updatedSubregions, indices):\n",
    "  with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    for a in range(8):\n",
    "      for b in range(8):\n",
    "        if (updatedRegions[a][b] > 0) and ((a, b) in toProcess):\n",
    "          print(f\"[INFO] {current_time()} Starting thread for {a} {b}\")\n",
    "          executor.submit(HandleRegion, a, b, num_iteration, indices[a][b], updatedSubregions[a][b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OWuoBskVhch",
    "outputId": "dcfaba9f-e20d-440f-9186-2d5217f94fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting...\n",
      "[INFO] Calling Part2 to update GIB\n",
      "[INFO] Processing file: blablabla.csv\n",
      "    lat0    lon0   lat1    lon1   lat2    lon2   lat3    lon3    Fe  Ti  Ca  \\\n",
      "0 -44.76  120.40 -50.51  120.50 -50.49  121.91 -44.74  121.66  3.64   0   0   \n",
      "1 -54.45  120.57 -60.20  120.72 -60.17  122.53 -54.43  122.12  3.04   0   0   \n",
      "2 -59.29  120.69 -65.04  120.88 -65.01  123.02 -59.27  122.45  3.13   0   0   \n",
      "3  49.72  116.61  44.01  116.85  44.02  117.99  49.74  117.88  3.24   0   0   \n",
      "4  44.83  116.82  39.11  117.01  39.13  118.07  44.85  117.97  3.63   0   0   \n",
      "\n",
      "      Si     Al    Mg     Na   O  \n",
      "0  18.02  15.33  4.09  1.375  45  \n",
      "1  17.80  17.35  3.71  1.375  45  \n",
      "2  17.70  16.03  4.81  1.375  45  \n",
      "3  18.92  17.45  5.09  1.375  45  \n",
      "4  17.12  16.76  6.40  1.375  45  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating abundances: 100%|██████████| 2722/2722 [10:34<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subregion '(6, 6)' has 213 entries in batch 0\n",
      "Subregion '(1, 6)' has 196 entries in batch 0\n",
      "Subregion '(2, 6)' has 249 entries in batch 0\n",
      "Subregion '(3, 6)' has 227 entries in batch 0\n",
      "Subregion '(4, 6)' has 214 entries in batch 0\n",
      "Subregion '(3, 1)' has 412 entries in batch 0\n",
      "Subregion '(4, 1)' has 343 entries in batch 0\n",
      "Subregion '(5, 1)' has 280 entries in batch 0\n",
      "Subregion '(2, 1)' has 430 entries in batch 0\n",
      "Subregion '(1, 1)' has 559 entries in batch 0\n",
      "Subregion '(5, 6)' has 266 entries in batch 0\n",
      "Subregion '(5, 5)' has 353 entries in batch 0\n",
      "Subregion '(3, 5)' has 593 entries in batch 0\n",
      "Subregion '(2, 5)' has 566 entries in batch 0\n",
      "Subregion '(7, 4)' has 149 entries in batch 0\n",
      "Subregion '(7, 3)' has 195 entries in batch 0\n",
      "Subregion '(3, 4)' has 700 entries in batch 0\n",
      "Subregion '(4, 4)' has 588 entries in batch 0\n",
      "Subregion '(2, 4)' has 715 entries in batch 0\n",
      "Subregion '(1, 4)' has 592 entries in batch 0\n",
      "Subregion '(0, 4)' has 296 entries in batch 0\n",
      "Subregion '(7, 7)' has 109 entries in batch 0\n",
      "Subregion '(6, 7)' has 201 entries in batch 0\n",
      "Subregion '(5, 7)' has 190 entries in batch 0\n",
      "Subregion '(4, 7)' has 102 entries in batch 0\n",
      "Subregion '(3, 7)' has 87 entries in batch 0\n",
      "Subregion '(2, 7)' has 115 entries in batch 0\n",
      "Subregion '(1, 7)' has 132 entries in batch 0\n",
      "Subregion '(0, 7)' has 45 entries in batch 0\n",
      "Subregion '(7, 5)' has 175 entries in batch 0\n",
      "Subregion '(6, 5)' has 283 entries in batch 0\n",
      "Subregion '(0, 6)' has 117 entries in batch 0\n",
      "Subregion '(4, 5)' has 492 entries in batch 0\n",
      "Subregion '(1, 5)' has 362 entries in batch 0\n",
      "Subregion '(0, 5)' has 146 entries in batch 0\n",
      "Subregion '(6, 4)' has 319 entries in batch 0\n",
      "Subregion '(5, 4)' has 311 entries in batch 0\n",
      "Subregion '(5, 2)' has 230 entries in batch 0\n",
      "Subregion '(6, 2)' has 251 entries in batch 0\n",
      "Subregion '(4, 2)' has 295 entries in batch 0\n",
      "Subregion '(7, 2)' has 120 entries in batch 0\n",
      "Subregion '(0, 2)' has 88 entries in batch 0\n",
      "Subregion '(3, 2)' has 638 entries in batch 0\n",
      "Subregion '(2, 2)' has 652 entries in batch 0\n",
      "Subregion '(1, 2)' has 392 entries in batch 0\n",
      "Subregion '(6, 1)' has 332 entries in batch 0\n",
      "Subregion '(7, 1)' has 101 entries in batch 0\n",
      "Subregion '(0, 1)' has 154 entries in batch 0\n",
      "Subregion '(0, 3)' has 222 entries in batch 0\n",
      "Subregion '(0, 0)' has 53 entries in batch 0\n",
      "Subregion '(1, 0)' has 202 entries in batch 0\n",
      "Subregion '(2, 0)' has 367 entries in batch 0\n",
      "Subregion '(3, 0)' has 258 entries in batch 0\n",
      "Subregion '(6, 0)' has 314 entries in batch 0\n",
      "Subregion '(7, 0)' has 42 entries in batch 0\n",
      "Subregion '(2, 3)' has 1119 entries in batch 0\n",
      "Subregion '(3, 3)' has 1218 entries in batch 0\n",
      "Subregion '(4, 3)' has 1104 entries in batch 0\n",
      "Subregion '(6, 3)' has 439 entries in batch 0\n",
      "Subregion '(5, 3)' has 624 entries in batch 0\n",
      "Subregion '(1, 3)' has 737 entries in batch 0\n",
      "Subregion '(5, 0)' has 264 entries in batch 0\n",
      "Subregion '(4, 0)' has 175 entries in batch 0\n",
      "Subregion '(7, 6)' has 63 entries in batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subregion: (6, 6)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_6_6.csv\n",
      "Processing subregion: (1, 6)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_1_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:   3%|▎         | 2/64 [02:23<1:01:08, 59.17s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_6_6.csv\n",
      "Processing subregion: (2, 6)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_2_6.csv\n",
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_1_6.csv\n",
      "Processing subregion: (3, 6)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_3_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:   5%|▍         | 3/64 [04:46<1:39:02, 97.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_2_6.csv\n",
      "Processing subregion: (4, 6)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_4_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:   6%|▋         | 4/64 [04:47<59:14, 59.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_3_6.csv\n",
      "Processing subregion: (3, 1)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_3_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:   8%|▊         | 5/64 [07:07<1:26:49, 88.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_4_6.csv\n",
      "Processing subregion: (4, 1)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_4_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:   9%|▉         | 6/64 [07:08<56:39, 58.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_3_1.csv\n",
      "Processing subregion: (5, 1)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_5_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  11%|█         | 7/64 [09:37<1:23:53, 88.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_4_1.csv\n",
      "Processing subregion: (2, 1)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_2_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  12%|█▎        | 8/64 [09:38<56:31, 60.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_5_1.csv\n",
      "Processing subregion: (1, 1)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_1_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  14%|█▍        | 9/64 [12:05<1:20:15, 87.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_2_1.csv\n",
      "Processing subregion: (5, 6)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_5_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  16%|█▌        | 10/64 [12:08<55:11, 61.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_1_1.csv\n",
      "Processing subregion: (5, 5)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_5_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  17%|█▋        | 11/64 [14:30<1:16:03, 86.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_5_6.csv\n",
      "Processing subregion: (3, 5)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_3_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  19%|█▉        | 12/64 [14:33<52:42, 60.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_5_5.csv\n",
      "Processing subregion: (2, 5)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_2_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  20%|██        | 13/64 [16:56<1:12:48, 85.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_3_5.csv\n",
      "Processing subregion: (7, 4)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_7_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  22%|██▏       | 14/64 [17:03<51:37, 61.95s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_2_5.csv\n",
      "Processing subregion: (7, 3)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_7_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  23%|██▎       | 15/64 [19:11<1:06:58, 82.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_7_4.csv\n",
      "Processing subregion: (3, 4)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_3_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  25%|██▌       | 16/64 [19:27<49:29, 61.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_7_3.csv\n",
      "Processing subregion: (4, 4)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_4_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  27%|██▋       | 17/64 [21:38<1:04:49, 82.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_3_4.csv\n",
      "Processing subregion: (2, 4)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_2_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  28%|██▊       | 18/64 [22:01<49:45, 64.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_4_4.csv\n",
      "Processing subregion: (1, 4)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_1_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  30%|██▉       | 19/64 [24:11<1:03:17, 84.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_2_4.csv\n",
      "Processing subregion: (0, 4)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_0_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  31%|███▏      | 20/64 [24:29<47:09, 64.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_1_4.csv\n",
      "Processing subregion: (7, 7)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_7_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  33%|███▎      | 21/64 [26:33<59:08, 82.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_0_4.csv\n",
      "Processing subregion: (6, 7)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_6_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  34%|███▍      | 22/64 [26:52<44:19, 63.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_7_7.csv\n",
      "Processing subregion: (5, 7)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_5_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  36%|███▌      | 23/64 [29:26<1:01:51, 90.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_6_7.csv\n",
      "Processing subregion: (4, 7)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_4_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  38%|███▊      | 24/64 [29:54<47:45, 71.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_5_7.csv\n",
      "Processing subregion: (3, 7)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_3_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  39%|███▉      | 25/64 [32:00<57:17, 88.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_4_7.csv\n",
      "Processing subregion: (2, 7)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_2_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  41%|████      | 26/64 [32:14<41:42, 65.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_3_7.csv\n",
      "Processing subregion: (1, 7)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_1_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  42%|████▏     | 27/64 [34:17<51:14, 83.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_2_7.csv\n",
      "Processing subregion: (0, 7)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_0_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  44%|████▍     | 28/64 [34:37<38:23, 64.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_1_7.csv\n",
      "Processing subregion: (7, 5)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_7_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  45%|████▌     | 29/64 [36:54<50:04, 85.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_0_7.csv\n",
      "Processing subregion: (6, 5)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_6_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  47%|████▋     | 30/64 [37:16<37:50, 66.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_7_5.csv\n",
      "Processing subregion: (0, 6)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_0_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  48%|████▊     | 31/64 [39:28<47:29, 86.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_6_5.csv\n",
      "Processing subregion: (4, 5)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_4_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  50%|█████     | 32/64 [39:50<35:47, 67.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_0_6.csv\n",
      "Processing subregion: (1, 5)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_1_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  52%|█████▏    | 33/64 [41:56<43:47, 84.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_4_5.csv\n",
      "Processing subregion: (0, 5)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_0_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  53%|█████▎    | 34/64 [42:20<33:15, 66.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_1_5.csv\n",
      "Processing subregion: (6, 4)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_6_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  55%|█████▍    | 35/64 [44:18<39:35, 81.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_0_5.csv\n",
      "Processing subregion: (5, 4)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_5_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  56%|█████▋    | 36/64 [44:45<30:31, 65.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_6_4.csv\n",
      "Processing subregion: (5, 2)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_5_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  58%|█████▊    | 37/64 [46:39<36:02, 80.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_5_4.csv\n",
      "Processing subregion: (6, 2)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_6_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  59%|█████▉    | 38/64 [47:07<27:53, 64.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_5_2.csv\n",
      "Processing subregion: (4, 2)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_4_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  61%|██████    | 39/64 [49:07<33:47, 81.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_6_2.csv\n",
      "Processing subregion: (7, 2)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_7_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  62%|██████▎   | 40/64 [49:34<25:57, 64.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_4_2.csv\n",
      "Processing subregion: (0, 2)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_0_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  64%|██████▍   | 41/64 [51:24<30:05, 78.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_7_2.csv\n",
      "Processing subregion: (3, 2)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_3_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  66%|██████▌   | 42/64 [51:51<23:07, 63.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_0_2.csv\n",
      "Processing subregion: (2, 2)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_2_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  67%|██████▋   | 43/64 [53:51<28:02, 80.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_3_2.csv\n",
      "Processing subregion: (1, 2)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_1_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  69%|██████▉   | 44/64 [54:25<22:04, 66.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_2_2.csv\n",
      "Processing subregion: (6, 1)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_6_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  70%|███████   | 45/64 [56:14<25:03, 79.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_1_2.csv\n",
      "Processing subregion: (7, 1)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_7_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  72%|███████▏  | 46/64 [56:43<19:14, 64.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_6_1.csv\n",
      "Processing subregion: (0, 1)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_0_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  73%|███████▎  | 47/64 [58:36<22:17, 78.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_7_1.csv\n",
      "Processing subregion: (0, 3)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_0_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  75%|███████▌  | 48/64 [59:02<16:46, 62.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_0_1.csv\n",
      "Processing subregion: (0, 0)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_0_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  77%|███████▋  | 49/64 [1:00:56<19:31, 78.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_0_3.csv\n",
      "Processing subregion: (1, 0)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_1_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  78%|███████▊  | 50/64 [1:01:21<14:31, 62.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_0_0.csv\n",
      "Processing subregion: (2, 0)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_2_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  80%|███████▉  | 51/64 [1:03:16<16:56, 78.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_1_0.csv\n",
      "Processing subregion: (3, 0)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_3_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  81%|████████▏ | 52/64 [1:03:41<12:26, 62.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_2_0.csv\n",
      "Processing subregion: (6, 0)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_6_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  83%|████████▎ | 53/64 [1:05:37<14:22, 78.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_3_0.csv\n",
      "Processing subregion: (7, 0)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_7_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  84%|████████▍ | 54/64 [1:06:03<10:26, 62.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_6_0.csv\n",
      "Processing subregion: (2, 3)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_2_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  86%|████████▌ | 55/64 [1:08:00<11:49, 78.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_7_0.csv\n",
      "Processing subregion: (3, 3)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData23/subregion_3_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  88%|████████▊ | 56/64 [1:08:27<08:27, 63.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_2_3.csv\n",
      "Processing subregion: (4, 3)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_4_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  89%|████████▉ | 57/64 [1:10:34<09:37, 82.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData23/subregion_3_3.csv\n",
      "Processing subregion: (6, 3)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_6_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  91%|█████████ | 58/64 [1:11:02<06:35, 65.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_4_3.csv\n",
      "Processing subregion: (5, 3)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_5_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  92%|█████████▏| 59/64 [1:13:03<06:53, 82.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_6_3.csv\n",
      "Processing subregion: (1, 3)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData01/subregion_1_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  94%|█████████▍| 60/64 [1:13:40<04:34, 68.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_5_3.csv\n",
      "Processing subregion: (5, 0)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_5_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  95%|█████████▌| 61/64 [1:15:31<04:04, 81.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData01/subregion_1_3.csv\n",
      "Processing subregion: (4, 0)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData45/subregion_4_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  97%|█████████▋| 62/64 [1:15:58<02:10, 65.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_5_0.csv\n",
      "Processing subregion: (7, 6)\n",
      "[INFO] Processing file: ./regions/ISRO_RegionData67/subregion_7_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files:  98%|█████████▊| 63/64 [1:17:55<01:20, 80.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData45/subregion_4_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CSV Files: 100%|██████████| 64/64 [1:18:15<00:00, 73.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved updates to file: ./regions/ISRO_RegionData67/subregion_7_6.csv\n",
      "[[  53.  154.   88.  222.  296.  146.  117.   45.]\n",
      " [ 202.  559.  392.  737.  592.  362.  196.  132.]\n",
      " [ 367.  430.  652. 1119.  715.  566.  249.  115.]\n",
      " [ 258.  412.  638. 1218.  700.  593.  227.   87.]\n",
      " [ 175.  343.  295. 1104.  588.  492.  214.  102.]\n",
      " [ 264.  280.  230.  624.  311.  353.  266.  190.]\n",
      " [ 314.  332.  251.  439.  319.  283.  213.  201.]\n",
      " [  42.  101.  120.  195.  149.  175.   63.  109.]]\n",
      "[INFO] Successfully updated GIB. UpdatedRegions: [[  53.  154.   88.  222.  296.  146.  117.   45.]\n",
      " [ 202.  559.  392.  737.  592.  362.  196.  132.]\n",
      " [ 367.  430.  652. 1119.  715.  566.  249.  115.]\n",
      " [ 258.  412.  638. 1218.  700.  593.  227.   87.]\n",
      " [ 175.  343.  295. 1104.  588.  492.  214.  102.]\n",
      " [ 264.  280.  230.  624.  311.  353.  266.  190.]\n",
      " [ 314.  332.  251.  439.  319.  283.  213.  201.]\n",
      " [  42.  101.  120.  195.  149.  175.   63.  109.]], UpdatedSubregions shape: (8, 8, 6, 13), UpdatedIndices shape: (8, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updatedRegions, updatedSubregions, indices = ProcessDataP1('blablabla.csv', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "IAmvN7C5VhZe"
   },
   "outputs": [],
   "source": [
    "# PART 2 is complete\n",
    "from numpy import load as np_load_old\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedRegions = np.load('PART2Output.npz')['updatedRegions']\n",
    "updatedSubregions = np.load('PART2Output.npz')['updatedSubregions']\n",
    "indices = np.load('PART2Output.npz')['indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  88.,  222.,  296.,  146.,  117.,  258.,  412.,  638., 1218.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toProcess = np.array([(0,2), (0,3), (0,4), (0,5), (0,6), (3,0), (3,1), (3,2), (3,3)])\n",
    "updatedRegions[toProcess[:, 0], toProcess[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_16560\\3023201799.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load('./graphs/graphs_subregion_3_3/subgraph_3_3_0_0.pt')\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.load('./graphs/graphs_subregion_3_3/subgraph_3_3_0_0.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 17:18:56 Dataframe Read. Size = 553.427891 MB\n",
      "[INFO] 17:18:56 Will generate 78 subgraphs (6 rows x 13 columns)\n",
      "[INFO] 17:18:56 Generating subgraph 0 0 for Region 3 3\n",
      "[INFO] 17:18:56 Started processing subregion 3 3, subgraph id: 0 0\n",
      "[INFO] 17:18:56 Generating subgraph 1 0 for Region 3 3\n",
      "[INFO] 17:18:56 Started processing subregion 3 3, subgraph id: 1 0\n",
      "[INFO] 17:18:56 Generating subgraph 2 0 for Region 3 3\n",
      "[INFO] 17:18:56 Started processing subregion 3 3, subgraph id: 2 0\n",
      "[INFO] 17:18:56 Generating subgraph 3 0 for Region 3 3\n",
      "[INFO] 17:18:56 Started processing subregion 3 3, subgraph id: 3 0\n",
      "[INFO] 17:18:56 Generating subgraph 4 0 for Region 3 3\n",
      "[INFO] 17:18:56 Started processing subregion 3 3, subgraph id: 4 0\n",
      "[INFO] 17:18:56 Generating subgraph 5 0 for Region 3 3\n",
      "[INFO] 17:18:56 Started processing subregion 3 3, subgraph id: 5 0\n",
      "[INFO] 17:18:57 Created and saved mask for subgraph: 0, 0\n",
      "[INFO] 17:18:57 Created and saved mask for subgraph: 1, 0\n",
      "[INFO] 17:18:57 Created and saved mask for subgraph: 4, 0\n",
      "[INFO] 17:18:57 Created and saved mask for subgraph: 3, 0\n",
      "[INFO] 17:18:57 Created and saved mask for subgraph: 5, 0\n",
      "[INFO] 17:18:57 Created and saved mask for subgraph: 2, 0\n",
      "[INFO] 17:19:03 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_0.pt\n",
      "[INFO] 17:19:03 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_0.pt\n",
      "[INFO] 17:19:03 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_0.pt\n",
      "[INFO] 17:19:03 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_0.pt\n",
      "[INFO] 17:19:03 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_0.pt\n",
      "[INFO] 17:19:03 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_0.pt\n",
      "[INFO] 17:19:03 Generating subgraph 0 1 for Region 3 3\n",
      "[INFO] 17:19:03 Generating subgraph 1 1 for Region 3 3\n",
      "[INFO] 17:19:03 Generating subgraph 2 1 for Region 3 3\n",
      "[INFO] 17:19:03 Generating subgraph 3 1 for Region 3 3\n",
      "[INFO] 17:19:03 Generating subgraph 4 1 for Region 3 3\n",
      "[INFO] 17:19:03 Generating subgraph 5 1 for Region 3 3\n",
      "[INFO] 17:19:03 Started processing subregion 3 3, subgraph id: 0 1\n",
      "[INFO] 17:19:03 Started processing subregion 3 3, subgraph id: 1 1\n",
      "[INFO] 17:19:03 Started processing subregion 3 3, subgraph id: 3 1\n",
      "[INFO] 17:19:03 Started processing subregion 3 3, subgraph id: 2 1\n",
      "[INFO] 17:19:03 Started processing subregion 3 3, subgraph id: 4 1\n",
      "[INFO] 17:19:03 Started processing subregion 3 3, subgraph id: 5 1\n",
      "[INFO] 17:19:03 Created and saved mask for subgraph: 2, 1\n",
      "[INFO] 17:19:03 Created and saved mask for subgraph: 4, 1\n",
      "[INFO] 17:19:03 Created and saved mask for subgraph: 1, 1\n",
      "[INFO] 17:19:03 Created and saved mask for subgraph: 3, 1\n",
      "[INFO] 17:19:03 Created and saved mask for subgraph: 0, 1\n",
      "[INFO] 17:19:03 Created and saved mask for subgraph: 5, 1\n",
      "[INFO] 17:19:08 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_1.pt\n",
      "[INFO] 17:19:08 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_1.pt\n",
      "[INFO] 17:19:08 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_1.pt\n",
      "[INFO] 17:19:08 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_1.pt\n",
      "[INFO] 17:19:08 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_1.pt\n",
      "[INFO] 17:19:08 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_1.pt\n",
      "[INFO] 17:19:08 Generating subgraph 0 2 for Region 3 3\n",
      "[INFO] 17:19:08 Generating subgraph 1 2 for Region 3 3\n",
      "[INFO] 17:19:08 Generating subgraph 2 2 for Region 3 3\n",
      "[INFO] 17:19:08 Generating subgraph 3 2 for Region 3 3\n",
      "[INFO] 17:19:08 Generating subgraph 4 2 for Region 3 3\n",
      "[INFO] 17:19:08 Generating subgraph 5 2 for Region 3 3\n",
      "[INFO] 17:19:08 Started processing subregion 3 3, subgraph id: 0 2\n",
      "[INFO] 17:19:08 Started processing subregion 3 3, subgraph id: 1 2\n",
      "[INFO] 17:19:08 Started processing subregion 3 3, subgraph id: 2 2\n",
      "[INFO] 17:19:08 Started processing subregion 3 3, subgraph id: 3 2\n",
      "[INFO] 17:19:08 Started processing subregion 3 3, subgraph id: 4 2\n",
      "[INFO] 17:19:08 Started processing subregion 3 3, subgraph id: 5 2\n",
      "[INFO] 17:19:08 Created and saved mask for subgraph: 4, 2\n",
      "[INFO] 17:19:08 Created and saved mask for subgraph: 1, 2\n",
      "[INFO] 17:19:08 Created and saved mask for subgraph: 0, 2\n",
      "[INFO] 17:19:08 Created and saved mask for subgraph: 5, 2\n",
      "[INFO] 17:19:08 Created and saved mask for subgraph: 3, 2\n",
      "[INFO] 17:19:08 Created and saved mask for subgraph: 2, 2\n",
      "[INFO] 17:19:13 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_2.pt\n",
      "[INFO] 17:19:13 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_2.pt[INFO] 17:19:13 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_2.pt\n",
      "\n",
      "[INFO] 17:19:13 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_2.pt\n",
      "[INFO] 17:19:13 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_2.pt\n",
      "[INFO] 17:19:13 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_2.pt\n",
      "[INFO] 17:19:13 Generating subgraph 0 3 for Region 3 3\n",
      "[INFO] 17:19:13 Generating subgraph 1 3 for Region 3 3\n",
      "[INFO] 17:19:13 Generating subgraph 2 3 for Region 3 3\n",
      "[INFO] 17:19:13 Generating subgraph 3 3 for Region 3 3\n",
      "[INFO] 17:19:13 Generating subgraph 4 3 for Region 3 3\n",
      "[INFO] 17:19:13 Generating subgraph 5 3 for Region 3 3\n",
      "[INFO] 17:19:13 Started processing subregion 3 3, subgraph id: 0 3\n",
      "[INFO] 17:19:13 Started processing subregion 3 3, subgraph id: 1 3\n",
      "[INFO] 17:19:13 Started processing subregion 3 3, subgraph id: 2 3\n",
      "[INFO] 17:19:13 Started processing subregion 3 3, subgraph id: 3 3\n",
      "[INFO] 17:19:13 Started processing subregion 3 3, subgraph id: 4 3\n",
      "[INFO] 17:19:13 Started processing subregion 3 3, subgraph id: 5 3\n",
      "[INFO] 17:19:14 Created and saved mask for subgraph: 3, 3\n",
      "[INFO] 17:19:14 Created and saved mask for subgraph: 0, 3\n",
      "[INFO] 17:19:14 Created and saved mask for subgraph: 1, 3\n",
      "[INFO] 17:19:14 Created and saved mask for subgraph: 2, 3\n",
      "[INFO] 17:19:14 Created and saved mask for subgraph: 5, 3\n",
      "[INFO] 17:19:14 Created and saved mask for subgraph: 4, 3\n",
      "[INFO] 17:19:18 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_3.pt\n",
      "[INFO] 17:19:18 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_3.pt\n",
      "[INFO] 17:19:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_3.pt[INFO] 17:19:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_3.pt\n",
      "[INFO] 17:19:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_3.pt\n",
      "\n",
      "[INFO] 17:19:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_3.pt\n",
      "[INFO] 17:19:19 Generating subgraph 0 4 for Region 3 3\n",
      "[INFO] 17:19:19 Generating subgraph 1 4 for Region 3 3\n",
      "[INFO] 17:19:19 Generating subgraph 2 4 for Region 3 3\n",
      "[INFO] 17:19:19 Generating subgraph 3 4 for Region 3 3\n",
      "[INFO] 17:19:19 Generating subgraph 4 4 for Region 3 3\n",
      "[INFO] 17:19:19 Generating subgraph 5 4 for Region 3 3\n",
      "[INFO] 17:19:19 Started processing subregion 3 3, subgraph id: 0 4\n",
      "[INFO] 17:19:19 Started processing subregion 3 3, subgraph id: 1 4\n",
      "[INFO] 17:19:19 Started processing subregion 3 3, subgraph id: 2 4\n",
      "[INFO] 17:19:19 Started processing subregion 3 3, subgraph id: 3 4\n",
      "[INFO] 17:19:19 Started processing subregion 3 3, subgraph id: 4 4\n",
      "[INFO] 17:19:19 Started processing subregion 3 3, subgraph id: 5 4\n",
      "[INFO] 17:19:19 Created and saved mask for subgraph: 4, 4\n",
      "[INFO] 17:19:19 Created and saved mask for subgraph: 2, 4\n",
      "[INFO] 17:19:19 Created and saved mask for subgraph: 3, 4\n",
      "[INFO] 17:19:19 Created and saved mask for subgraph: 0, 4\n",
      "[INFO] 17:19:19 Created and saved mask for subgraph: 1, 4\n",
      "[INFO] 17:19:19 Created and saved mask for subgraph: 5, 4\n",
      "[INFO] 17:19:23 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_4.pt\n",
      "[INFO] 17:19:23 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_4.pt\n",
      "[INFO] 17:19:23 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_4.pt\n",
      "[INFO] 17:19:24 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_4.pt\n",
      "[INFO] 17:19:24 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_4.pt\n",
      "[INFO] 17:19:24 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_4.pt\n",
      "[INFO] 17:19:24 Generating subgraph 0 5 for Region 3 3\n",
      "[INFO] 17:19:24 Generating subgraph 1 5 for Region 3 3\n",
      "[INFO] 17:19:24 Generating subgraph 2 5 for Region 3 3\n",
      "[INFO] 17:19:24 Generating subgraph 3 5 for Region 3 3\n",
      "[INFO] 17:19:24 Generating subgraph 4 5 for Region 3 3\n",
      "[INFO] 17:19:24 Generating subgraph 5 5 for Region 3 3\n",
      "[INFO] 17:19:24 Started processing subregion 3 3, subgraph id: 1 5\n",
      "[INFO] 17:19:24 Started processing subregion 3 3, subgraph id: 0 5\n",
      "[INFO] 17:19:24 Started processing subregion 3 3, subgraph id: 2 5\n",
      "[INFO] 17:19:24 Started processing subregion 3 3, subgraph id: 3 5\n",
      "[INFO] 17:19:24 Started processing subregion 3 3, subgraph id: 4 5\n",
      "[INFO] 17:19:24 Started processing subregion 3 3, subgraph id: 5 5\n",
      "[INFO] 17:19:24 Created and saved mask for subgraph: 2, 5[INFO] 17:19:24 Created and saved mask for subgraph: 0, 5\n",
      "[INFO] 17:19:24 Created and saved mask for subgraph: 1, 5\n",
      "[INFO] 17:19:24 Created and saved mask for subgraph: 3, 5\n",
      "\n",
      "[INFO] 17:19:24 Created and saved mask for subgraph: 4, 5\n",
      "[INFO] 17:19:24 Created and saved mask for subgraph: 5, 5\n",
      "[INFO] 17:19:28 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_5.pt\n",
      "[INFO] 17:19:28 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_5.pt\n",
      "[INFO] 17:19:28 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_5.pt\n",
      "[INFO] 17:19:29 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_5.pt\n",
      "[INFO] 17:19:29 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_5.pt\n",
      "[INFO] 17:19:29 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_5.pt\n",
      "[INFO] 17:19:29 Generating subgraph 0 6 for Region 3 3\n",
      "[INFO] 17:19:29 Generating subgraph 1 6 for Region 3 3\n",
      "[INFO] 17:19:29 Generating subgraph 2 6 for Region 3 3\n",
      "[INFO] 17:19:29 Generating subgraph 3 6 for Region 3 3\n",
      "[INFO] 17:19:29 Generating subgraph 4 6 for Region 3 3\n",
      "[INFO] 17:19:29 Generating subgraph 5 6 for Region 3 3\n",
      "[INFO] 17:19:29 Started processing subregion 3 3, subgraph id: 0 6\n",
      "[INFO] 17:19:29 Started processing subregion 3 3, subgraph id: 1 6\n",
      "[INFO] 17:19:29 Started processing subregion 3 3, subgraph id: 2 6\n",
      "[INFO] 17:19:29 Started processing subregion 3 3, subgraph id: 3 6\n",
      "[INFO] 17:19:29 Started processing subregion 3 3, subgraph id: 4 6\n",
      "[INFO] 17:19:29 Started processing subregion 3 3, subgraph id: 5 6\n",
      "[INFO] 17:19:29 Created and saved mask for subgraph: 4, 6\n",
      "[INFO] 17:19:29 Created and saved mask for subgraph: 3, 6\n",
      "[INFO] 17:19:29 Created and saved mask for subgraph: 5, 6\n",
      "[INFO] 17:19:29 Created and saved mask for subgraph: 1, 6\n",
      "[INFO] 17:19:29 Created and saved mask for subgraph: 0, 6\n",
      "[INFO] 17:19:29 Created and saved mask for subgraph: 2, 6\n",
      "[INFO] 17:19:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_6.pt[INFO] 17:19:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_6.pt\n",
      "\n",
      "[INFO] 17:19:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_6.pt\n",
      "[INFO] 17:19:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_6.pt\n",
      "[INFO] 17:19:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_6.pt\n",
      "[INFO] 17:19:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_6.pt\n",
      "[INFO] 17:19:33 Generating subgraph 0 7 for Region 3 3\n",
      "[INFO] 17:19:33 Generating subgraph 1 7 for Region 3 3\n",
      "[INFO] 17:19:33 Generating subgraph 2 7 for Region 3 3\n",
      "[INFO] 17:19:33 Generating subgraph 3 7 for Region 3 3\n",
      "[INFO] 17:19:33 Generating subgraph 4 7 for Region 3 3\n",
      "[INFO] 17:19:33 Generating subgraph 5 7 for Region 3 3\n",
      "[INFO] 17:19:33 Started processing subregion 3 3, subgraph id: 0 7\n",
      "[INFO] 17:19:33 Started processing subregion 3 3, subgraph id: 1 7\n",
      "[INFO] 17:19:33 Started processing subregion 3 3, subgraph id: 2 7\n",
      "[INFO] 17:19:33 Started processing subregion 3 3, subgraph id: 3 7\n",
      "[INFO] 17:19:33 Started processing subregion 3 3, subgraph id: 4 7\n",
      "[INFO] 17:19:33 Started processing subregion 3 3, subgraph id: 5 7\n",
      "[INFO] 17:19:33 Created and saved mask for subgraph: 0, 7\n",
      "[INFO] 17:19:33 Created and saved mask for subgraph: 5, 7\n",
      "[INFO] 17:19:33 Created and saved mask for subgraph: 4, 7\n",
      "[INFO] 17:19:33 Created and saved mask for subgraph: 1, 7\n",
      "[INFO] 17:19:33 Created and saved mask for subgraph: 3, 7\n",
      "[INFO] 17:19:33 Created and saved mask for subgraph: 2, 7\n",
      "[INFO] 17:19:37 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_7.pt\n",
      "[INFO] 17:19:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_7.pt\n",
      "[INFO] 17:19:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_7.pt[INFO] 17:19:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_7.pt\n",
      "\n",
      "[INFO] 17:19:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_7.pt\n",
      "[INFO] 17:19:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_7.pt\n",
      "[INFO] 17:19:38 Generating subgraph 0 8 for Region 3 3\n",
      "[INFO] 17:19:38 Generating subgraph 1 8 for Region 3 3\n",
      "[INFO] 17:19:38 Generating subgraph 2 8 for Region 3 3\n",
      "[INFO] 17:19:38 Generating subgraph 3 8 for Region 3 3\n",
      "[INFO] 17:19:38 Generating subgraph 4 8 for Region 3 3\n",
      "[INFO] 17:19:38 Generating subgraph 5 8 for Region 3 3\n",
      "[INFO] 17:19:38 Started processing subregion 3 3, subgraph id: 0 8\n",
      "[INFO] 17:19:38 Started processing subregion 3 3, subgraph id: 1 8\n",
      "[INFO] 17:19:38 Started processing subregion 3 3, subgraph id: 2 8\n",
      "[INFO] 17:19:38 Started processing subregion 3 3, subgraph id: 3 8\n",
      "[INFO] 17:19:38 Started processing subregion 3 3, subgraph id: 4 8\n",
      "[INFO] 17:19:38 Started processing subregion 3 3, subgraph id: 5 8\n",
      "[INFO] 17:19:39 Created and saved mask for subgraph: 5, 8\n",
      "[INFO] 17:19:39 Created and saved mask for subgraph: 2, 8\n",
      "[INFO] 17:19:39 Created and saved mask for subgraph: 0, 8\n",
      "[INFO] 17:19:39 Created and saved mask for subgraph: 3, 8\n",
      "[INFO] 17:19:39 Created and saved mask for subgraph: 4, 8\n",
      "[INFO] 17:19:39 Created and saved mask for subgraph: 1, 8\n",
      "[INFO] 17:19:44 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_8.pt\n",
      "[INFO] 17:19:44 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_8.pt\n",
      "[INFO] 17:19:44 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_8.pt\n",
      "[INFO] 17:19:44 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_8.pt\n",
      "[INFO] 17:19:44 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_8.pt\n",
      "[INFO] 17:19:44 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_8.pt\n",
      "[INFO] 17:19:44 Generating subgraph 0 9 for Region 3 3\n",
      "[INFO] 17:19:44 Generating subgraph 1 9 for Region 3 3\n",
      "[INFO] 17:19:44 Generating subgraph 2 9 for Region 3 3\n",
      "[INFO] 17:19:44 Generating subgraph 3 9 for Region 3 3\n",
      "[INFO] 17:19:44 Generating subgraph 4 9 for Region 3 3\n",
      "[INFO] 17:19:44 Generating subgraph 5 9 for Region 3 3\n",
      "[INFO] 17:19:44 Started processing subregion 3 3, subgraph id: 0 9\n",
      "[INFO] 17:19:44 Started processing subregion 3 3, subgraph id: 1 9\n",
      "[INFO] 17:19:44 Started processing subregion 3 3, subgraph id: 2 9\n",
      "[INFO] 17:19:44 Started processing subregion 3 3, subgraph id: 3 9\n",
      "[INFO] 17:19:44 Started processing subregion 3 3, subgraph id: 4 9\n",
      "[INFO] 17:19:44 Started processing subregion 3 3, subgraph id: 5 9\n",
      "[INFO] 17:19:44 Created and saved mask for subgraph: 0, 9\n",
      "[INFO] 17:19:44 Created and saved mask for subgraph: 4, 9\n",
      "[INFO] 17:19:44 Created and saved mask for subgraph: 2, 9\n",
      "[INFO] 17:19:44 Created and saved mask for subgraph: 1, 9\n",
      "[INFO] 17:19:44 Created and saved mask for subgraph: 3, 9\n",
      "[INFO] 17:19:44 Created and saved mask for subgraph: 5, 9\n",
      "[INFO] 17:19:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_9.pt\n",
      "[INFO] 17:19:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_9.pt\n",
      "[INFO] 17:19:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_9.pt\n",
      "[INFO] 17:19:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_9.pt\n",
      "[INFO] 17:19:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_9.pt\n",
      "[INFO] 17:19:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_9.pt\n",
      "[INFO] 17:19:48 Generating subgraph 0 10 for Region 3 3\n",
      "[INFO] 17:19:48 Generating subgraph 1 10 for Region 3 3\n",
      "[INFO] 17:19:48 Generating subgraph 2 10 for Region 3 3\n",
      "[INFO] 17:19:48 Generating subgraph 3 10 for Region 3 3\n",
      "[INFO] 17:19:48 Generating subgraph 4 10 for Region 3 3\n",
      "[INFO] 17:19:48 Generating subgraph 5 10 for Region 3 3\n",
      "[INFO] 17:19:48 Started processing subregion 3 3, subgraph id: 0 10\n",
      "[INFO] 17:19:48 Started processing subregion 3 3, subgraph id: 1 10\n",
      "[INFO] 17:19:48 Started processing subregion 3 3, subgraph id: 2 10\n",
      "[INFO] 17:19:48 Started processing subregion 3 3, subgraph id: 3 10\n",
      "[INFO] 17:19:48 Started processing subregion 3 3, subgraph id: 4 10\n",
      "[INFO] 17:19:48 Started processing subregion 3 3, subgraph id: 5 10\n",
      "[INFO] 17:19:48 Created and saved mask for subgraph: 0, 10\n",
      "[INFO] 17:19:48 Created and saved mask for subgraph: 3, 10\n",
      "[INFO] 17:19:48 Created and saved mask for subgraph: 4, 10\n",
      "[INFO] 17:19:48 Created and saved mask for subgraph: 5, 10\n",
      "[INFO] 17:19:48 Created and saved mask for subgraph: 1, 10\n",
      "[INFO] 17:19:48 Created and saved mask for subgraph: 2, 10\n",
      "[INFO] 17:19:51 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_10.pt\n",
      "[INFO] 17:19:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_10.pt[INFO] 17:19:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_10.pt\n",
      "\n",
      "[INFO] 17:19:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_10.pt\n",
      "[INFO] 17:19:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_10.pt\n",
      "[INFO] 17:19:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_10.pt\n",
      "[INFO] 17:19:53 Generating subgraph 0 11 for Region 3 3\n",
      "[INFO] 17:19:53 Generating subgraph 1 11 for Region 3 3\n",
      "[INFO] 17:19:53 Generating subgraph 2 11 for Region 3 3\n",
      "[INFO] 17:19:53 Generating subgraph 3 11 for Region 3 3\n",
      "[INFO] 17:19:53 Generating subgraph 4 11 for Region 3 3\n",
      "[INFO] 17:19:53 Generating subgraph 5 11 for Region 3 3\n",
      "[INFO] 17:19:53 Started processing subregion 3 3, subgraph id: 0 11\n",
      "[INFO] 17:19:53 Started processing subregion 3 3, subgraph id: 1 11\n",
      "[INFO] 17:19:53 Started processing subregion 3 3, subgraph id: 2 11\n",
      "[INFO] 17:19:53 Started processing subregion 3 3, subgraph id: 3 11\n",
      "[INFO] 17:19:53 Started processing subregion 3 3, subgraph id: 4 11\n",
      "[INFO] 17:19:53 Started processing subregion 3 3, subgraph id: 5 11\n",
      "[INFO] 17:19:53 Created and saved mask for subgraph: 1, 11\n",
      "[INFO] 17:19:53 Created and saved mask for subgraph: 0, 11\n",
      "[INFO] 17:19:53 Created and saved mask for subgraph: 2, 11\n",
      "[INFO] 17:19:53 Created and saved mask for subgraph: 5, 11\n",
      "[INFO] 17:19:53 Created and saved mask for subgraph: 3, 11\n",
      "[INFO] 17:19:53 Created and saved mask for subgraph: 4, 11\n",
      "[INFO] 17:19:56 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_11.pt[INFO] 17:19:56 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_11.pt\n",
      "\n",
      "[INFO] 17:19:57 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_11.pt\n",
      "[INFO] 17:19:57 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_11.pt\n",
      "[INFO] 17:19:57 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_11.pt\n",
      "[INFO] 17:19:57 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_11.pt\n",
      "[INFO] 17:19:57 Generating subgraph 0 12 for Region 3 3\n",
      "[INFO] 17:19:57 Generating subgraph 1 12 for Region 3 3\n",
      "[INFO] 17:19:57 Generating subgraph 2 12 for Region 3 3\n",
      "[INFO] 17:19:57 Generating subgraph 3 12 for Region 3 3\n",
      "[INFO] 17:19:57 Generating subgraph 4 12 for Region 3 3\n",
      "[INFO] 17:19:57 Generating subgraph 5 12 for Region 3 3\n",
      "[INFO] 17:19:57 Started processing subregion 3 3, subgraph id: 0 12\n",
      "[INFO] 17:19:57 Started processing subregion 3 3, subgraph id: 1 12\n",
      "[INFO] 17:19:57 Started processing subregion 3 3, subgraph id: 2 12\n",
      "[INFO] 17:19:57 Started processing subregion 3 3, subgraph id: 3 12\n",
      "[INFO] 17:19:57 Started processing subregion 3 3, subgraph id: 4 12\n",
      "[INFO] 17:19:57 Started processing subregion 3 3, subgraph id: 5 12\n",
      "[INFO] 17:19:57 Created and saved mask for subgraph: 0, 12\n",
      "[INFO] 17:19:57 Created and saved mask for subgraph: 1, 12\n",
      "[INFO] 17:19:57 Created and saved mask for subgraph: 2, 12\n",
      "[INFO] 17:19:57 Created and saved mask for subgraph: 4, 12\n",
      "[INFO] 17:19:57 Created and saved mask for subgraph: 3, 12\n",
      "[INFO] 17:19:57 Created and saved mask for subgraph: 5, 12\n",
      "[INFO] 17:20:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_12.pt\n",
      "[INFO] 17:20:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_12.pt\n",
      "[INFO] 17:20:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_12.pt\n",
      "[INFO] 17:20:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_12.pt\n",
      "[INFO] 17:20:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_12.pt\n",
      "[INFO] 17:20:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_12.pt\n"
     ]
    }
   ],
   "source": [
    "Part3(3, 3, 1, indices[3][3], updatedSubregions[3][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:21:18 Starting subgraph_3_3_0_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:19<00:00,  1.07it/s, 304.293335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:23:37 Trained model on subgraph_3_3_0_1.pt. Now saving predictions.\n",
      "[INFO] 22:23:38 Processed and deleted subgraph_3_3_0_1.pt\n",
      "[INFO] 22:23:38 Starting subgraph_3_3_0_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  65%|██████▍   | 97/150 [01:13<01:32,  1.75s/it, 422.461029] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:24:51 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:39<00:00,  1.07s/it, 382.398438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:26:18 Trained model on subgraph_3_3_0_10.pt. Now saving predictions.\n",
      "[INFO] 22:26:18 Processed and deleted subgraph_3_3_0_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:26:18 Starting subgraph_3_3_0_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:08<01:34,  1.73s/it, 405.736572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:27:28 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:37<00:00,  1.05s/it, 161.110977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:28:55 Trained model on subgraph_3_3_0_11.pt. Now saving predictions.\n",
      "[INFO] 22:28:55 Processed and deleted subgraph_3_3_0_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:28:55 Starting subgraph_3_3_0_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  64%|██████▍   | 96/150 [01:07<01:34,  1.75s/it, 189.386581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:30:04 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:36<00:00,  1.05s/it, 153.030167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:31:32 Trained model on subgraph_3_3_0_12.pt. Now saving predictions.\n",
      "[INFO] 22:31:33 Processed and deleted subgraph_3_3_0_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:31:33 Starting subgraph_3_3_0_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:06<01:36,  1.76s/it, 127.742340] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:32:41 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:36<00:00,  1.04s/it, 68.260742] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:34:09 Trained model on subgraph_3_3_0_2.pt. Now saving predictions.\n",
      "[INFO] 22:34:10 Processed and deleted subgraph_3_3_0_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:34:10 Starting subgraph_3_3_0_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:10<01:36,  1.75s/it, 73.868851] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:35:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:39<00:00,  1.06s/it, 67.609131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:36:49 Trained model on subgraph_3_3_0_3.pt. Now saving predictions.\n",
      "[INFO] 22:36:49 Processed and deleted subgraph_3_3_0_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:36:49 Starting subgraph_3_3_0_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:09<01:36,  1.76s/it, 75.587685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:37:59 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:39<00:00,  1.06s/it, 72.180481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:39:28 Trained model on subgraph_3_3_0_4.pt. Now saving predictions.\n",
      "[INFO] 22:39:29 Processed and deleted subgraph_3_3_0_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:39:29 Starting subgraph_3_3_0_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:09<01:42,  1.82s/it, 101.418182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:40:39 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:40<00:00,  1.07s/it, 93.412720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:42:09 Trained model on subgraph_3_3_0_5.pt. Now saving predictions.\n",
      "[INFO] 22:42:09 Processed and deleted subgraph_3_3_0_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:42:09 Starting subgraph_3_3_0_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:39,  1.75s/it, 101.913437]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:43:21 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:42<00:00,  1.09s/it, 97.624481] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:44:52 Trained model on subgraph_3_3_0_6.pt. Now saving predictions.\n",
      "[INFO] 22:44:53 Processed and deleted subgraph_3_3_0_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:44:53 Starting subgraph_3_3_0_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:09<01:37,  1.77s/it, 95.598862] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:46:04 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:39<00:00,  1.07s/it, 91.452187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:47:33 Trained model on subgraph_3_3_0_7.pt. Now saving predictions.\n",
      "[INFO] 22:47:33 Processed and deleted subgraph_3_3_0_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:47:33 Starting subgraph_3_3_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:10<01:36,  1.75s/it, 93.924774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:48:43 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:39<00:00,  1.06s/it, 89.863335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:50:12 Trained model on subgraph_3_3_0_8.pt. Now saving predictions.\n",
      "[INFO] 22:50:13 Processed and deleted subgraph_3_3_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:50:13 Starting subgraph_3_3_0_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:10<01:35,  1.74s/it, 109.687675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:51:24 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:40<00:00,  1.07s/it, 105.768257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:52:53 Trained model on subgraph_3_3_0_9.pt. Now saving predictions.\n",
      "[INFO] 22:52:53 Processed and deleted subgraph_3_3_0_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:52:53 Starting subgraph_3_3_1_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  64%|██████▍   | 96/150 [01:09<01:34,  1.74s/it, 54.876644] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:54:02 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:37<00:00,  1.05s/it, 51.377678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:55:31 Trained model on subgraph_3_3_1_0.pt. Now saving predictions.\n",
      "[INFO] 22:55:31 Processed and deleted subgraph_3_3_1_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:55:31 Starting subgraph_3_3_1_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:09<01:38,  1.79s/it, 60.112160]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:56:40 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:37<00:00,  1.05s/it, 59.158321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:58:08 Trained model on subgraph_3_3_1_1.pt. Now saving predictions.\n",
      "[INFO] 22:58:08 Processed and deleted subgraph_3_3_1_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:58:08 Starting subgraph_3_3_1_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:10<01:38,  1.76s/it, 132.730438]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 22:59:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:41<00:00,  1.08s/it, 116.207825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:00:50 Trained model on subgraph_3_3_1_10.pt. Now saving predictions.\n",
      "[INFO] 23:00:50 Processed and deleted subgraph_3_3_1_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:00:50 Starting subgraph_3_3_1_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:07<01:35,  1.74s/it, 116.030884]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:01:59 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:37<00:00,  1.05s/it, 113.752190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:03:28 Trained model on subgraph_3_3_1_11.pt. Now saving predictions.\n",
      "[INFO] 23:03:28 Processed and deleted subgraph_3_3_1_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:03:28 Starting subgraph_3_3_1_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:09<01:38,  1.76s/it, 156.961136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:04:39 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:41<00:00,  1.07s/it, 151.892517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:06:09 Trained model on subgraph_3_3_1_12.pt. Now saving predictions.\n",
      "[INFO] 23:06:09 Processed and deleted subgraph_3_3_1_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_11092\\2236325353.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:06:09 Starting subgraph_3_3_1_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:09<01:38,  1.76s/it, 71.807327] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:07:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  92%|█████████▏| 138/150 [02:24<00:12,  1.05s/it, 66.986565]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Part4(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 212\u001b[0m, in \u001b[0;36mPart4\u001b[1;34m(x, y, iteration_number)\u001b[0m\n\u001b[0;32m    209\u001b[0m graph_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(file_path)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# For each data object file I need to train the Graph using that data\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m train_graph(graph_data, model, optimizer, num_epochs, lr)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Trained model on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Now saving predictions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# # Finally for that graph I need to open the region csv file and place where data is not avalaible\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# save_predictions(x, y, model, graph_data)\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute the output on the CPU and pass it to save_predictions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m, in \u001b[0;36mtrain_graph\u001b[1;34m(graph_data, model, optimizer, num_epochs, lr)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_graph\u001b[39m(graph_data, model, optimizer, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m):\n\u001b[0;32m      2\u001b[0m     graph_data \u001b[38;5;241m=\u001b[39m graph_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m     train_combined(data\u001b[38;5;241m=\u001b[39mgraph_data, model\u001b[38;5;241m=\u001b[39mmodel, optimizer\u001b[38;5;241m=\u001b[39moptimizer, epochs\u001b[38;5;241m=\u001b[39mnum_epochs, element_weights\u001b[38;5;241m=\u001b[39m element_weights, lr \u001b[38;5;241m=\u001b[39m lr)\n",
      "Cell \u001b[1;32mIn[35], line 69\u001b[0m, in \u001b[0;36mtrain_combined\u001b[1;34m(data, model, optimizer, epochs, element_weights, lr)\u001b[0m\n\u001b[0;32m     65\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 69\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m!=\u001b[39m (totalEpochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Part4(3, 3, 1)\n",
    "# NEED TO INCREASE NUM EPOCHS!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:17:16 Starting Iteration 1 for region 3 3\n",
      "[INFO] 23:17:16 Running PART 3 (iter: 1)\n",
      "[INFO] 23:17:28 Dataframe Read. Size = 553.427891 MB\n",
      "[INFO] 23:17:28 Will generate 78 subgraphs (6 rows x 13 columns)\n",
      "[INFO] 23:17:28 Generating subgraph 0 0 for Region 3 3\n",
      "[INFO] 23:17:28 Started processing subregion 3 3, subgraph id: 0 0\n",
      "[INFO] 23:17:28 Generating subgraph 1 0 for Region 3 3\n",
      "[INFO] 23:17:28 Started processing subregion 3 3, subgraph id: 1 0\n",
      "[INFO] 23:17:28 Generating subgraph 2 0 for Region 3 3\n",
      "[INFO] 23:17:28 Started processing subregion 3 3, subgraph id: 2 0\n",
      "[INFO] 23:17:28 Generating subgraph 3 0 for Region 3 3\n",
      "[INFO] 23:17:28 Generating subgraph 4 0 for Region 3 3\n",
      "[INFO] 23:17:28 Started processing subregion 3 3, subgraph id: 3 0\n",
      "[INFO] 23:17:28 Started processing subregion 3 3, subgraph id: 4 0\n",
      "[INFO] 23:17:28 Generating subgraph 5 0 for Region 3 3\n",
      "[INFO] 23:17:28 Started processing subregion 3 3, subgraph id: 5 0\n",
      "[INFO] 23:17:28 Loaded mask for subgraph : 2, 0\n",
      "[INFO] 23:17:28 Updated and saved new mask for subgraph : 2, 0 in iteration 1\n",
      "[INFO] 23:17:28 Loaded mask for subgraph : 3, 0\n",
      "[INFO] 23:17:28 Loaded mask for subgraph : 4, 0\n",
      "[INFO] 23:17:28 Loaded mask for subgraph : 1, 0\n",
      "[INFO] 23:17:28 Updated and saved new mask for subgraph : 3, 0 in iteration 1\n",
      "[INFO] 23:17:28 Updated and saved new mask for subgraph : 4, 0 in iteration 1\n",
      "[INFO] 23:17:28 Updated and saved new mask for subgraph : 1, 0 in iteration 1\n",
      "[INFO] 23:17:28 Loaded mask for subgraph : 0, 0\n",
      "[INFO] 23:17:28 Loaded mask for subgraph : 5, 0\n",
      "[INFO] 23:17:28 Updated and saved new mask for subgraph : 5, 0 in iteration 1\n",
      "[INFO] 23:17:28 Updated and saved new mask for subgraph : 0, 0 in iteration 1\n",
      "[INFO] 23:17:31 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_0.pt\n",
      "[INFO] 23:17:32 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_0.pt\n",
      "[INFO] 23:17:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_0.pt\n",
      "[INFO] 23:17:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_0.pt\n",
      "[INFO] 23:17:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_0.pt\n",
      "[INFO] 23:17:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_0.pt\n",
      "[INFO] 23:17:33 Generating subgraph 0 1 for Region 3 3\n",
      "[INFO] 23:17:33 Generating subgraph 1 1 for Region 3 3\n",
      "[INFO] 23:17:33 Generating subgraph 2 1 for Region 3 3\n",
      "[INFO] 23:17:33 Generating subgraph 3 1 for Region 3 3\n",
      "[INFO] 23:17:33 Generating subgraph 4 1 for Region 3 3\n",
      "[INFO] 23:17:33 Generating subgraph 5 1 for Region 3 3\n",
      "[INFO] 23:17:33 Started processing subregion 3 3, subgraph id: 0 1\n",
      "[INFO] 23:17:33 Started processing subregion 3 3, subgraph id: 1 1\n",
      "[INFO] 23:17:33 Started processing subregion 3 3, subgraph id: 2 1\n",
      "[INFO] 23:17:33 Started processing subregion 3 3, subgraph id: 3 1\n",
      "[INFO] 23:17:33 Started processing subregion 3 3, subgraph id: 4 1\n",
      "[INFO] 23:17:33 Started processing subregion 3 3, subgraph id: 5 1\n",
      "[INFO] 23:17:33 Loaded mask for subgraph : 4, 1\n",
      "[INFO] 23:17:33 Updated and saved new mask for subgraph : 4, 1 in iteration 1\n",
      "[INFO] 23:17:33 Loaded mask for subgraph : 2, 1\n",
      "[INFO] 23:17:33 Updated and saved new mask for subgraph : 2, 1 in iteration 1\n",
      "[INFO] 23:17:33 Loaded mask for subgraph : 0, 1\n",
      "[INFO] 23:17:33 Updated and saved new mask for subgraph : 0, 1 in iteration 1\n",
      "[INFO] 23:17:33 Loaded mask for subgraph : 5, 1\n",
      "[INFO] 23:17:33 Loaded mask for subgraph : 1, 1\n",
      "[INFO] 23:17:33 Updated and saved new mask for subgraph : 1, 1 in iteration 1\n",
      "[INFO] 23:17:33 Updated and saved new mask for subgraph : 5, 1 in iteration 1\n",
      "[INFO] 23:17:33 Loaded mask for subgraph : 3, 1\n",
      "[INFO] 23:17:33 Updated and saved new mask for subgraph : 3, 1 in iteration 1\n",
      "[INFO] 23:17:36 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_1.pt\n",
      "[INFO] 23:17:37 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_1.pt\n",
      "[INFO] 23:17:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_1.pt[INFO] 23:17:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_1.pt\n",
      "[INFO] 23:17:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_1.pt\n",
      "\n",
      "[INFO] 23:17:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_1.pt\n",
      "[INFO] 23:17:38 Generating subgraph 0 2 for Region 3 3\n",
      "[INFO] 23:17:38 Generating subgraph 1 2 for Region 3 3\n",
      "[INFO] 23:17:38 Generating subgraph 2 2 for Region 3 3\n",
      "[INFO] 23:17:38 Generating subgraph 3 2 for Region 3 3\n",
      "[INFO] 23:17:38 Generating subgraph 4 2 for Region 3 3\n",
      "[INFO] 23:17:38 Generating subgraph 5 2 for Region 3 3\n",
      "[INFO] 23:17:38 Started processing subregion 3 3, subgraph id: 0 2\n",
      "[INFO] 23:17:38 Started processing subregion 3 3, subgraph id: 1 2\n",
      "[INFO] 23:17:38 Started processing subregion 3 3, subgraph id: 2 2\n",
      "[INFO] 23:17:38 Started processing subregion 3 3, subgraph id: 3 2\n",
      "[INFO] 23:17:38 Started processing subregion 3 3, subgraph id: 4 2\n",
      "[INFO] 23:17:38 Started processing subregion 3 3, subgraph id: 5 2\n",
      "[INFO] 23:17:38 Loaded mask for subgraph : 5, 2\n",
      "[INFO] 23:17:38 Loaded mask for subgraph : 0, 2\n",
      "[INFO] 23:17:38 Updated and saved new mask for subgraph : 5, 2 in iteration 1\n",
      "[INFO] 23:17:38 Updated and saved new mask for subgraph : 0, 2 in iteration 1\n",
      "[INFO] 23:17:38 Loaded mask for subgraph : 1, 2\n",
      "[INFO] 23:17:38 Updated and saved new mask for subgraph : 1, 2 in iteration 1\n",
      "[INFO] 23:17:38 Loaded mask for subgraph : 2, 2\n",
      "[INFO] 23:17:38 Loaded mask for subgraph : 3, 2\n",
      "[INFO] 23:17:38 Updated and saved new mask for subgraph : 3, 2 in iteration 1\n",
      "[INFO] 23:17:38 Updated and saved new mask for subgraph : 2, 2 in iteration 1\n",
      "[INFO] 23:17:38 Loaded mask for subgraph : 4, 2\n",
      "[INFO] 23:17:38 Updated and saved new mask for subgraph : 4, 2 in iteration 1\n",
      "[INFO] 23:17:43 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_2.pt\n",
      "[INFO] 23:17:43 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_2.pt\n",
      "[INFO] 23:17:43 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_2.pt\n",
      "[INFO] 23:17:43 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_2.pt\n",
      "[INFO] 23:17:43 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_2.pt\n",
      "[INFO] 23:17:43 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_2.pt\n",
      "[INFO] 23:17:43 Generating subgraph 0 3 for Region 3 3\n",
      "[INFO] 23:17:43 Generating subgraph 1 3 for Region 3 3\n",
      "[INFO] 23:17:43 Generating subgraph 2 3 for Region 3 3\n",
      "[INFO] 23:17:43 Generating subgraph 3 3 for Region 3 3\n",
      "[INFO] 23:17:43 Generating subgraph 4 3 for Region 3 3\n",
      "[INFO] 23:17:43 Generating subgraph 5 3 for Region 3 3\n",
      "[INFO] 23:17:43 Started processing subregion 3 3, subgraph id: 0 3\n",
      "[INFO] 23:17:43 Started processing subregion 3 3, subgraph id: 1 3\n",
      "[INFO] 23:17:43 Started processing subregion 3 3, subgraph id: 2 3\n",
      "[INFO] 23:17:43 Started processing subregion 3 3, subgraph id: 3 3\n",
      "[INFO] 23:17:43 Started processing subregion 3 3, subgraph id: 4 3\n",
      "[INFO] 23:17:43 Started processing subregion 3 3, subgraph id: 5 3\n",
      "[INFO] 23:17:43 Loaded mask for subgraph : 4, 3\n",
      "[INFO] 23:17:43 Updated and saved new mask for subgraph : 4, 3 in iteration 1\n",
      "[INFO] 23:17:43 Loaded mask for subgraph : 5, 3\n",
      "[INFO] 23:17:43 Loaded mask for subgraph : 2, 3\n",
      "[INFO] 23:17:43 Updated and saved new mask for subgraph : 5, 3 in iteration 1\n",
      "[INFO] 23:17:43 Loaded mask for subgraph : 3, 3\n",
      "[INFO] 23:17:43 Updated and saved new mask for subgraph : 2, 3 in iteration 1\n",
      "[INFO] 23:17:43 Loaded mask for subgraph : 0, 3\n",
      "[INFO] 23:17:43 Updated and saved new mask for subgraph : 3, 3 in iteration 1\n",
      "[INFO] 23:17:43 Loaded mask for subgraph : 1, 3\n",
      "[INFO] 23:17:43 Updated and saved new mask for subgraph : 0, 3 in iteration 1\n",
      "[INFO] 23:17:43 Updated and saved new mask for subgraph : 1, 3 in iteration 1\n",
      "[INFO] 23:17:46 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_3.pt\n",
      "[INFO] 23:17:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_3.pt\n",
      "[INFO] 23:17:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_3.pt\n",
      "[INFO] 23:17:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_3.pt\n",
      "[INFO] 23:17:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_3.pt\n",
      "[INFO] 23:17:48 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_3.pt\n",
      "[INFO] 23:17:48 Generating subgraph 0 4 for Region 3 3\n",
      "[INFO] 23:17:48 Generating subgraph 1 4 for Region 3 3\n",
      "[INFO] 23:17:48 Generating subgraph 2 4 for Region 3 3\n",
      "[INFO] 23:17:48 Generating subgraph 3 4 for Region 3 3\n",
      "[INFO] 23:17:48 Generating subgraph 4 4 for Region 3 3\n",
      "[INFO] 23:17:48 Generating subgraph 5 4 for Region 3 3\n",
      "[INFO] 23:17:48 Started processing subregion 3 3, subgraph id: 0 4\n",
      "[INFO] 23:17:48 Started processing subregion 3 3, subgraph id: 1 4\n",
      "[INFO] 23:17:48 Started processing subregion 3 3, subgraph id: 2 4\n",
      "[INFO] 23:17:48 Started processing subregion 3 3, subgraph id: 3 4\n",
      "[INFO] 23:17:48 Started processing subregion 3 3, subgraph id: 4 4\n",
      "[INFO] 23:17:48 Started processing subregion 3 3, subgraph id: 5 4\n",
      "[INFO] 23:17:49 Loaded mask for subgraph : 4, 4[INFO] 23:17:49 Loaded mask for subgraph : 3, 4\n",
      "\n",
      "[INFO] 23:17:49 Loaded mask for subgraph : 5, 4\n",
      "[INFO] 23:17:49 Updated and saved new mask for subgraph : 3, 4 in iteration 1\n",
      "[INFO] 23:17:49 Updated and saved new mask for subgraph : 5, 4 in iteration 1\n",
      "[INFO] 23:17:49 Updated and saved new mask for subgraph : 4, 4 in iteration 1\n",
      "[INFO] 23:17:49 Loaded mask for subgraph : 1, 4\n",
      "[INFO] 23:17:49 Loaded mask for subgraph : 2, 4\n",
      "[INFO] 23:17:49 Updated and saved new mask for subgraph : 1, 4 in iteration 1\n",
      "[INFO] 23:17:49 Updated and saved new mask for subgraph : 2, 4 in iteration 1\n",
      "[INFO] 23:17:49 Loaded mask for subgraph : 0, 4\n",
      "[INFO] 23:17:49 Updated and saved new mask for subgraph : 0, 4 in iteration 1\n",
      "[INFO] 23:17:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_4.pt\n",
      "[INFO] 23:17:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_4.pt\n",
      "[INFO] 23:17:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_4.pt\n",
      "[INFO] 23:17:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_4.pt\n",
      "[INFO] 23:17:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_4.pt\n",
      "[INFO] 23:17:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_4.pt\n",
      "[INFO] 23:17:53 Generating subgraph 0 5 for Region 3 3\n",
      "[INFO] 23:17:53 Generating subgraph 1 5 for Region 3 3\n",
      "[INFO] 23:17:53 Generating subgraph 2 5 for Region 3 3\n",
      "[INFO] 23:17:53 Generating subgraph 3 5 for Region 3 3\n",
      "[INFO] 23:17:53 Generating subgraph 4 5 for Region 3 3\n",
      "[INFO] 23:17:53 Generating subgraph 5 5 for Region 3 3\n",
      "[INFO] 23:17:53 Started processing subregion 3 3, subgraph id: 0 5\n",
      "[INFO] 23:17:53 Started processing subregion 3 3, subgraph id: 1 5\n",
      "[INFO] 23:17:53 Started processing subregion 3 3, subgraph id: 2 5\n",
      "[INFO] 23:17:53 Started processing subregion 3 3, subgraph id: 3 5\n",
      "[INFO] 23:17:53 Started processing subregion 3 3, subgraph id: 4 5\n",
      "[INFO] 23:17:53 Started processing subregion 3 3, subgraph id: 5 5\n",
      "[INFO] 23:17:53 Loaded mask for subgraph : 1, 5\n",
      "[INFO] 23:17:53 Updated and saved new mask for subgraph : 1, 5 in iteration 1\n",
      "[INFO] 23:17:53 Loaded mask for subgraph : 2, 5\n",
      "[INFO] 23:17:53 Updated and saved new mask for subgraph : 2, 5 in iteration 1\n",
      "[INFO] 23:17:53 Loaded mask for subgraph : 3, 5\n",
      "[INFO] 23:17:53 Loaded mask for subgraph : 4, 5\n",
      "[INFO] 23:17:53 Updated and saved new mask for subgraph : 4, 5 in iteration 1\n",
      "[INFO] 23:17:53 Updated and saved new mask for subgraph : 3, 5 in iteration 1\n",
      "[INFO] 23:17:53 Loaded mask for subgraph : 0, 5\n",
      "[INFO] 23:17:53 Loaded mask for subgraph : 5, 5\n",
      "[INFO] 23:17:53 Updated and saved new mask for subgraph : 0, 5 in iteration 1\n",
      "[INFO] 23:17:53 Updated and saved new mask for subgraph : 5, 5 in iteration 1\n",
      "[INFO] 23:17:57 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_5.pt\n",
      "[INFO] 23:17:57 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_5.pt\n",
      "[INFO] 23:17:58 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_5.pt\n",
      "[INFO] 23:17:58 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_5.pt\n",
      "[INFO] 23:17:58 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_5.pt\n",
      "[INFO] 23:17:58 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_5.pt\n",
      "[INFO] 23:17:58 Generating subgraph 0 6 for Region 3 3\n",
      "[INFO] 23:17:58 Generating subgraph 1 6 for Region 3 3\n",
      "[INFO] 23:17:58 Generating subgraph 2 6 for Region 3 3\n",
      "[INFO] 23:17:58 Generating subgraph 3 6 for Region 3 3\n",
      "[INFO] 23:17:58 Generating subgraph 4 6 for Region 3 3\n",
      "[INFO] 23:17:58 Generating subgraph 5 6 for Region 3 3\n",
      "[INFO] 23:17:58 Started processing subregion 3 3, subgraph id: 0 6\n",
      "[INFO] 23:17:58 Started processing subregion 3 3, subgraph id: 1 6\n",
      "[INFO] 23:17:58 Started processing subregion 3 3, subgraph id: 2 6\n",
      "[INFO] 23:17:58 Started processing subregion 3 3, subgraph id: 3 6\n",
      "[INFO] 23:17:58 Started processing subregion 3 3, subgraph id: 4 6\n",
      "[INFO] 23:17:58 Started processing subregion 3 3, subgraph id: 5 6\n",
      "[INFO] 23:17:58 Loaded mask for subgraph : 5, 6\n",
      "[INFO] 23:17:58 Loaded mask for subgraph : 1, 6\n",
      "[INFO] 23:17:58 Loaded mask for subgraph : 3, 6\n",
      "[INFO] 23:17:58 Updated and saved new mask for subgraph : 1, 6 in iteration 1\n",
      "[INFO] 23:17:58 Updated and saved new mask for subgraph : 3, 6 in iteration 1\n",
      "[INFO] 23:17:58 Updated and saved new mask for subgraph : 5, 6 in iteration 1\n",
      "[INFO] 23:17:58 Loaded mask for subgraph : 2, 6\n",
      "[INFO] 23:17:58 Updated and saved new mask for subgraph : 2, 6 in iteration 1\n",
      "[INFO] 23:17:58 Loaded mask for subgraph : 0, 6\n",
      "[INFO] 23:17:58 Loaded mask for subgraph : 4, 6\n",
      "[INFO] 23:17:58 Updated and saved new mask for subgraph : 0, 6 in iteration 1\n",
      "[INFO] 23:17:58 Updated and saved new mask for subgraph : 4, 6 in iteration 1\n",
      "[INFO] 23:18:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_6.pt\n",
      "[INFO] 23:18:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_6.pt\n",
      "[INFO] 23:18:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_6.pt\n",
      "[INFO] 23:18:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_6.pt\n",
      "[INFO] 23:18:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_6.pt\n",
      "[INFO] 23:18:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_6.pt\n",
      "[INFO] 23:18:02 Generating subgraph 0 7 for Region 3 3\n",
      "[INFO] 23:18:02 Generating subgraph 1 7 for Region 3 3\n",
      "[INFO] 23:18:02 Generating subgraph 2 7 for Region 3 3\n",
      "[INFO] 23:18:02 Generating subgraph 3 7 for Region 3 3\n",
      "[INFO] 23:18:02 Generating subgraph 4 7 for Region 3 3\n",
      "[INFO] 23:18:02 Generating subgraph 5 7 for Region 3 3\n",
      "[INFO] 23:18:02 Started processing subregion 3 3, subgraph id: 0 7\n",
      "[INFO] 23:18:02 Started processing subregion 3 3, subgraph id: 1 7\n",
      "[INFO] 23:18:02 Started processing subregion 3 3, subgraph id: 2 7\n",
      "[INFO] 23:18:02 Started processing subregion 3 3, subgraph id: 3 7\n",
      "[INFO] 23:18:02 Started processing subregion 3 3, subgraph id: 4 7\n",
      "[INFO] 23:18:02 Started processing subregion 3 3, subgraph id: 5 7\n",
      "[INFO] 23:18:03 Loaded mask for subgraph : 3, 7\n",
      "[INFO] 23:18:03 Loaded mask for subgraph : 0, 7\n",
      "[INFO] 23:18:03 Loaded mask for subgraph : 2, 7\n",
      "[INFO] 23:18:03 Updated and saved new mask for subgraph : 3, 7 in iteration 1\n",
      "[INFO] 23:18:03 Updated and saved new mask for subgraph : 2, 7 in iteration 1\n",
      "[INFO] 23:18:03 Updated and saved new mask for subgraph : 0, 7 in iteration 1\n",
      "[INFO] 23:18:03 Loaded mask for subgraph : 4, 7\n",
      "[INFO] 23:18:03 Loaded mask for subgraph : 5, 7\n",
      "[INFO] 23:18:03 Loaded mask for subgraph : 1, 7\n",
      "[INFO] 23:18:03 Updated and saved new mask for subgraph : 4, 7 in iteration 1\n",
      "[INFO] 23:18:03 Updated and saved new mask for subgraph : 5, 7 in iteration 1\n",
      "[INFO] 23:18:03 Updated and saved new mask for subgraph : 1, 7 in iteration 1\n",
      "[INFO] 23:18:06 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_7.pt\n",
      "[INFO] 23:18:06 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_7.pt\n",
      "[INFO] 23:18:06 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_7.pt\n",
      "[INFO] 23:18:07 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_7.pt\n",
      "[INFO] 23:18:07 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_7.pt\n",
      "[INFO] 23:18:07 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_7.pt\n",
      "[INFO] 23:18:07 Generating subgraph 0 8 for Region 3 3\n",
      "[INFO] 23:18:07 Generating subgraph 1 8 for Region 3 3\n",
      "[INFO] 23:18:07 Generating subgraph 2 8 for Region 3 3\n",
      "[INFO] 23:18:07 Generating subgraph 3 8 for Region 3 3\n",
      "[INFO] 23:18:07 Generating subgraph 4 8 for Region 3 3\n",
      "[INFO] 23:18:07 Generating subgraph 5 8 for Region 3 3\n",
      "[INFO] 23:18:07 Started processing subregion 3 3, subgraph id: 0 8\n",
      "[INFO] 23:18:07 Started processing subregion 3 3, subgraph id: 1 8\n",
      "[INFO] 23:18:07 Started processing subregion 3 3, subgraph id: 2 8\n",
      "[INFO] 23:18:07 Started processing subregion 3 3, subgraph id: 3 8\n",
      "[INFO] 23:18:07 Started processing subregion 3 3, subgraph id: 4 8\n",
      "[INFO] 23:18:07 Started processing subregion 3 3, subgraph id: 5 8\n",
      "[INFO] 23:18:07 Loaded mask for subgraph : 5, 8\n",
      "[INFO] 23:18:07 Updated and saved new mask for subgraph : 5, 8 in iteration 1\n",
      "[INFO] 23:18:07 Loaded mask for subgraph : 0, 8\n",
      "[INFO] 23:18:07 Loaded mask for subgraph : 4, 8\n",
      "[INFO] 23:18:07 Updated and saved new mask for subgraph : 4, 8 in iteration 1\n",
      "[INFO] 23:18:07 Updated and saved new mask for subgraph : 0, 8 in iteration 1\n",
      "[INFO] 23:18:07 Loaded mask for subgraph : 1, 8\n",
      "[INFO] 23:18:07 Updated and saved new mask for subgraph : 1, 8 in iteration 1\n",
      "[INFO] 23:18:07 Loaded mask for subgraph : 2, 8\n",
      "[INFO] 23:18:07 Loaded mask for subgraph : 3, 8\n",
      "[INFO] 23:18:07 Updated and saved new mask for subgraph : 2, 8 in iteration 1\n",
      "[INFO] 23:18:07 Updated and saved new mask for subgraph : 3, 8 in iteration 1\n",
      "[INFO] 23:18:11 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_8.pt\n",
      "[INFO] 23:18:11 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_8.pt\n",
      "[INFO] 23:18:11 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_8.pt\n",
      "[INFO] 23:18:12 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_8.pt\n",
      "[INFO] 23:18:12 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_8.pt\n",
      "[INFO] 23:18:12 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_8.pt\n",
      "[INFO] 23:18:12 Generating subgraph 0 9 for Region 3 3\n",
      "[INFO] 23:18:12 Generating subgraph 1 9 for Region 3 3\n",
      "[INFO] 23:18:12 Generating subgraph 2 9 for Region 3 3\n",
      "[INFO] 23:18:12 Generating subgraph 3 9 for Region 3 3\n",
      "[INFO] 23:18:12 Generating subgraph 4 9 for Region 3 3\n",
      "[INFO] 23:18:12 Generating subgraph 5 9 for Region 3 3\n",
      "[INFO] 23:18:12 Started processing subregion 3 3, subgraph id: 0 9\n",
      "[INFO] 23:18:12 Started processing subregion 3 3, subgraph id: 1 9\n",
      "[INFO] 23:18:12 Started processing subregion 3 3, subgraph id: 2 9\n",
      "[INFO] 23:18:12 Started processing subregion 3 3, subgraph id: 3 9\n",
      "[INFO] 23:18:12 Started processing subregion 3 3, subgraph id: 4 9\n",
      "[INFO] 23:18:12 Started processing subregion 3 3, subgraph id: 5 9\n",
      "[INFO] 23:18:12 Loaded mask for subgraph : 4, 9\n",
      "[INFO] 23:18:12 Loaded mask for subgraph : 3, 9\n",
      "[INFO] 23:18:12 Loaded mask for subgraph : 2, 9\n",
      "[INFO] 23:18:12 Updated and saved new mask for subgraph : 4, 9 in iteration 1\n",
      "[INFO] 23:18:12 Updated and saved new mask for subgraph : 3, 9 in iteration 1\n",
      "[INFO] 23:18:12 Updated and saved new mask for subgraph : 2, 9 in iteration 1\n",
      "[INFO] 23:18:12 Loaded mask for subgraph : 5, 9\n",
      "[INFO] 23:18:12 Loaded mask for subgraph : 0, 9\n",
      "[INFO] 23:18:12 Updated and saved new mask for subgraph : 0, 9 in iteration 1\n",
      "[INFO] 23:18:12 Updated and saved new mask for subgraph : 5, 9 in iteration 1\n",
      "[INFO] 23:18:12 Loaded mask for subgraph : 1, 9\n",
      "[INFO] 23:18:12 Updated and saved new mask for subgraph : 1, 9 in iteration 1\n",
      "[INFO] 23:18:14 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_9.pt\n",
      "[INFO] 23:18:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_9.pt\n",
      "[INFO] 23:18:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_9.pt\n",
      "[INFO] 23:18:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_9.pt\n",
      "[INFO] 23:18:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_9.pt\n",
      "[INFO] 23:18:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_9.pt\n",
      "[INFO] 23:18:16 Generating subgraph 0 10 for Region 3 3\n",
      "[INFO] 23:18:16 Generating subgraph 1 10 for Region 3 3\n",
      "[INFO] 23:18:16 Generating subgraph 2 10 for Region 3 3\n",
      "[INFO] 23:18:16 Generating subgraph 3 10 for Region 3 3\n",
      "[INFO] 23:18:16 Generating subgraph 4 10 for Region 3 3\n",
      "[INFO] 23:18:16 Generating subgraph 5 10 for Region 3 3\n",
      "[INFO] 23:18:16 Started processing subregion 3 3, subgraph id: 0 10\n",
      "[INFO] 23:18:16 Started processing subregion 3 3, subgraph id: 1 10\n",
      "[INFO] 23:18:16 Started processing subregion 3 3, subgraph id: 2 10\n",
      "[INFO] 23:18:16 Started processing subregion 3 3, subgraph id: 3 10\n",
      "[INFO] 23:18:16 Started processing subregion 3 3, subgraph id: 4 10\n",
      "[INFO] 23:18:16 Started processing subregion 3 3, subgraph id: 5 10\n",
      "[INFO] 23:18:16 Loaded mask for subgraph : 0, 10\n",
      "[INFO] 23:18:16 Loaded mask for subgraph : 3, 10\n",
      "[INFO] 23:18:16 Loaded mask for subgraph : 2, 10\n",
      "[INFO] 23:18:16 Updated and saved new mask for subgraph : 0, 10 in iteration 1\n",
      "[INFO] 23:18:16 Loaded mask for subgraph : 4, 10\n",
      "[INFO] 23:18:16 Updated and saved new mask for subgraph : 3, 10 in iteration 1\n",
      "[INFO] 23:18:16 Updated and saved new mask for subgraph : 2, 10 in iteration 1\n",
      "[INFO] 23:18:16 Updated and saved new mask for subgraph : 4, 10 in iteration 1\n",
      "[INFO] 23:18:16 Loaded mask for subgraph : 5, 10\n",
      "[INFO] 23:18:16 Loaded mask for subgraph : 1, 10\n",
      "[INFO] 23:18:16 Updated and saved new mask for subgraph : 5, 10 in iteration 1\n",
      "[INFO] 23:18:16 Updated and saved new mask for subgraph : 1, 10 in iteration 1\n",
      "[INFO] 23:18:20 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_10.pt[INFO] 23:18:20 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_10.pt\n",
      "\n",
      "[INFO] 23:18:20 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_10.pt\n",
      "[INFO] 23:18:20 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_10.pt\n",
      "[INFO] 23:18:20 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_10.pt\n",
      "[INFO] 23:18:21 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_10.pt\n",
      "[INFO] 23:18:21 Generating subgraph 0 11 for Region 3 3\n",
      "[INFO] 23:18:21 Generating subgraph 1 11 for Region 3 3\n",
      "[INFO] 23:18:21 Generating subgraph 2 11 for Region 3 3\n",
      "[INFO] 23:18:21 Generating subgraph 3 11 for Region 3 3\n",
      "[INFO] 23:18:21 Generating subgraph 4 11 for Region 3 3\n",
      "[INFO] 23:18:21 Generating subgraph 5 11 for Region 3 3\n",
      "[INFO] 23:18:21 Started processing subregion 3 3, subgraph id: 0 11\n",
      "[INFO] 23:18:21 Started processing subregion 3 3, subgraph id: 1 11\n",
      "[INFO] 23:18:21 Started processing subregion 3 3, subgraph id: 2 11\n",
      "[INFO] 23:18:21 Started processing subregion 3 3, subgraph id: 3 11\n",
      "[INFO] 23:18:21 Started processing subregion 3 3, subgraph id: 4 11\n",
      "[INFO] 23:18:21 Started processing subregion 3 3, subgraph id: 5 11\n",
      "[INFO] 23:18:21 Loaded mask for subgraph : 2, 11\n",
      "[INFO] 23:18:21 Updated and saved new mask for subgraph : 2, 11 in iteration 1\n",
      "[INFO] 23:18:21 Loaded mask for subgraph : 0, 11\n",
      "[INFO] 23:18:21 Updated and saved new mask for subgraph : 0, 11 in iteration 1\n",
      "[INFO] 23:18:21 Loaded mask for subgraph : 3, 11\n",
      "[INFO] 23:18:21 Loaded mask for subgraph : 5, 11\n",
      "[INFO] 23:18:21 Loaded mask for subgraph : 1, 11\n",
      "[INFO] 23:18:21 Loaded mask for subgraph : 4, 11\n",
      "[INFO] 23:18:21 Updated and saved new mask for subgraph : 3, 11 in iteration 1\n",
      "[INFO] 23:18:21 Updated and saved new mask for subgraph : 5, 11 in iteration 1\n",
      "[INFO] 23:18:21 Updated and saved new mask for subgraph : 1, 11 in iteration 1\n",
      "[INFO] 23:18:21 Updated and saved new mask for subgraph : 4, 11 in iteration 1\n",
      "[INFO] 23:18:23 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_11.pt\n",
      "[INFO] 23:18:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_11.pt\n",
      "[INFO] 23:18:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_11.pt\n",
      "[INFO] 23:18:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_11.pt\n",
      "[INFO] 23:18:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_11.pt\n",
      "[INFO] 23:18:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_11.pt\n",
      "[INFO] 23:18:25 Generating subgraph 0 12 for Region 3 3\n",
      "[INFO] 23:18:25 Generating subgraph 1 12 for Region 3 3\n",
      "[INFO] 23:18:25 Generating subgraph 2 12 for Region 3 3\n",
      "[INFO] 23:18:25 Generating subgraph 3 12 for Region 3 3\n",
      "[INFO] 23:18:25 Generating subgraph 4 12 for Region 3 3\n",
      "[INFO] 23:18:25 Generating subgraph 5 12 for Region 3 3\n",
      "[INFO] 23:18:25 Started processing subregion 3 3, subgraph id: 0 12\n",
      "[INFO] 23:18:25 Started processing subregion 3 3, subgraph id: 1 12\n",
      "[INFO] 23:18:25 Started processing subregion 3 3, subgraph id: 2 12\n",
      "[INFO] 23:18:25 Started processing subregion 3 3, subgraph id: 3 12\n",
      "[INFO] 23:18:25 Started processing subregion 3 3, subgraph id: 4 12\n",
      "[INFO] 23:18:25 Started processing subregion 3 3, subgraph id: 5 12\n",
      "[INFO] 23:18:25 Loaded mask for subgraph : 3, 12\n",
      "[INFO] 23:18:25 Updated and saved new mask for subgraph : 3, 12 in iteration 1\n",
      "[INFO] 23:18:25 Loaded mask for subgraph : 5, 12\n",
      "[INFO] 23:18:25 Loaded mask for subgraph : 2, 12\n",
      "[INFO] 23:18:25 Loaded mask for subgraph : 1, 12\n",
      "[INFO] 23:18:25 Loaded mask for subgraph : 0, 12\n",
      "[INFO] 23:18:25 Loaded mask for subgraph : 4, 12\n",
      "[INFO] 23:18:25 Updated and saved new mask for subgraph : 5, 12 in iteration 1\n",
      "[INFO] 23:18:25 Updated and saved new mask for subgraph : 2, 12 in iteration 1\n",
      "[INFO] 23:18:25 Updated and saved new mask for subgraph : 1, 12 in iteration 1\n",
      "[INFO] 23:18:25 Updated and saved new mask for subgraph : 0, 12 in iteration 1\n",
      "[INFO] 23:18:25 Updated and saved new mask for subgraph : 4, 12 in iteration 1\n",
      "[INFO] 23:18:29 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_12.pt\n",
      "[INFO] 23:18:29 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_12.pt\n",
      "[INFO] 23:18:29 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_12.pt\n",
      "[INFO] 23:18:29 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_12.pt\n",
      "[INFO] 23:18:30 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_12.pt\n",
      "[INFO] 23:18:30 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_12.pt\n",
      "[INFO] 23:18:30 Running PART 4 (iter: 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:18:30 Starting subgraph_3_3_0_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [04:51<00:00,  1.03it/s, 35.306664] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:23:21 Trained model on subgraph_3_3_0_0.pt. Now saving predictions.\n",
      "[INFO] 23:23:21 Processed and deleted subgraph_3_3_0_0.pt\n",
      "[INFO] 23:23:21 Starting subgraph_3_3_0_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  49%|████▉     | 147/300 [01:09<01:17,  1.97it/s, 2.494323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:24:31 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:29<00:00,  1.10s/it, 35.064068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:28:51 Trained model on subgraph_3_3_0_1.pt. Now saving predictions.\n",
      "[INFO] 23:28:51 Processed and deleted subgraph_3_3_0_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:28:51 Starting subgraph_3_3_0_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 133/300 [01:10<01:26,  1.93it/s, 42.996525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:30:02 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:33<00:00,  1.11s/it, 64.135437]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:34:25 Trained model on subgraph_3_3_0_10.pt. Now saving predictions.\n",
      "[INFO] 23:34:25 Processed and deleted subgraph_3_3_0_10.pt\n",
      "[INFO] 23:34:25 Starting subgraph_3_3_0_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:08<01:32,  1.79it/s, 35.654934]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:35:33 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:25<00:00,  1.09s/it, 93.848732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:39:51 Trained model on subgraph_3_3_0_11.pt. Now saving predictions.\n",
      "[INFO] 23:39:51 Processed and deleted subgraph_3_3_0_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:39:51 Starting subgraph_3_3_0_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:09<01:30,  1.83it/s, 39.809780]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:41:01 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:27<00:00,  1.09s/it, 112.483292]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:45:19 Trained model on subgraph_3_3_0_12.pt. Now saving predictions.\n",
      "[INFO] 23:45:19 Processed and deleted subgraph_3_3_0_12.pt\n",
      "[INFO] 23:45:19 Starting subgraph_3_3_0_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▋     | 139/300 [01:09<01:29,  1.80it/s, 2.555848]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:46:28 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:30<00:00,  1.10s/it, 35.868576]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:50:50 Trained model on subgraph_3_3_0_2.pt. Now saving predictions.\n",
      "[INFO] 23:50:50 Processed and deleted subgraph_3_3_0_2.pt\n",
      "[INFO] 23:50:50 Starting subgraph_3_3_0_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:09<01:33,  1.77it/s, 2.768389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:52:00 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:25<00:00,  1.08s/it, 44.730831]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:56:16 Trained model on subgraph_3_3_0_3.pt. Now saving predictions.\n",
      "[INFO] 23:56:16 Processed and deleted subgraph_3_3_0_3.pt\n",
      "[INFO] 23:56:16 Starting subgraph_3_3_0_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:09<01:30,  1.82it/s, 1.815527]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 23:57:25 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 50.795982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:01:48 Trained model on subgraph_3_3_0_4.pt. Now saving predictions.\n",
      "[INFO] 00:01:49 Processed and deleted subgraph_3_3_0_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:01:49 Starting subgraph_3_3_0_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 136/300 [01:08<01:32,  1.78it/s, 12.408227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:02:58 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:26<00:00,  1.09s/it, 70.909561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:07:15 Trained model on subgraph_3_3_0_5.pt. Now saving predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:07:15 Processed and deleted subgraph_3_3_0_5.pt\n",
      "[INFO] 00:07:15 Starting subgraph_3_3_0_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:08<01:30,  1.79it/s, 9.416907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:08:23 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:25<00:00,  1.08s/it, 68.859367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:12:40 Trained model on subgraph_3_3_0_6.pt. Now saving predictions.\n",
      "[INFO] 00:12:41 Processed and deleted subgraph_3_3_0_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:12:41 Starting subgraph_3_3_0_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▋     | 139/300 [01:10<01:32,  1.74it/s, 8.731386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:13:51 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:25<00:00,  1.09s/it, 62.549908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:18:06 Trained model on subgraph_3_3_0_7.pt. Now saving predictions.\n",
      "[INFO] 00:18:07 Processed and deleted subgraph_3_3_0_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:18:07 Starting subgraph_3_3_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▋     | 139/300 [01:08<01:40,  1.60it/s, 7.075460]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:19:15 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:31<00:00,  1.11s/it, 60.770039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:23:38 Trained model on subgraph_3_3_0_8.pt. Now saving predictions.\n",
      "[INFO] 00:23:39 Processed and deleted subgraph_3_3_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:23:39 Starting subgraph_3_3_0_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▋     | 139/300 [01:09<01:31,  1.76it/s, 7.901258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:24:48 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:34<00:00,  1.11s/it, 63.991421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:29:13 Trained model on subgraph_3_3_0_9.pt. Now saving predictions.\n",
      "[INFO] 00:29:13 Processed and deleted subgraph_3_3_0_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:29:13 Starting subgraph_3_3_1_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 138/300 [01:09<01:23,  1.95it/s, 3.686432]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:30:23 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:31<00:00,  1.10s/it, 39.662739]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:34:45 Trained model on subgraph_3_3_1_0.pt. Now saving predictions.\n",
      "[INFO] 00:34:45 Processed and deleted subgraph_3_3_1_0.pt\n",
      "[INFO] 00:34:45 Starting subgraph_3_3_1_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 136/300 [01:08<01:35,  1.72it/s, 3.232508]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:35:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:28<00:00,  1.10s/it, 42.909767]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:40:14 Trained model on subgraph_3_3_1_1.pt. Now saving predictions.\n",
      "[INFO] 00:40:14 Processed and deleted subgraph_3_3_1_1.pt\n",
      "[INFO] 00:40:14 Starting subgraph_3_3_1_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:08<01:25,  1.95it/s, 14.325026]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:41:23 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:30<00:00,  1.10s/it, 72.173531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:45:45 Trained model on subgraph_3_3_1_10.pt. Now saving predictions.\n",
      "[INFO] 00:45:45 Processed and deleted subgraph_3_3_1_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:45:45 Starting subgraph_3_3_1_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:08<01:32,  1.78it/s, 21.150291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:46:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:25<00:00,  1.08s/it, 83.829636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:51:10 Trained model on subgraph_3_3_1_11.pt. Now saving predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:51:10 Processed and deleted subgraph_3_3_1_11.pt\n",
      "[INFO] 00:51:11 Starting subgraph_3_3_1_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  47%|████▋     | 140/300 [01:09<01:17,  2.05it/s, 29.163719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:52:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 99.387009] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:56:46 Trained model on subgraph_3_3_1_12.pt. Now saving predictions.\n",
      "[INFO] 00:56:46 Processed and deleted subgraph_3_3_1_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:56:46 Starting subgraph_3_3_1_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 132/300 [01:09<01:32,  1.81it/s, 5.267963] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 00:57:55 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:28<00:00,  1.09s/it, 51.835873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:02:15 Trained model on subgraph_3_3_1_2.pt. Now saving predictions.\n",
      "[INFO] 01:02:15 Processed and deleted subgraph_3_3_1_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:02:15 Starting subgraph_3_3_1_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:09<01:39,  1.64it/s, 11.799062]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:03:24 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:26<00:00,  1.09s/it, 68.511772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:07:42 Trained model on subgraph_3_3_1_3.pt. Now saving predictions.\n",
      "[INFO] 01:07:42 Processed and deleted subgraph_3_3_1_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:07:42 Starting subgraph_3_3_1_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 138/300 [01:09<01:20,  2.01it/s, 15.634641]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:08:51 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:41<00:00,  1.14s/it, 74.887550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:13:23 Trained model on subgraph_3_3_1_4.pt. Now saving predictions.\n",
      "[INFO] 01:13:23 Processed and deleted subgraph_3_3_1_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:13:24 Starting subgraph_3_3_1_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  42%|████▏     | 127/300 [01:10<01:34,  1.84it/s, 34.050331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:14:34 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [06:08<00:00,  1.23s/it, 102.985214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:19:32 Trained model on subgraph_3_3_1_5.pt. Now saving predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:19:33 Processed and deleted subgraph_3_3_1_5.pt\n",
      "[INFO] 01:19:33 Starting subgraph_3_3_1_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  43%|████▎     | 129/300 [01:10<01:22,  2.06it/s, 57.561596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:20:43 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:38<00:00,  1.13s/it, 129.994858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:25:11 Trained model on subgraph_3_3_1_6.pt. Now saving predictions.\n",
      "[INFO] 01:25:11 Processed and deleted subgraph_3_3_1_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:25:11 Starting subgraph_3_3_1_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 132/300 [01:09<01:27,  1.92it/s, 49.828945]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:26:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:33<00:00,  1.11s/it, 123.218170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:30:44 Trained model on subgraph_3_3_1_7.pt. Now saving predictions.\n",
      "[INFO] 01:30:45 Processed and deleted subgraph_3_3_1_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:30:45 Starting subgraph_3_3_1_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:09<01:34,  1.75it/s, 23.453323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:31:54 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:31<00:00,  1.10s/it, 91.263519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:36:16 Trained model on subgraph_3_3_1_8.pt. Now saving predictions.\n",
      "[INFO] 01:36:16 Processed and deleted subgraph_3_3_1_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:36:16 Starting subgraph_3_3_1_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 132/300 [01:10<01:56,  1.44it/s, 16.480791]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:37:26 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:29<00:00,  1.10s/it, 77.826675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:41:45 Trained model on subgraph_3_3_1_9.pt. Now saving predictions.\n",
      "[INFO] 01:41:46 Processed and deleted subgraph_3_3_1_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:41:46 Starting subgraph_3_3_2_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▋     | 139/300 [01:09<01:25,  1.88it/s, 5.083029] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:42:55 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:34<00:00,  1.11s/it, 46.443665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:47:20 Trained model on subgraph_3_3_2_0.pt. Now saving predictions.\n",
      "[INFO] 01:47:20 Processed and deleted subgraph_3_3_2_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:47:20 Starting subgraph_3_3_2_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  43%|████▎     | 130/300 [01:08<01:46,  1.59it/s, 8.292658]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:48:28 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 55.748894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:52:55 Trained model on subgraph_3_3_2_1.pt. Now saving predictions.\n",
      "[INFO] 01:52:56 Processed and deleted subgraph_3_3_2_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:52:56 Starting subgraph_3_3_2_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:09<01:34,  1.74it/s, 28.941685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:54:05 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 96.344650] \n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:58:31 Trained model on subgraph_3_3_2_10.pt. Now saving predictions.\n",
      "[INFO] 01:58:31 Processed and deleted subgraph_3_3_2_10.pt\n",
      "[INFO] 01:58:31 Starting subgraph_3_3_2_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 138/300 [01:09<01:24,  1.92it/s, 20.455482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 01:59:40 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:27<00:00,  1.09s/it, 91.204437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:03:58 Trained model on subgraph_3_3_2_11.pt. Now saving predictions.\n",
      "[INFO] 02:03:59 Processed and deleted subgraph_3_3_2_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:03:59 Starting subgraph_3_3_2_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  47%|████▋     | 140/300 [01:11<01:26,  1.85it/s, 38.840237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:05:10 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:29<00:00,  1.10s/it, 112.901054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:09:28 Trained model on subgraph_3_3_2_12.pt. Now saving predictions.\n",
      "[INFO] 02:09:29 Processed and deleted subgraph_3_3_2_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:09:29 Starting subgraph_3_3_2_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▋     | 139/300 [01:10<01:31,  1.77it/s, 7.179511] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:10:39 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 65.369812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:15:04 Trained model on subgraph_3_3_2_2.pt. Now saving predictions.\n",
      "[INFO] 02:15:05 Processed and deleted subgraph_3_3_2_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:15:05 Starting subgraph_3_3_2_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 136/300 [01:09<01:24,  1.93it/s, 16.968874]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:16:14 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:29<00:00,  1.10s/it, 85.043846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:20:34 Trained model on subgraph_3_3_2_3.pt. Now saving predictions.\n",
      "[INFO] 02:20:35 Processed and deleted subgraph_3_3_2_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:20:35 Starting subgraph_3_3_2_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:10<01:27,  1.86it/s, 25.123648]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:21:45 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 96.465843] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:26:10 Trained model on subgraph_3_3_2_4.pt. Now saving predictions.\n",
      "[INFO] 02:26:10 Processed and deleted subgraph_3_3_2_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:26:10 Starting subgraph_3_3_2_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:08<01:43,  1.60it/s, 53.541428]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:27:19 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:28<00:00,  1.09s/it, 130.445938]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:31:38 Trained model on subgraph_3_3_2_5.pt. Now saving predictions.\n",
      "[INFO] 02:31:39 Processed and deleted subgraph_3_3_2_5.pt\n",
      "[INFO] 02:31:39 Starting subgraph_3_3_2_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:11<01:30,  1.80it/s, 116.152382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:32:50 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:37<00:00,  1.12s/it, 195.158142]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:37:16 Trained model on subgraph_3_3_2_6.pt. Now saving predictions.\n",
      "[INFO] 02:37:16 Processed and deleted subgraph_3_3_2_6.pt\n",
      "[INFO] 02:37:16 Starting subgraph_3_3_2_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:08<01:33,  1.77it/s, 106.042679]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:38:25 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:31<00:00,  1.11s/it, 185.782684]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:42:47 Trained model on subgraph_3_3_2_7.pt. Now saving predictions.\n",
      "[INFO] 02:42:48 Processed and deleted subgraph_3_3_2_7.pt\n",
      "[INFO] 02:42:48 Starting subgraph_3_3_2_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 132/300 [01:09<01:26,  1.93it/s, 48.227432]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:43:57 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:33<00:00,  1.11s/it, 124.044121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:48:21 Trained model on subgraph_3_3_2_8.pt. Now saving predictions.\n",
      "[INFO] 02:48:21 Processed and deleted subgraph_3_3_2_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:48:21 Starting subgraph_3_3_2_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 133/300 [01:10<01:28,  1.90it/s, 28.293243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:49:32 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:29<00:00,  1.10s/it, 101.047218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:53:51 Trained model on subgraph_3_3_2_9.pt. Now saving predictions.\n",
      "[INFO] 02:53:51 Processed and deleted subgraph_3_3_2_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:53:51 Starting subgraph_3_3_3_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 136/300 [01:10<01:28,  1.86it/s, 5.121435] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:55:02 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 44.822380]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 02:59:27 Trained model on subgraph_3_3_3_0.pt. Now saving predictions.\n",
      "[INFO] 02:59:27 Processed and deleted subgraph_3_3_3_0.pt\n",
      "[INFO] 02:59:27 Starting subgraph_3_3_3_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 136/300 [01:09<01:33,  1.75it/s, 21.388100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:00:37 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:33<00:00,  1.11s/it, 76.246216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:05:00 Trained model on subgraph_3_3_3_1.pt. Now saving predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:05:01 Processed and deleted subgraph_3_3_3_1.pt\n",
      "[INFO] 03:05:01 Starting subgraph_3_3_3_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  43%|████▎     | 129/300 [01:08<01:33,  1.82it/s, 45.740929]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:06:09 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:33<00:00,  1.11s/it, 118.744759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:10:35 Trained model on subgraph_3_3_3_10.pt. Now saving predictions.\n",
      "[INFO] 03:10:35 Processed and deleted subgraph_3_3_3_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:10:35 Starting subgraph_3_3_3_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  47%|████▋     | 140/300 [01:09<01:14,  2.15it/s, 32.657532]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:11:44 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:26<00:00,  1.09s/it, 107.742485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:16:02 Trained model on subgraph_3_3_3_11.pt. Now saving predictions.\n",
      "[INFO] 03:16:02 Processed and deleted subgraph_3_3_3_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:16:02 Starting subgraph_3_3_3_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 138/300 [01:10<01:44,  1.56it/s, 53.902615]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:17:13 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:37<00:00,  1.12s/it, 130.569870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:21:39 Trained model on subgraph_3_3_3_12.pt. Now saving predictions.\n",
      "[INFO] 03:21:39 Processed and deleted subgraph_3_3_3_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:21:39 Starting subgraph_3_3_3_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:09<01:26,  1.90it/s, 17.594301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:22:49 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 83.256218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:27:12 Trained model on subgraph_3_3_3_2.pt. Now saving predictions.\n",
      "[INFO] 03:27:12 Processed and deleted subgraph_3_3_3_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:27:12 Starting subgraph_3_3_3_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:09<01:41,  1.64it/s, 12.040751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:28:21 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:30<00:00,  1.10s/it, 80.557579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:32:42 Trained model on subgraph_3_3_3_3.pt. Now saving predictions.\n",
      "[INFO] 03:32:42 Processed and deleted subgraph_3_3_3_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:32:43 Starting subgraph_3_3_3_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:10<01:25,  1.93it/s, 22.437607]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:33:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:30<00:00,  1.10s/it, 96.613045]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:38:13 Trained model on subgraph_3_3_3_4.pt. Now saving predictions.\n",
      "[INFO] 03:38:13 Processed and deleted subgraph_3_3_3_4.pt\n",
      "[INFO] 03:38:13 Starting subgraph_3_3_3_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  47%|████▋     | 141/300 [01:09<01:26,  1.83it/s, 51.175579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:39:23 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 128.021500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:43:49 Trained model on subgraph_3_3_3_5.pt. Now saving predictions.\n",
      "[INFO] 03:43:49 Processed and deleted subgraph_3_3_3_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:43:49 Starting subgraph_3_3_3_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:10<01:26,  1.91it/s, 125.597008]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:44:59 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:30<00:00,  1.10s/it, 205.028412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:49:19 Trained model on subgraph_3_3_3_6.pt. Now saving predictions.\n",
      "[INFO] 03:49:20 Processed and deleted subgraph_3_3_3_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:49:20 Starting subgraph_3_3_3_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:10<01:29,  1.84it/s, 115.117065]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:50:30 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 194.063507]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:54:52 Trained model on subgraph_3_3_3_7.pt. Now saving predictions.\n",
      "[INFO] 03:54:52 Processed and deleted subgraph_3_3_3_7.pt\n",
      "[INFO] 03:54:52 Starting subgraph_3_3_3_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:09<01:19,  2.04it/s, 42.191055]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 03:56:02 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:28<00:00,  1.09s/it, 115.334129]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:00:20 Trained model on subgraph_3_3_3_8.pt. Now saving predictions.\n",
      "[INFO] 04:00:20 Processed and deleted subgraph_3_3_3_8.pt\n",
      "[INFO] 04:00:20 Starting subgraph_3_3_3_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  47%|████▋     | 140/300 [01:12<01:10,  2.28it/s, 34.334019]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:01:32 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:34<00:00,  1.12s/it, 103.354530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:05:55 Trained model on subgraph_3_3_3_9.pt. Now saving predictions.\n",
      "[INFO] 04:05:55 Processed and deleted subgraph_3_3_3_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:05:55 Starting subgraph_3_3_4_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:10<01:28,  1.85it/s, 5.631472]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:07:06 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:31<00:00,  1.11s/it, 51.783279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:11:27 Trained model on subgraph_3_3_4_0.pt. Now saving predictions.\n",
      "[INFO] 04:11:28 Processed and deleted subgraph_3_3_4_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:11:28 Starting subgraph_3_3_4_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:09<01:55,  1.43it/s, 38.373859]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:12:38 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:39<00:00,  1.13s/it, 105.194511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:17:07 Trained model on subgraph_3_3_4_1.pt. Now saving predictions.\n",
      "[INFO] 04:17:07 Processed and deleted subgraph_3_3_4_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:17:07 Starting subgraph_3_3_4_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 138/300 [01:10<01:26,  1.87it/s, 51.165257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:18:18 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:30<00:00,  1.10s/it, 125.994202]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:22:38 Trained model on subgraph_3_3_4_10.pt. Now saving predictions.\n",
      "[INFO] 04:22:38 Processed and deleted subgraph_3_3_4_10.pt\n",
      "[INFO] 04:22:38 Starting subgraph_3_3_4_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  47%|████▋     | 140/300 [01:11<01:38,  1.62it/s, 52.711678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:23:50 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:38<00:00,  1.13s/it, 129.919388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:28:16 Trained model on subgraph_3_3_4_11.pt. Now saving predictions.\n",
      "[INFO] 04:28:16 Processed and deleted subgraph_3_3_4_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:28:17 Starting subgraph_3_3_4_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:14<01:24,  1.95it/s, 58.342072]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:29:31 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:38<00:00,  1.13s/it, 132.486984]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:33:55 Trained model on subgraph_3_3_4_12.pt. Now saving predictions.\n",
      "[INFO] 04:33:55 Processed and deleted subgraph_3_3_4_12.pt\n",
      "[INFO] 04:33:55 Starting subgraph_3_3_4_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 132/300 [01:12<01:48,  1.55it/s, 30.077057]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:35:08 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 100.731224]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:39:28 Trained model on subgraph_3_3_4_2.pt. Now saving predictions.\n",
      "[INFO] 04:39:28 Processed and deleted subgraph_3_3_4_2.pt\n",
      "[INFO] 04:39:28 Starting subgraph_3_3_4_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 135/300 [01:12<01:44,  1.58it/s, 6.390380] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:40:41 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 75.230064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:45:04 Trained model on subgraph_3_3_4_3.pt. Now saving predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:45:04 Processed and deleted subgraph_3_3_4_3.pt\n",
      "[INFO] 04:45:04 Starting subgraph_3_3_4_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 133/300 [01:14<01:40,  1.67it/s, 16.647554]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:46:19 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 90.020691] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:50:40 Trained model on subgraph_3_3_4_4.pt. Now saving predictions.\n",
      "[INFO] 04:50:40 Processed and deleted subgraph_3_3_4_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:50:40 Starting subgraph_3_3_4_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 133/300 [01:11<01:48,  1.55it/s, 46.488632]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:51:52 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:33<00:00,  1.11s/it, 123.125473]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:56:14 Trained model on subgraph_3_3_4_5.pt. Now saving predictions.\n",
      "[INFO] 04:56:14 Processed and deleted subgraph_3_3_4_5.pt\n",
      "[INFO] 04:56:14 Starting subgraph_3_3_4_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:13<01:32,  1.80it/s, 85.913025]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 04:57:27 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:33<00:00,  1.11s/it, 164.975845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:01:48 Trained model on subgraph_3_3_4_6.pt. Now saving predictions.\n",
      "[INFO] 05:01:48 Processed and deleted subgraph_3_3_4_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:01:48 Starting subgraph_3_3_4_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▎     | 131/300 [01:12<01:45,  1.60it/s, 71.954353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:03:01 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:31<00:00,  1.11s/it, 148.130051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:07:20 Trained model on subgraph_3_3_4_7.pt. Now saving predictions.\n",
      "[INFO] 05:07:20 Processed and deleted subgraph_3_3_4_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:07:20 Starting subgraph_3_3_4_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:13<01:54,  1.42it/s, 21.951548]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:08:34 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:34<00:00,  1.12s/it, 92.874344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:12:55 Trained model on subgraph_3_3_4_8.pt. Now saving predictions.\n",
      "[INFO] 05:12:55 Processed and deleted subgraph_3_3_4_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:12:55 Starting subgraph_3_3_4_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 136/300 [01:12<01:43,  1.59it/s, 38.035980]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:14:07 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 104.917290]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:18:27 Trained model on subgraph_3_3_4_9.pt. Now saving predictions.\n",
      "[INFO] 05:18:27 Processed and deleted subgraph_3_3_4_9.pt\n",
      "[INFO] 05:18:27 Starting subgraph_3_3_5_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:12<01:39,  1.67it/s, 4.930225] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:19:40 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:39<00:00,  1.13s/it, 59.700542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:24:07 Trained model on subgraph_3_3_5_0.pt. Now saving predictions.\n",
      "[INFO] 05:24:07 Processed and deleted subgraph_3_3_5_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:24:07 Starting subgraph_3_3_5_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▎     | 131/300 [01:12<01:40,  1.68it/s, 23.392971]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:25:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 94.756737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:29:40 Trained model on subgraph_3_3_5_1.pt. Now saving predictions.\n",
      "[INFO] 05:29:40 Processed and deleted subgraph_3_3_5_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:29:40 Starting subgraph_3_3_5_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 132/300 [01:15<01:39,  1.70it/s, 63.577450]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:30:55 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:36<00:00,  1.12s/it, 135.922791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:35:16 Trained model on subgraph_3_3_5_10.pt. Now saving predictions.\n",
      "[INFO] 05:35:17 Processed and deleted subgraph_3_3_5_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:35:17 Starting subgraph_3_3_5_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▎     | 131/300 [01:13<02:07,  1.33it/s, 60.750862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:36:30 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:35<00:00,  1.12s/it, 138.463989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:40:52 Trained model on subgraph_3_3_5_11.pt. Now saving predictions.\n",
      "[INFO] 05:40:52 Processed and deleted subgraph_3_3_5_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:40:52 Starting subgraph_3_3_5_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:13<01:34,  1.75it/s, 58.979950]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:42:05 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:41<00:00,  1.14s/it, 131.405075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:46:34 Trained model on subgraph_3_3_5_12.pt. Now saving predictions.\n",
      "[INFO] 05:46:34 Processed and deleted subgraph_3_3_5_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:46:34 Starting subgraph_3_3_5_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  44%|████▍     | 132/300 [01:12<01:46,  1.58it/s, 22.017292]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:47:46 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:30<00:00,  1.10s/it, 93.915482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:52:05 Trained model on subgraph_3_3_5_2.pt. Now saving predictions.\n",
      "[INFO] 05:52:05 Processed and deleted subgraph_3_3_5_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:52:05 Starting subgraph_3_3_5_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:14<01:30,  1.80it/s, 5.497364] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:53:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 75.558594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:57:38 Trained model on subgraph_3_3_5_3.pt. Now saving predictions.\n",
      "[INFO] 05:57:38 Processed and deleted subgraph_3_3_5_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:57:38 Starting subgraph_3_3_5_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 137/300 [01:14<01:23,  1.95it/s, 10.329289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 05:58:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:52<00:00,  1.18s/it, 81.932579] \n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:03:31 Trained model on subgraph_3_3_5_4.pt. Now saving predictions.\n",
      "[INFO] 06:03:31 Processed and deleted subgraph_3_3_5_4.pt\n",
      "[INFO] 06:03:31 Starting subgraph_3_3_5_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 136/300 [01:13<01:32,  1.78it/s, 49.023453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:04:45 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:37<00:00,  1.12s/it, 125.772331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:09:09 Trained model on subgraph_3_3_5_5.pt. Now saving predictions.\n",
      "[INFO] 06:09:09 Processed and deleted subgraph_3_3_5_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:09:09 Starting subgraph_3_3_5_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  46%|████▌     | 138/300 [01:14<01:34,  1.71it/s, 74.269524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:10:24 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 152.045563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:14:42 Trained model on subgraph_3_3_5_6.pt. Now saving predictions.\n",
      "[INFO] 06:14:42 Processed and deleted subgraph_3_3_5_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:14:42 Starting subgraph_3_3_5_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  47%|████▋     | 142/300 [01:13<01:33,  1.69it/s, 64.704926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:15:55 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 138.472092]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:20:15 Trained model on subgraph_3_3_5_7.pt. Now saving predictions.\n",
      "[INFO] 06:20:15 Processed and deleted subgraph_3_3_5_7.pt\n",
      "[INFO] 06:20:15 Starting subgraph_3_3_5_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▌     | 136/300 [01:13<01:37,  1.69it/s, 29.360935]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:21:29 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 102.029465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:25:48 Trained model on subgraph_3_3_5_8.pt. Now saving predictions.\n",
      "[INFO] 06:25:48 Processed and deleted subgraph_3_3_5_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:25:48 Starting subgraph_3_3_5_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  45%|████▍     | 134/300 [01:12<01:38,  1.69it/s, 48.483875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:27:01 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 300/300 [05:32<00:00,  1.11s/it, 114.686089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:31:21 Trained model on subgraph_3_3_5_9.pt. Now saving predictions.\n",
      "[INFO] 06:31:21 Processed and deleted subgraph_3_3_5_9.pt\n",
      "[INFO] 06:31:21 Starting Iteration 2 for region 3 3\n",
      "[INFO] 06:31:21 Running PART 3 (iter: 2)\n",
      "[INFO] 06:31:32 Dataframe Read. Size = 553.427891 MB\n",
      "[INFO] 06:31:32 Will generate 78 subgraphs (6 rows x 13 columns)\n",
      "[INFO] 06:31:32 Generating subgraph 0 0 for Region 3 3\n",
      "[INFO] 06:31:32 Started processing subregion 3 3, subgraph id: 0 0\n",
      "[INFO] 06:31:32 Generating subgraph 1 0 for Region 3 3\n",
      "[INFO] 06:31:32 Started processing subregion 3 3, subgraph id: 1 0\n",
      "[INFO] 06:31:32 Generating subgraph 2 0 for Region 3 3\n",
      "[INFO] 06:31:32 Started processing subregion 3 3, subgraph id: 2 0\n",
      "[INFO] 06:31:32 Generating subgraph 3 0 for Region 3 3\n",
      "[INFO] 06:31:32 Started processing subregion 3 3, subgraph id: 3 0\n",
      "[INFO] 06:31:32 Generating subgraph 4 0 for Region 3 3\n",
      "[INFO] 06:31:32 Started processing subregion 3 3, subgraph id: 4 0\n",
      "[INFO] 06:31:32 Generating subgraph 5 0 for Region 3 3\n",
      "[INFO] 06:31:32 Started processing subregion 3 3, subgraph id: 5 0\n",
      "[INFO] 06:31:32 Loaded mask for subgraph : 0, 0\n",
      "[INFO] 06:31:32 Loaded mask for subgraph : 2, 0\n",
      "[INFO] 06:31:32 Loaded mask for subgraph : 1, 0\n",
      "[INFO] 06:31:32 Loaded mask for subgraph : 3, 0\n",
      "[INFO] 06:31:32 Loaded mask for subgraph : 4, 0[INFO] 06:31:32 Loaded mask for subgraph : 5, 0\n",
      "\n",
      "[INFO] 06:31:37 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_0.pt\n",
      "[INFO] 06:31:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_0.pt\n",
      "[INFO] 06:31:39 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_0.pt\n",
      "[INFO] 06:31:39 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_0.pt\n",
      "[INFO] 06:31:39 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_0.pt\n",
      "[INFO] 06:31:39 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_0.pt\n",
      "[INFO] 06:31:39 Generating subgraph 0 1 for Region 3 3\n",
      "[INFO] 06:31:39 Generating subgraph 1 1 for Region 3 3\n",
      "[INFO] 06:31:39 Generating subgraph 2 1 for Region 3 3\n",
      "[INFO] 06:31:39 Generating subgraph 3 1 for Region 3 3\n",
      "[INFO] 06:31:39 Generating subgraph 4 1 for Region 3 3\n",
      "[INFO] 06:31:39 Generating subgraph 5 1 for Region 3 3\n",
      "[INFO] 06:31:39 Started processing subregion 3 3, subgraph id: 0 1\n",
      "[INFO] 06:31:39 Started processing subregion 3 3, subgraph id: 1 1\n",
      "[INFO] 06:31:39 Started processing subregion 3 3, subgraph id: 2 1\n",
      "[INFO] 06:31:39 Started processing subregion 3 3, subgraph id: 3 1\n",
      "[INFO] 06:31:39 Started processing subregion 3 3, subgraph id: 4 1\n",
      "[INFO] 06:31:39 Started processing subregion 3 3, subgraph id: 5 1\n",
      "[INFO] 06:31:40 Loaded mask for subgraph : 3, 1[INFO] 06:31:40 Loaded mask for subgraph : 4, 1\n",
      "\n",
      "[INFO] 06:31:40 Loaded mask for subgraph : 0, 1\n",
      "[INFO] 06:31:40 Loaded mask for subgraph : 2, 1\n",
      "[INFO] 06:31:40 Loaded mask for subgraph : 1, 1\n",
      "[INFO] 06:31:40 Loaded mask for subgraph : 5, 1\n",
      "[INFO] 06:31:44 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_1.pt\n",
      "[INFO] 06:31:45 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_1.pt\n",
      "[INFO] 06:31:46 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_1.pt\n",
      "[INFO] 06:31:46 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_1.pt\n",
      "[INFO] 06:31:46 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_1.pt\n",
      "[INFO] 06:31:46 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_1.pt\n",
      "[INFO] 06:31:46 Generating subgraph 0 2 for Region 3 3\n",
      "[INFO] 06:31:46 Generating subgraph 1 2 for Region 3 3\n",
      "[INFO] 06:31:46 Generating subgraph 2 2 for Region 3 3\n",
      "[INFO] 06:31:46 Generating subgraph 3 2 for Region 3 3\n",
      "[INFO] 06:31:46 Generating subgraph 4 2 for Region 3 3\n",
      "[INFO] 06:31:46 Generating subgraph 5 2 for Region 3 3\n",
      "[INFO] 06:31:46 Started processing subregion 3 3, subgraph id: 0 2\n",
      "[INFO] 06:31:46 Started processing subregion 3 3, subgraph id: 1 2\n",
      "[INFO] 06:31:46 Started processing subregion 3 3, subgraph id: 2 2\n",
      "[INFO] 06:31:46 Started processing subregion 3 3, subgraph id: 3 2\n",
      "[INFO] 06:31:46 Started processing subregion 3 3, subgraph id: 4 2\n",
      "[INFO] 06:31:46 Started processing subregion 3 3, subgraph id: 5 2\n",
      "[INFO] 06:31:46 Loaded mask for subgraph : 0, 2\n",
      "[INFO] 06:31:46 Loaded mask for subgraph : 3, 2\n",
      "[INFO] 06:31:46 Loaded mask for subgraph : 4, 2\n",
      "[INFO] 06:31:46 Loaded mask for subgraph : 2, 2\n",
      "[INFO] 06:31:46 Loaded mask for subgraph : 1, 2\n",
      "[INFO] 06:31:47 Loaded mask for subgraph : 5, 2\n",
      "[INFO] 06:31:52 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_2.pt\n",
      "[INFO] 06:31:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_2.pt\n",
      "[INFO] 06:31:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_2.pt\n",
      "[INFO] 06:31:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_2.pt\n",
      "[INFO] 06:31:54 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_2.pt\n",
      "[INFO] 06:31:54 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_2.pt\n",
      "[INFO] 06:31:54 Generating subgraph 0 3 for Region 3 3\n",
      "[INFO] 06:31:54 Generating subgraph 1 3 for Region 3 3\n",
      "[INFO] 06:31:54 Generating subgraph 2 3 for Region 3 3\n",
      "[INFO] 06:31:54 Generating subgraph 3 3 for Region 3 3\n",
      "[INFO] 06:31:54 Generating subgraph 4 3 for Region 3 3\n",
      "[INFO] 06:31:54 Generating subgraph 5 3 for Region 3 3\n",
      "[INFO] 06:31:54 Started processing subregion 3 3, subgraph id: 0 3\n",
      "[INFO] 06:31:54 Started processing subregion 3 3, subgraph id: 1 3\n",
      "[INFO] 06:31:54 Started processing subregion 3 3, subgraph id: 2 3\n",
      "[INFO] 06:31:54 Started processing subregion 3 3, subgraph id: 3 3\n",
      "[INFO] 06:31:54 Started processing subregion 3 3, subgraph id: 4 3\n",
      "[INFO] 06:31:54 Started processing subregion 3 3, subgraph id: 5 3\n",
      "[INFO] 06:31:54 Loaded mask for subgraph : 5, 3\n",
      "[INFO] 06:31:54 Loaded mask for subgraph : 1, 3\n",
      "[INFO] 06:31:54 Loaded mask for subgraph : 0, 3\n",
      "[INFO] 06:31:54 Loaded mask for subgraph : 3, 3\n",
      "[INFO] 06:31:54 Loaded mask for subgraph : 2, 3\n",
      "[INFO] 06:31:54 Loaded mask for subgraph : 4, 3\n",
      "[INFO] 06:31:58 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_3.pt\n",
      "[INFO] 06:31:59 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_3.pt\n",
      "[INFO] 06:31:59 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_3.pt\n",
      "[INFO] 06:32:00 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_3.pt\n",
      "[INFO] 06:32:00 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_3.pt\n",
      "[INFO] 06:32:00 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_3.pt\n",
      "[INFO] 06:32:00 Generating subgraph 0 4 for Region 3 3\n",
      "[INFO] 06:32:00 Generating subgraph 1 4 for Region 3 3\n",
      "[INFO] 06:32:00 Generating subgraph 2 4 for Region 3 3\n",
      "[INFO] 06:32:00 Generating subgraph 3 4 for Region 3 3\n",
      "[INFO] 06:32:00 Generating subgraph 4 4 for Region 3 3\n",
      "[INFO] 06:32:00 Generating subgraph 5 4 for Region 3 3\n",
      "[INFO] 06:32:00 Started processing subregion 3 3, subgraph id: 0 4\n",
      "[INFO] 06:32:00 Started processing subregion 3 3, subgraph id: 1 4\n",
      "[INFO] 06:32:00 Started processing subregion 3 3, subgraph id: 2 4\n",
      "[INFO] 06:32:00 Started processing subregion 3 3, subgraph id: 3 4\n",
      "[INFO] 06:32:00 Started processing subregion 3 3, subgraph id: 4 4\n",
      "[INFO] 06:32:00 Started processing subregion 3 3, subgraph id: 5 4\n",
      "[INFO] 06:32:01 Loaded mask for subgraph : 0, 4[INFO] 06:32:01 Loaded mask for subgraph : 4, 4\n",
      "\n",
      "[INFO] 06:32:01 Loaded mask for subgraph : 5, 4\n",
      "[INFO] 06:32:01 Loaded mask for subgraph : 3, 4\n",
      "[INFO] 06:32:01 Loaded mask for subgraph : 2, 4\n",
      "[INFO] 06:32:01 Loaded mask for subgraph : 1, 4\n",
      "[INFO] 06:32:06 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_4.pt\n",
      "[INFO] 06:32:06 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_4.pt\n",
      "[INFO] 06:32:07 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_4.pt[INFO] 06:32:07 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_4.pt\n",
      "\n",
      "[INFO] 06:32:07 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_4.pt\n",
      "[INFO] 06:32:07 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_4.pt\n",
      "[INFO] 06:32:07 Generating subgraph 0 5 for Region 3 3\n",
      "[INFO] 06:32:07 Generating subgraph 1 5 for Region 3 3\n",
      "[INFO] 06:32:07 Generating subgraph 2 5 for Region 3 3\n",
      "[INFO] 06:32:07 Generating subgraph 3 5 for Region 3 3\n",
      "[INFO] 06:32:07 Generating subgraph 4 5 for Region 3 3\n",
      "[INFO] 06:32:07 Generating subgraph 5 5 for Region 3 3\n",
      "[INFO] 06:32:07 Started processing subregion 3 3, subgraph id: 0 5\n",
      "[INFO] 06:32:07 Started processing subregion 3 3, subgraph id: 1 5\n",
      "[INFO] 06:32:07 Started processing subregion 3 3, subgraph id: 2 5\n",
      "[INFO] 06:32:07 Started processing subregion 3 3, subgraph id: 3 5\n",
      "[INFO] 06:32:07 Started processing subregion 3 3, subgraph id: 4 5\n",
      "[INFO] 06:32:07 Started processing subregion 3 3, subgraph id: 5 5\n",
      "[INFO] 06:32:07 Loaded mask for subgraph : 2, 5[INFO] 06:32:07 Loaded mask for subgraph : 4, 5\n",
      "\n",
      "[INFO] 06:32:07 Loaded mask for subgraph : 3, 5\n",
      "[INFO] 06:32:07 Loaded mask for subgraph : 0, 5\n",
      "[INFO] 06:32:07 Loaded mask for subgraph : 5, 5\n",
      "[INFO] 06:32:07 Loaded mask for subgraph : 1, 5\n",
      "[INFO] 06:32:12 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_5.pt\n",
      "[INFO] 06:32:12 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_5.pt\n",
      "[INFO] 06:32:12 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_5.pt\n",
      "[INFO] 06:32:12 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_5.pt\n",
      "[INFO] 06:32:13 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_5.pt\n",
      "[INFO] 06:32:13 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_5.pt\n",
      "[INFO] 06:32:13 Generating subgraph 0 6 for Region 3 3\n",
      "[INFO] 06:32:13 Generating subgraph 1 6 for Region 3 3\n",
      "[INFO] 06:32:13 Generating subgraph 2 6 for Region 3 3\n",
      "[INFO] 06:32:13 Generating subgraph 3 6 for Region 3 3\n",
      "[INFO] 06:32:13 Generating subgraph 4 6 for Region 3 3\n",
      "[INFO] 06:32:13 Generating subgraph 5 6 for Region 3 3\n",
      "[INFO] 06:32:13 Started processing subregion 3 3, subgraph id: 0 6\n",
      "[INFO] 06:32:13 Started processing subregion 3 3, subgraph id: 1 6\n",
      "[INFO] 06:32:13 Started processing subregion 3 3, subgraph id: 2 6\n",
      "[INFO] 06:32:13 Started processing subregion 3 3, subgraph id: 3 6\n",
      "[INFO] 06:32:13 Started processing subregion 3 3, subgraph id: 4 6\n",
      "[INFO] 06:32:13 Started processing subregion 3 3, subgraph id: 5 6\n",
      "[INFO] 06:32:13 Loaded mask for subgraph : 0, 6\n",
      "[INFO] 06:32:13 Loaded mask for subgraph : 4, 6\n",
      "[INFO] 06:32:13 Loaded mask for subgraph : 2, 6\n",
      "[INFO] 06:32:13 Loaded mask for subgraph : 1, 6\n",
      "[INFO] 06:32:13 Loaded mask for subgraph : 3, 6\n",
      "[INFO] 06:32:13 Loaded mask for subgraph : 5, 6\n",
      "[INFO] 06:32:18 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_6.pt\n",
      "[INFO] 06:32:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_6.pt\n",
      "[INFO] 06:32:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_6.pt\n",
      "[INFO] 06:32:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_6.pt\n",
      "[INFO] 06:32:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_6.pt\n",
      "[INFO] 06:32:19 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_6.pt\n",
      "[INFO] 06:32:19 Generating subgraph 0 7 for Region 3 3\n",
      "[INFO] 06:32:19 Generating subgraph 1 7 for Region 3 3\n",
      "[INFO] 06:32:19 Generating subgraph 2 7 for Region 3 3\n",
      "[INFO] 06:32:19 Generating subgraph 3 7 for Region 3 3\n",
      "[INFO] 06:32:19 Generating subgraph 4 7 for Region 3 3\n",
      "[INFO] 06:32:19 Generating subgraph 5 7 for Region 3 3\n",
      "[INFO] 06:32:19 Started processing subregion 3 3, subgraph id: 0 7\n",
      "[INFO] 06:32:19 Started processing subregion 3 3, subgraph id: 1 7\n",
      "[INFO] 06:32:19 Started processing subregion 3 3, subgraph id: 2 7\n",
      "[INFO] 06:32:19 Started processing subregion 3 3, subgraph id: 3 7\n",
      "[INFO] 06:32:19 Started processing subregion 3 3, subgraph id: 4 7\n",
      "[INFO] 06:32:19 Started processing subregion 3 3, subgraph id: 5 7\n",
      "[INFO] 06:32:20 Loaded mask for subgraph : 0, 7\n",
      "[INFO] 06:32:20 Loaded mask for subgraph : 2, 7\n",
      "[INFO] 06:32:20 Loaded mask for subgraph : 4, 7\n",
      "[INFO] 06:32:20 Loaded mask for subgraph : 1, 7\n",
      "[INFO] 06:32:20 Loaded mask for subgraph : 3, 7\n",
      "[INFO] 06:32:20 Loaded mask for subgraph : 5, 7\n",
      "[INFO] 06:32:26 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_7.pt\n",
      "[INFO] 06:32:26 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_7.pt\n",
      "[INFO] 06:32:26 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_7.pt\n",
      "[INFO] 06:32:26 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_7.pt\n",
      "[INFO] 06:32:26 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_7.pt\n",
      "[INFO] 06:32:26 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_7.pt\n",
      "[INFO] 06:32:26 Generating subgraph 0 8 for Region 3 3\n",
      "[INFO] 06:32:26 Generating subgraph 1 8 for Region 3 3\n",
      "[INFO] 06:32:26 Generating subgraph 2 8 for Region 3 3\n",
      "[INFO] 06:32:26 Generating subgraph 3 8 for Region 3 3\n",
      "[INFO] 06:32:26 Generating subgraph 4 8 for Region 3 3\n",
      "[INFO] 06:32:26 Generating subgraph 5 8 for Region 3 3\n",
      "[INFO] 06:32:26 Started processing subregion 3 3, subgraph id: 0 8\n",
      "[INFO] 06:32:26 Started processing subregion 3 3, subgraph id: 1 8\n",
      "[INFO] 06:32:26 Started processing subregion 3 3, subgraph id: 2 8\n",
      "[INFO] 06:32:26 Started processing subregion 3 3, subgraph id: 3 8\n",
      "[INFO] 06:32:26 Started processing subregion 3 3, subgraph id: 4 8\n",
      "[INFO] 06:32:26 Started processing subregion 3 3, subgraph id: 5 8\n",
      "[INFO] 06:32:27 Loaded mask for subgraph : 0, 8\n",
      "[INFO] 06:32:27 Loaded mask for subgraph : 2, 8\n",
      "[INFO] 06:32:27 Loaded mask for subgraph : 1, 8\n",
      "[INFO] 06:32:27 Loaded mask for subgraph : 4, 8\n",
      "[INFO] 06:32:27 Loaded mask for subgraph : 3, 8\n",
      "[INFO] 06:32:27 Loaded mask for subgraph : 5, 8\n",
      "[INFO] 06:32:32 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_8.pt\n",
      "[INFO] 06:32:32 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_8.pt\n",
      "[INFO] 06:32:32 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_8.pt\n",
      "[INFO] 06:32:32 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_8.pt\n",
      "[INFO] 06:32:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_8.pt\n",
      "[INFO] 06:32:33 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_8.pt\n",
      "[INFO] 06:32:33 Generating subgraph 0 9 for Region 3 3\n",
      "[INFO] 06:32:33 Generating subgraph 1 9 for Region 3 3\n",
      "[INFO] 06:32:33 Generating subgraph 2 9 for Region 3 3\n",
      "[INFO] 06:32:33 Generating subgraph 3 9 for Region 3 3\n",
      "[INFO] 06:32:33 Generating subgraph 4 9 for Region 3 3\n",
      "[INFO] 06:32:33 Generating subgraph 5 9 for Region 3 3\n",
      "[INFO] 06:32:33 Started processing subregion 3 3, subgraph id: 0 9\n",
      "[INFO] 06:32:33 Started processing subregion 3 3, subgraph id: 1 9\n",
      "[INFO] 06:32:33 Started processing subregion 3 3, subgraph id: 2 9\n",
      "[INFO] 06:32:33 Started processing subregion 3 3, subgraph id: 3 9\n",
      "[INFO] 06:32:33 Started processing subregion 3 3, subgraph id: 4 9\n",
      "[INFO] 06:32:33 Started processing subregion 3 3, subgraph id: 5 9\n",
      "[INFO] 06:32:34 Loaded mask for subgraph : 0, 9\n",
      "[INFO] 06:32:34 Loaded mask for subgraph : 1, 9\n",
      "[INFO] 06:32:34 Loaded mask for subgraph : 3, 9\n",
      "[INFO] 06:32:34 Loaded mask for subgraph : 2, 9\n",
      "[INFO] 06:32:34 Loaded mask for subgraph : 5, 9\n",
      "[INFO] 06:32:34 Loaded mask for subgraph : 4, 9\n",
      "[INFO] 06:32:38 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_9.pt\n",
      "[INFO] 06:32:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_9.pt\n",
      "[INFO] 06:32:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_9.pt\n",
      "[INFO] 06:32:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_9.pt\n",
      "[INFO] 06:32:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_9.pt\n",
      "[INFO] 06:32:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_9.pt\n",
      "[INFO] 06:32:40 Generating subgraph 0 10 for Region 3 3\n",
      "[INFO] 06:32:40 Generating subgraph 1 10 for Region 3 3\n",
      "[INFO] 06:32:40 Generating subgraph 2 10 for Region 3 3\n",
      "[INFO] 06:32:40 Generating subgraph 3 10 for Region 3 3\n",
      "[INFO] 06:32:40 Generating subgraph 4 10 for Region 3 3\n",
      "[INFO] 06:32:40 Generating subgraph 5 10 for Region 3 3\n",
      "[INFO] 06:32:40 Started processing subregion 3 3, subgraph id: 0 10\n",
      "[INFO] 06:32:40 Started processing subregion 3 3, subgraph id: 1 10\n",
      "[INFO] 06:32:40 Started processing subregion 3 3, subgraph id: 2 10\n",
      "[INFO] 06:32:40 Started processing subregion 3 3, subgraph id: 3 10\n",
      "[INFO] 06:32:40 Started processing subregion 3 3, subgraph id: 4 10\n",
      "[INFO] 06:32:40 Started processing subregion 3 3, subgraph id: 5 10\n",
      "[INFO] 06:32:40 Loaded mask for subgraph : 2, 10[INFO] 06:32:40 Loaded mask for subgraph : 3, 10\n",
      "[INFO] 06:32:40 Loaded mask for subgraph : 4, 10\n",
      "\n",
      "[INFO] 06:32:40 Loaded mask for subgraph : 0, 10\n",
      "[INFO] 06:32:40 Loaded mask for subgraph : 5, 10\n",
      "[INFO] 06:32:40 Loaded mask for subgraph : 1, 10\n",
      "[INFO] 06:32:46 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_10.pt\n",
      "[INFO] 06:32:46 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_10.pt\n",
      "[INFO] 06:32:46 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_10.pt\n",
      "[INFO] 06:32:47 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_10.pt\n",
      "[INFO] 06:32:47 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_10.pt\n",
      "[INFO] 06:32:47 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_10.pt\n",
      "[INFO] 06:32:47 Generating subgraph 0 11 for Region 3 3\n",
      "[INFO] 06:32:47 Generating subgraph 1 11 for Region 3 3\n",
      "[INFO] 06:32:47 Generating subgraph 2 11 for Region 3 3\n",
      "[INFO] 06:32:47 Generating subgraph 3 11 for Region 3 3\n",
      "[INFO] 06:32:47 Generating subgraph 4 11 for Region 3 3\n",
      "[INFO] 06:32:47 Generating subgraph 5 11 for Region 3 3\n",
      "[INFO] 06:32:47 Started processing subregion 3 3, subgraph id: 0 11\n",
      "[INFO] 06:32:47 Started processing subregion 3 3, subgraph id: 1 11\n",
      "[INFO] 06:32:47 Started processing subregion 3 3, subgraph id: 2 11\n",
      "[INFO] 06:32:47 Started processing subregion 3 3, subgraph id: 3 11\n",
      "[INFO] 06:32:47 Started processing subregion 3 3, subgraph id: 4 11\n",
      "[INFO] 06:32:47 Started processing subregion 3 3, subgraph id: 5 11\n",
      "[INFO] 06:32:48 Loaded mask for subgraph : 3, 11\n",
      "[INFO] 06:32:48 Loaded mask for subgraph : 1, 11\n",
      "[INFO] 06:32:48 Loaded mask for subgraph : 2, 11\n",
      "[INFO] 06:32:48 Loaded mask for subgraph : 0, 11\n",
      "[INFO] 06:32:48 Loaded mask for subgraph : 5, 11\n",
      "[INFO] 06:32:48 Loaded mask for subgraph : 4, 11\n",
      "[INFO] 06:32:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_11.pt\n",
      "[INFO] 06:32:53 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_11.pt\n",
      "[INFO] 06:32:54 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_11.pt\n",
      "[INFO] 06:32:54 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_11.pt\n",
      "[INFO] 06:32:55 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_11.pt\n",
      "[INFO] 06:32:55 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_11.pt\n",
      "[INFO] 06:32:55 Generating subgraph 0 12 for Region 3 3\n",
      "[INFO] 06:32:55 Generating subgraph 1 12 for Region 3 3\n",
      "[INFO] 06:32:55 Generating subgraph 2 12 for Region 3 3\n",
      "[INFO] 06:32:55 Generating subgraph 3 12 for Region 3 3\n",
      "[INFO] 06:32:55 Generating subgraph 4 12 for Region 3 3\n",
      "[INFO] 06:32:55 Generating subgraph 5 12 for Region 3 3\n",
      "[INFO] 06:32:55 Started processing subregion 3 3, subgraph id: 0 12\n",
      "[INFO] 06:32:55 Started processing subregion 3 3, subgraph id: 1 12\n",
      "[INFO] 06:32:55 Started processing subregion 3 3, subgraph id: 2 12\n",
      "[INFO] 06:32:55 Started processing subregion 3 3, subgraph id: 3 12\n",
      "[INFO] 06:32:55 Started processing subregion 3 3, subgraph id: 4 12\n",
      "[INFO] 06:32:55 Started processing subregion 3 3, subgraph id: 5 12\n",
      "[INFO] 06:32:55 Loaded mask for subgraph : 0, 12\n",
      "[INFO] 06:32:55 Loaded mask for subgraph : 2, 12\n",
      "[INFO] 06:32:55 Loaded mask for subgraph : 5, 12\n",
      "[INFO] 06:32:55 Loaded mask for subgraph : 1, 12\n",
      "[INFO] 06:32:55 Loaded mask for subgraph : 4, 12\n",
      "[INFO] 06:32:55 Loaded mask for subgraph : 3, 12\n",
      "[INFO] 06:33:00 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_12.pt\n",
      "[INFO] 06:33:00 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_12.pt\n",
      "[INFO] 06:33:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_12.pt\n",
      "[INFO] 06:33:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_12.pt\n",
      "[INFO] 06:33:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_12.pt\n",
      "[INFO] 06:33:02 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_12.pt\n",
      "[INFO] 06:33:02 Running PART 4 (iter: 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"./drive/MyDrive/ISRO_SuperResolution/models/{x}_{y}.pth\"))\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:33:02 Starting subgraph_3_3_0_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  34%|███▍      | 51/150 [00:30<00:49,  2.02it/s, 9.760057]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:33:33 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:57<00:00,  1.18s/it, 29.563683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:36:00 Trained model on subgraph_3_3_0_0.pt. Now saving predictions.\n",
      "[INFO] 06:36:00 Processed and deleted subgraph_3_3_0_0.pt\n",
      "[INFO] 06:36:00 Starting subgraph_3_3_0_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:11<01:43,  1.89s/it, 27.134825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:37:11 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 25.694075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:38:45 Trained model on subgraph_3_3_0_1.pt. Now saving predictions.\n",
      "[INFO] 06:38:45 Processed and deleted subgraph_3_3_0_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:38:45 Starting subgraph_3_3_0_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:09<01:51,  1.99s/it, 49.049004]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:39:56 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 46.980633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:41:30 Trained model on subgraph_3_3_0_10.pt. Now saving predictions.\n",
      "[INFO] 06:41:30 Processed and deleted subgraph_3_3_0_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:41:30 Starting subgraph_3_3_0_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:07<01:41,  1.81s/it, 86.883308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:42:39 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:42<00:00,  1.09s/it, 85.084068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:44:13 Trained model on subgraph_3_3_0_11.pt. Now saving predictions.\n",
      "[INFO] 06:44:13 Processed and deleted subgraph_3_3_0_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:44:13 Starting subgraph_3_3_0_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:10<01:43,  1.84s/it, 99.096214] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:45:25 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 96.882782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:47:00 Trained model on subgraph_3_3_0_12.pt. Now saving predictions.\n",
      "[INFO] 06:47:00 Processed and deleted subgraph_3_3_0_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:47:00 Starting subgraph_3_3_0_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:10<01:48,  1.90s/it, 25.774609] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:48:10 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 24.452541]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:49:47 Trained model on subgraph_3_3_0_2.pt. Now saving predictions.\n",
      "[INFO] 06:49:47 Processed and deleted subgraph_3_3_0_2.pt\n",
      "[INFO] 06:49:47 Starting subgraph_3_3_0_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:07<01:43,  1.82s/it, 31.803066]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:50:56 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 31.268713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:52:33 Trained model on subgraph_3_3_0_3.pt. Now saving predictions.\n",
      "[INFO] 06:52:33 Processed and deleted subgraph_3_3_0_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:52:33 Starting subgraph_3_3_0_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:10<01:43,  1.82s/it, 37.138718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:53:43 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.12s/it, 36.537350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:55:21 Trained model on subgraph_3_3_0_4.pt. Now saving predictions.\n",
      "[INFO] 06:55:21 Processed and deleted subgraph_3_3_0_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:55:21 Starting subgraph_3_3_0_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:07<01:47,  1.88s/it, 57.361919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:56:29 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 56.033867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:58:05 Trained model on subgraph_3_3_0_5.pt. Now saving predictions.\n",
      "[INFO] 06:58:05 Processed and deleted subgraph_3_3_0_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:58:05 Starting subgraph_3_3_0_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:10<01:45,  1.89s/it, 54.859089]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 06:59:16 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 54.110352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:00:52 Trained model on subgraph_3_3_0_6.pt. Now saving predictions.\n",
      "[INFO] 07:00:52 Processed and deleted subgraph_3_3_0_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:00:52 Starting subgraph_3_3_0_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:44,  1.84s/it, 48.226883]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:02:02 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 47.327225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:03:39 Trained model on subgraph_3_3_0_7.pt. Now saving predictions.\n",
      "[INFO] 07:03:39 Processed and deleted subgraph_3_3_0_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:03:39 Starting subgraph_3_3_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:10<01:47,  1.95s/it, 43.450611]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:04:50 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.09s/it, 42.675774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:06:23 Trained model on subgraph_3_3_0_8.pt. Now saving predictions.\n",
      "[INFO] 07:06:23 Processed and deleted subgraph_3_3_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:06:23 Starting subgraph_3_3_0_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:43,  1.82s/it, 45.666531]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:07:33 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 44.820061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:09:09 Trained model on subgraph_3_3_0_9.pt. Now saving predictions.\n",
      "[INFO] 07:09:09 Processed and deleted subgraph_3_3_0_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:09:09 Starting subgraph_3_3_1_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:10<01:46,  1.87s/it, 33.233723] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:10:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:51<00:00,  1.14s/it, 32.084126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:12:00 Trained model on subgraph_3_3_1_0.pt. Now saving predictions.\n",
      "[INFO] 07:12:01 Processed and deleted subgraph_3_3_1_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:12:01 Starting subgraph_3_3_1_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<01:55,  1.99s/it, 32.607658]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:13:11 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.12s/it, 31.872946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:14:48 Trained model on subgraph_3_3_1_1.pt. Now saving predictions.\n",
      "[INFO] 07:14:48 Processed and deleted subgraph_3_3_1_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:14:48 Starting subgraph_3_3_1_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:42,  1.80s/it, 52.553001] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:15:58 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.11s/it, 50.871780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:17:35 Trained model on subgraph_3_3_1_10.pt. Now saving predictions.\n",
      "[INFO] 07:17:35 Processed and deleted subgraph_3_3_1_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:17:36 Starting subgraph_3_3_1_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:12<01:40,  1.83s/it, 69.799507]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:18:48 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.11s/it, 68.979118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:20:22 Trained model on subgraph_3_3_1_11.pt. Now saving predictions.\n",
      "[INFO] 07:20:22 Processed and deleted subgraph_3_3_1_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:20:22 Starting subgraph_3_3_1_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<02:02,  2.12s/it, 89.460274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:21:32 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:49<00:00,  1.13s/it, 83.408524]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:23:11 Trained model on subgraph_3_3_1_12.pt. Now saving predictions.\n",
      "[INFO] 07:23:11 Processed and deleted subgraph_3_3_1_12.pt\n",
      "[INFO] 07:23:11 Starting subgraph_3_3_1_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:09<01:41,  1.84s/it, 36.075172] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:24:22 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:43<00:00,  1.09s/it, 35.091850]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:25:55 Trained model on subgraph_3_3_1_2.pt. Now saving predictions.\n",
      "[INFO] 07:25:55 Processed and deleted subgraph_3_3_1_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:25:55 Starting subgraph_3_3_1_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:45,  1.84s/it, 46.931198]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:27:05 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 46.181835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:28:42 Trained model on subgraph_3_3_1_3.pt. Now saving predictions.\n",
      "[INFO] 07:28:42 Processed and deleted subgraph_3_3_1_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:28:42 Starting subgraph_3_3_1_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:10<01:45,  1.86s/it, 46.458839]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:29:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.12s/it, 45.705349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:31:30 Trained model on subgraph_3_3_1_4.pt. Now saving predictions.\n",
      "[INFO] 07:31:30 Processed and deleted subgraph_3_3_1_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:31:30 Starting subgraph_3_3_1_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:46,  1.88s/it, 82.914803]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:32:38 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 81.883545]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:34:15 Trained model on subgraph_3_3_1_5.pt. Now saving predictions.\n",
      "[INFO] 07:34:15 Processed and deleted subgraph_3_3_1_5.pt\n",
      "[INFO] 07:34:15 Starting subgraph_3_3_1_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:07<01:43,  1.82s/it, 98.806297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:35:24 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 97.657669]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:37:01 Trained model on subgraph_3_3_1_6.pt. Now saving predictions.\n",
      "[INFO] 07:37:01 Processed and deleted subgraph_3_3_1_6.pt\n",
      "[INFO] 07:37:01 Starting subgraph_3_3_1_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:09<01:44,  1.89s/it, 94.106308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:38:12 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:43<00:00,  1.09s/it, 92.981140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:39:45 Trained model on subgraph_3_3_1_7.pt. Now saving predictions.\n",
      "[INFO] 07:39:45 Processed and deleted subgraph_3_3_1_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:39:45 Starting subgraph_3_3_1_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████    | 91/150 [01:07<01:47,  1.82s/it, 65.885155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:40:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.12s/it, 65.282364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:42:33 Trained model on subgraph_3_3_1_8.pt. Now saving predictions.\n",
      "[INFO] 07:42:34 Processed and deleted subgraph_3_3_1_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:42:34 Starting subgraph_3_3_1_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:08<01:42,  1.83s/it, 50.191273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:43:44 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 49.130688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:45:19 Trained model on subgraph_3_3_1_9.pt. Now saving predictions.\n",
      "[INFO] 07:45:19 Processed and deleted subgraph_3_3_1_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:45:19 Starting subgraph_3_3_2_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:46,  1.87s/it, 36.053089]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:46:29 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.12s/it, 34.901817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:48:07 Trained model on subgraph_3_3_2_0.pt. Now saving predictions.\n",
      "[INFO] 07:48:07 Processed and deleted subgraph_3_3_2_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:48:07 Starting subgraph_3_3_2_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<01:47,  1.85s/it, 43.833996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:49:18 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.12s/it, 42.271206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:50:55 Trained model on subgraph_3_3_2_1.pt. Now saving predictions.\n",
      "[INFO] 07:50:56 Processed and deleted subgraph_3_3_2_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:50:56 Starting subgraph_3_3_2_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:10<01:42,  1.83s/it, 63.882504] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:52:06 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 62.241531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:53:42 Trained model on subgraph_3_3_2_10.pt. Now saving predictions.\n",
      "[INFO] 07:53:42 Processed and deleted subgraph_3_3_2_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:53:42 Starting subgraph_3_3_2_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:49,  1.92s/it, 71.467781]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:54:52 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 68.903976]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:56:28 Trained model on subgraph_3_3_2_11.pt. Now saving predictions.\n",
      "[INFO] 07:56:28 Processed and deleted subgraph_3_3_2_11.pt\n",
      "[INFO] 07:56:28 Starting subgraph_3_3_2_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:12<01:45,  1.88s/it, 83.064613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:57:41 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.12s/it, 79.956413]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 07:59:16 Trained model on subgraph_3_3_2_12.pt. Now saving predictions.\n",
      "[INFO] 07:59:16 Processed and deleted subgraph_3_3_2_12.pt\n",
      "[INFO] 07:59:16 Starting subgraph_3_3_2_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:07<01:44,  1.83s/it, 51.850128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:00:25 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 50.014187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:02:01 Trained model on subgraph_3_3_2_2.pt. Now saving predictions.\n",
      "[INFO] 08:02:01 Processed and deleted subgraph_3_3_2_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:02:01 Starting subgraph_3_3_2_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:10<01:42,  1.87s/it, 55.942837]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:03:12 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.09s/it, 54.789593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:04:45 Trained model on subgraph_3_3_2_3.pt. Now saving predictions.\n",
      "[INFO] 08:04:45 Processed and deleted subgraph_3_3_2_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:04:45 Starting subgraph_3_3_2_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<01:48,  1.88s/it, 53.383873]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:05:55 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.12s/it, 53.195690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:07:34 Trained model on subgraph_3_3_2_4.pt. Now saving predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:07:35 Processed and deleted subgraph_3_3_2_4.pt\n",
      "[INFO] 08:07:35 Starting subgraph_3_3_2_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:43,  1.82s/it, 83.634140]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:08:45 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 82.566795]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:10:21 Trained model on subgraph_3_3_2_5.pt. Now saving predictions.\n",
      "[INFO] 08:10:21 Processed and deleted subgraph_3_3_2_5.pt\n",
      "[INFO] 08:10:21 Starting subgraph_3_3_2_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<02:07,  2.21s/it, 132.097519]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:11:32 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:52<00:00,  1.15s/it, 129.724930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:13:14 Trained model on subgraph_3_3_2_6.pt. Now saving predictions.\n",
      "[INFO] 08:13:14 Processed and deleted subgraph_3_3_2_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:13:14 Starting subgraph_3_3_2_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:10<01:45,  1.84s/it, 126.229721]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:14:25 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.12s/it, 124.552795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:16:01 Trained model on subgraph_3_3_2_7.pt. Now saving predictions.\n",
      "[INFO] 08:16:02 Processed and deleted subgraph_3_3_2_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:16:02 Starting subgraph_3_3_2_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:45,  1.85s/it, 86.153580]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:17:12 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 83.128220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:18:48 Trained model on subgraph_3_3_2_8.pt. Now saving predictions.\n",
      "[INFO] 08:18:49 Processed and deleted subgraph_3_3_2_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:18:49 Starting subgraph_3_3_2_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:47,  1.89s/it, 64.984467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:19:58 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.11s/it, 64.348877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:21:34 Trained model on subgraph_3_3_2_9.pt. Now saving predictions.\n",
      "[INFO] 08:21:35 Processed and deleted subgraph_3_3_2_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:21:35 Starting subgraph_3_3_3_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:12<01:40,  1.83s/it, 33.960442] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:22:47 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 2 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 33.545040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:24:21 Trained model on subgraph_3_3_3_0.pt. Now saving predictions.\n",
      "[INFO] 08:24:21 Processed and deleted subgraph_3_3_3_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:24:21 Starting subgraph_3_3_3_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:10<01:47,  1.86s/it, 60.282608]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:25:32 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.13s/it, 59.145733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:27:10 Trained model on subgraph_3_3_3_1.pt. Now saving predictions.\n",
      "[INFO] 08:27:10 Processed and deleted subgraph_3_3_3_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:27:11 Starting subgraph_3_3_3_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:10<01:52,  2.01s/it, 61.402111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:28:21 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 59.333687]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:29:56 Trained model on subgraph_3_3_3_10.pt. Now saving predictions.\n",
      "[INFO] 08:29:56 Processed and deleted subgraph_3_3_3_10.pt\n",
      "[INFO] 08:29:56 Starting subgraph_3_3_3_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:44,  1.83s/it, 72.609825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:31:06 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 70.639053]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:32:41 Trained model on subgraph_3_3_3_11.pt. Now saving predictions.\n",
      "[INFO] 08:32:42 Processed and deleted subgraph_3_3_3_11.pt\n",
      "[INFO] 08:32:42 Starting subgraph_3_3_3_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:10<01:45,  1.85s/it, 84.732040]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:33:52 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.12s/it, 81.794792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:35:30 Trained model on subgraph_3_3_3_12.pt. Now saving predictions.\n",
      "[INFO] 08:35:30 Processed and deleted subgraph_3_3_3_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:35:30 Starting subgraph_3_3_3_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:47,  1.88s/it, 64.960358] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:36:40 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.11s/it, 63.042789]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:38:16 Trained model on subgraph_3_3_3_2.pt. Now saving predictions.\n",
      "[INFO] 08:38:16 Processed and deleted subgraph_3_3_3_2.pt\n",
      "[INFO] 08:38:16 Starting subgraph_3_3_3_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:07<01:44,  1.84s/it, 54.518085]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:39:25 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 53.696228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:41:02 Trained model on subgraph_3_3_3_3.pt. Now saving predictions.\n",
      "[INFO] 08:41:02 Processed and deleted subgraph_3_3_3_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:41:02 Starting subgraph_3_3_3_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:09<01:43,  1.85s/it, 62.691036]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:42:13 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 61.867260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:43:47 Trained model on subgraph_3_3_3_4.pt. Now saving predictions.\n",
      "[INFO] 08:43:47 Processed and deleted subgraph_3_3_3_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:43:48 Starting subgraph_3_3_3_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:10<01:55,  1.98s/it, 64.109352]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:44:58 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:49<00:00,  1.13s/it, 63.306313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:46:37 Trained model on subgraph_3_3_3_5.pt. Now saving predictions.\n",
      "[INFO] 08:46:37 Processed and deleted subgraph_3_3_3_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:46:37 Starting subgraph_3_3_3_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:10<01:43,  1.86s/it, 112.748085]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:47:48 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 111.459648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:49:23 Trained model on subgraph_3_3_3_6.pt. Now saving predictions.\n",
      "[INFO] 08:49:23 Processed and deleted subgraph_3_3_3_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:49:23 Starting subgraph_3_3_3_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:07<01:43,  1.82s/it, 109.607124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:50:33 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 107.975471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:52:10 Trained model on subgraph_3_3_3_7.pt. Now saving predictions.\n",
      "[INFO] 08:52:10 Processed and deleted subgraph_3_3_3_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:52:10 Starting subgraph_3_3_3_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:09<01:43,  1.84s/it, 62.619595]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:53:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 61.479210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:54:55 Trained model on subgraph_3_3_3_8.pt. Now saving predictions.\n",
      "[INFO] 08:54:55 Processed and deleted subgraph_3_3_3_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:54:55 Starting subgraph_3_3_3_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:09<01:45,  1.82s/it, 58.179085]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:56:05 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.13s/it, 56.920830]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:57:44 Trained model on subgraph_3_3_3_9.pt. Now saving predictions.\n",
      "[INFO] 08:57:44 Processed and deleted subgraph_3_3_3_9.pt\n",
      "[INFO] 08:57:44 Starting subgraph_3_3_4_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<01:44,  1.81s/it, 51.813976]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 08:58:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 3 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.12s/it, 47.268963]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:00:32 Trained model on subgraph_3_3_4_0.pt. Now saving predictions.\n",
      "[INFO] 09:00:32 Processed and deleted subgraph_3_3_4_0.pt\n",
      "[INFO] 09:00:32 Starting subgraph_3_3_4_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:10<01:45,  1.91s/it, 75.503250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:01:42 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.09s/it, 74.412308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:03:16 Trained model on subgraph_3_3_4_1.pt. Now saving predictions.\n",
      "[INFO] 09:03:16 Processed and deleted subgraph_3_3_4_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:03:16 Starting subgraph_3_3_4_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:06<01:46,  1.84s/it, 60.008709] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:04:24 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 57.549278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:06:02 Trained model on subgraph_3_3_4_10.pt. Now saving predictions.\n",
      "[INFO] 09:06:02 Processed and deleted subgraph_3_3_4_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:06:02 Starting subgraph_3_3_4_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:11<01:43,  1.85s/it, 55.056335]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:07:14 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 54.439953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:08:49 Trained model on subgraph_3_3_4_11.pt. Now saving predictions.\n",
      "[INFO] 09:08:49 Processed and deleted subgraph_3_3_4_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:08:50 Starting subgraph_3_3_4_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:45,  1.86s/it, 72.598328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:09:59 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.11s/it, 71.661552]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:11:35 Trained model on subgraph_3_3_4_12.pt. Now saving predictions.\n",
      "[INFO] 09:11:36 Processed and deleted subgraph_3_3_4_12.pt\n",
      "[INFO] 09:11:36 Starting subgraph_3_3_4_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:09<01:40,  1.83s/it, 65.677391] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:12:46 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:42<00:00,  1.09s/it, 64.712776]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:14:18 Trained model on subgraph_3_3_4_2.pt. Now saving predictions.\n",
      "[INFO] 09:14:19 Processed and deleted subgraph_3_3_4_2.pt\n",
      "[INFO] 09:14:19 Starting subgraph_3_3_4_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:43,  1.82s/it, 57.466347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:15:27 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 56.857082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:17:04 Trained model on subgraph_3_3_4_3.pt. Now saving predictions.\n",
      "[INFO] 09:17:04 Processed and deleted subgraph_3_3_4_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:17:04 Starting subgraph_3_3_4_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:09<01:59,  2.06s/it, 75.644585]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:18:14 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:50<00:00,  1.14s/it, 74.847725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:19:55 Trained model on subgraph_3_3_4_4.pt. Now saving predictions.\n",
      "[INFO] 09:19:55 Processed and deleted subgraph_3_3_4_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:19:55 Starting subgraph_3_3_4_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████    | 91/150 [01:10<01:48,  1.83s/it, 75.417931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:21:07 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:51<00:00,  1.14s/it, 74.673393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:22:46 Trained model on subgraph_3_3_4_5.pt. Now saving predictions.\n",
      "[INFO] 09:22:47 Processed and deleted subgraph_3_3_4_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:22:47 Starting subgraph_3_3_4_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:43,  1.82s/it, 76.643555]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:23:56 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 75.883842]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:25:32 Trained model on subgraph_3_3_4_6.pt. Now saving predictions.\n",
      "[INFO] 09:25:33 Processed and deleted subgraph_3_3_4_6.pt\n",
      "[INFO] 09:25:33 Starting subgraph_3_3_4_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:09<01:45,  1.89s/it, 72.496284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:26:43 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 71.190681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:28:18 Trained model on subgraph_3_3_4_7.pt. Now saving predictions.\n",
      "[INFO] 09:28:18 Processed and deleted subgraph_3_3_4_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:28:18 Starting subgraph_3_3_4_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<01:45,  1.81s/it, 56.932259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:29:27 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.12s/it, 55.750919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:31:05 Trained model on subgraph_3_3_4_8.pt. Now saving predictions.\n",
      "[INFO] 09:31:05 Processed and deleted subgraph_3_3_4_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:31:05 Starting subgraph_3_3_4_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:08<01:41,  1.82s/it, 56.371922]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:32:15 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.09s/it, 55.520535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:33:49 Trained model on subgraph_3_3_4_9.pt. Now saving predictions.\n",
      "[INFO] 09:33:50 Processed and deleted subgraph_3_3_4_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:33:50 Starting subgraph_3_3_5_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:10<01:46,  1.86s/it, 53.339008] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:35:00 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 4 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 52.291550]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:36:36 Trained model on subgraph_3_3_5_0.pt. Now saving predictions.\n",
      "[INFO] 09:36:36 Processed and deleted subgraph_3_3_5_0.pt\n",
      "[INFO] 09:36:36 Starting subgraph_3_3_5_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:08<01:42,  1.84s/it, 68.104355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:37:46 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:43<00:00,  1.09s/it, 67.108589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:39:20 Trained model on subgraph_3_3_5_1.pt. Now saving predictions.\n",
      "[INFO] 09:39:21 Processed and deleted subgraph_3_3_5_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:39:21 Starting subgraph_3_3_5_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:44,  1.84s/it, 79.147713] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:40:30 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:54<00:00,  1.16s/it, 75.168839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:42:15 Trained model on subgraph_3_3_5_10.pt. Now saving predictions.\n",
      "[INFO] 09:42:15 Processed and deleted subgraph_3_3_5_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:42:15 Starting subgraph_3_3_5_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:07<01:44,  1.83s/it, 57.216053]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:43:24 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 56.206001]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:45:00 Trained model on subgraph_3_3_5_11.pt. Now saving predictions.\n",
      "[INFO] 09:45:00 Processed and deleted subgraph_3_3_5_11.pt\n",
      "[INFO] 09:45:00 Starting subgraph_3_3_5_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:08<01:42,  1.86s/it, 65.826790]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:46:10 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:42<00:00,  1.08s/it, 64.793304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:47:43 Trained model on subgraph_3_3_5_12.pt. Now saving predictions.\n",
      "[INFO] 09:47:43 Processed and deleted subgraph_3_3_5_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:47:43 Starting subgraph_3_3_5_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████    | 91/150 [01:08<01:48,  1.84s/it, 62.032677] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:48:52 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:49<00:00,  1.13s/it, 60.082317]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:50:32 Trained model on subgraph_3_3_5_2.pt. Now saving predictions.\n",
      "[INFO] 09:50:32 Processed and deleted subgraph_3_3_5_2.pt\n",
      "[INFO] 09:50:32 Starting subgraph_3_3_5_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:45,  1.85s/it, 58.984814] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:51:42 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.11s/it, 57.814056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:53:18 Trained model on subgraph_3_3_5_3.pt. Now saving predictions.\n",
      "[INFO] 09:53:18 Processed and deleted subgraph_3_3_5_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:53:18 Starting subgraph_3_3_5_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<01:50,  1.90s/it, 65.148628]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:54:28 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.13s/it, 64.477859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:56:07 Trained model on subgraph_3_3_5_4.pt. Now saving predictions.\n",
      "[INFO] 09:56:08 Processed and deleted subgraph_3_3_5_4.pt\n",
      "[INFO] 09:56:08 Starting subgraph_3_3_5_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n",
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:11<01:46,  1.87s/it, 90.227882]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:57:19 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.12s/it, 89.359055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:58:56 Trained model on subgraph_3_3_5_5.pt. Now saving predictions.\n",
      "[INFO] 09:58:56 Processed and deleted subgraph_3_3_5_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 09:58:56 Starting subgraph_3_3_5_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:08<01:52,  1.94s/it, 80.357231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:00:04 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 79.638687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:01:43 Trained model on subgraph_3_3_5_6.pt. Now saving predictions.\n",
      "[INFO] 10:01:43 Processed and deleted subgraph_3_3_5_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:01:43 Starting subgraph_3_3_5_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:08<01:44,  1.87s/it, 76.024490]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:02:52 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 74.211464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:04:29 Trained model on subgraph_3_3_5_7.pt. Now saving predictions.\n",
      "[INFO] 10:04:29 Processed and deleted subgraph_3_3_5_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:04:29 Starting subgraph_3_3_5_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:10<01:44,  1.84s/it, 72.029655]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:05:41 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.12s/it, 71.161171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:07:17 Trained model on subgraph_3_3_5_8.pt. Now saving predictions.\n",
      "[INFO] 10:07:17 Processed and deleted subgraph_3_3_5_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\3173200017.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:07:17 Starting subgraph_3_3_5_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:05<01:50,  1.91s/it, 78.359856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:08:25 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 75.918716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:10:02 Trained model on subgraph_3_3_5_9.pt. Now saving predictions.\n",
      "[INFO] 10:10:02 Processed and deleted subgraph_3_3_5_9.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 10:11:03 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 5 9\n",
      "[INFO] 12:22:22 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 0\n",
      "[INFO] 12:25:07 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 1\n"
     ]
    }
   ],
   "source": [
    "HandleRegion(3, 3, 2, indices[3][3], updatedSubregions[3][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:16:28 Dataframe Read. Size = 553.427891 MB\n",
      "[INFO] 12:16:28 Will generate 78 subgraphs (6 rows x 13 columns)\n",
      "[INFO] 12:16:28 Generating subgraph 0 0 for Region 3 3\n",
      "[INFO] 12:16:28 Started processing subregion 3 3, subgraph id: 0 0\n",
      "[INFO] 12:16:28 Generating subgraph 1 0 for Region 3 3\n",
      "[INFO] 12:16:28 Started processing subregion 3 3, subgraph id: 1 0\n",
      "[INFO] 12:16:28 Generating subgraph 2 0 for Region 3 3\n",
      "[INFO] 12:16:28 Started processing subregion 3 3, subgraph id: 2 0\n",
      "[INFO] 12:16:28 Generating subgraph 3 0 for Region 3 3\n",
      "[INFO] 12:16:28 Started processing subregion 3 3, subgraph id: 3 0\n",
      "[INFO] 12:16:28 Generating subgraph 4 0 for Region 3 3\n",
      "[INFO] 12:16:28 Started processing subregion 3 3, subgraph id: 4 0\n",
      "[INFO] 12:16:28 Generating subgraph 5 0 for Region 3 3\n",
      "[INFO] 12:16:28 Started processing subregion 3 3, subgraph id: 5 0\n",
      "[INFO] 12:16:28 Loaded mask for subgraph : 0, 0\n",
      "[INFO] 12:16:28 Loaded mask for subgraph : 2, 0\n",
      "[INFO] 12:16:28 Loaded mask for subgraph : 5, 0\n",
      "[INFO] 12:16:28 Loaded mask for subgraph : 3, 0\n",
      "[INFO] 12:16:28 Loaded mask for subgraph : 1, 0\n",
      "[INFO] 12:16:28 Loaded mask for subgraph : 4, 0\n",
      "[INFO] 12:16:34 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_0.pt\n",
      "[INFO] 12:16:34 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_0.pt\n",
      "[INFO] 12:16:34 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_0.pt\n",
      "[INFO] 12:16:34 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_0.pt\n",
      "[INFO] 12:16:34 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_0.pt\n",
      "[INFO] 12:16:34 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_0.pt\n",
      "[INFO] 12:16:34 Generating subgraph 0 1 for Region 3 3\n",
      "[INFO] 12:16:34 Generating subgraph 1 1 for Region 3 3\n",
      "[INFO] 12:16:34 Generating subgraph 2 1 for Region 3 3\n",
      "[INFO] 12:16:34 Generating subgraph 3 1 for Region 3 3\n",
      "[INFO] 12:16:34 Generating subgraph 4 1 for Region 3 3\n",
      "[INFO] 12:16:34 Generating subgraph 5 1 for Region 3 3\n",
      "[INFO] 12:16:34 Started processing subregion 3 3, subgraph id: 0 1\n",
      "[INFO] 12:16:34 Started processing subregion 3 3, subgraph id: 1 1\n",
      "[INFO] 12:16:34 Started processing subregion 3 3, subgraph id: 2 1\n",
      "[INFO] 12:16:34 Started processing subregion 3 3, subgraph id: 3 1\n",
      "[INFO] 12:16:34 Started processing subregion 3 3, subgraph id: 4 1\n",
      "[INFO] 12:16:34 Started processing subregion 3 3, subgraph id: 5 1\n",
      "[INFO] 12:16:34 Loaded mask for subgraph : 1, 1\n",
      "[INFO] 12:16:34 Loaded mask for subgraph : 3, 1\n",
      "[INFO] 12:16:34 Loaded mask for subgraph : 0, 1\n",
      "[INFO] 12:16:34 Loaded mask for subgraph : 5, 1\n",
      "[INFO] 12:16:34 Loaded mask for subgraph : 2, 1\n",
      "[INFO] 12:16:34 Loaded mask for subgraph : 4, 1\n",
      "[INFO] 12:16:39 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_1.pt\n",
      "[INFO] 12:16:39 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_1.pt\n",
      "[INFO] 12:16:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_1.pt\n",
      "[INFO] 12:16:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_1.pt\n",
      "[INFO] 12:16:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_1.pt\n",
      "[INFO] 12:16:40 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_1.pt\n",
      "[INFO] 12:16:40 Generating subgraph 0 2 for Region 3 3\n",
      "[INFO] 12:16:40 Generating subgraph 1 2 for Region 3 3\n",
      "[INFO] 12:16:40 Generating subgraph 2 2 for Region 3 3\n",
      "[INFO] 12:16:40 Generating subgraph 3 2 for Region 3 3\n",
      "[INFO] 12:16:40 Generating subgraph 4 2 for Region 3 3\n",
      "[INFO] 12:16:40 Generating subgraph 5 2 for Region 3 3\n",
      "[INFO] 12:16:40 Started processing subregion 3 3, subgraph id: 0 2\n",
      "[INFO] 12:16:40 Started processing subregion 3 3, subgraph id: 1 2\n",
      "[INFO] 12:16:40 Started processing subregion 3 3, subgraph id: 2 2\n",
      "[INFO] 12:16:40 Started processing subregion 3 3, subgraph id: 3 2\n",
      "[INFO] 12:16:40 Started processing subregion 3 3, subgraph id: 4 2\n",
      "[INFO] 12:16:40 Started processing subregion 3 3, subgraph id: 5 2\n",
      "[INFO] 12:16:40 Loaded mask for subgraph : 1, 2\n",
      "[INFO] 12:16:40 Loaded mask for subgraph : 2, 2\n",
      "[INFO] 12:16:40 Loaded mask for subgraph : 0, 2\n",
      "[INFO] 12:16:40 Loaded mask for subgraph : 4, 2\n",
      "[INFO] 12:16:40 Loaded mask for subgraph : 3, 2\n",
      "[INFO] 12:16:40 Loaded mask for subgraph : 5, 2\n",
      "[INFO] 12:16:43 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_2.pt\n",
      "[INFO] 12:16:45 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_2.pt\n",
      "[INFO] 12:16:45 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_2.pt\n",
      "[INFO] 12:16:45 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_2.pt\n",
      "[INFO] 12:16:45 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_2.pt\n",
      "[INFO] 12:16:45 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_2.pt\n",
      "[INFO] 12:16:45 Generating subgraph 0 3 for Region 3 3\n",
      "[INFO] 12:16:45 Generating subgraph 1 3 for Region 3 3\n",
      "[INFO] 12:16:45 Generating subgraph 2 3 for Region 3 3\n",
      "[INFO] 12:16:45 Generating subgraph 3 3 for Region 3 3\n",
      "[INFO] 12:16:45 Generating subgraph 4 3 for Region 3 3\n",
      "[INFO] 12:16:45 Generating subgraph 5 3 for Region 3 3\n",
      "[INFO] 12:16:45 Started processing subregion 3 3, subgraph id: 0 3\n",
      "[INFO] 12:16:45 Started processing subregion 3 3, subgraph id: 1 3\n",
      "[INFO] 12:16:45 Started processing subregion 3 3, subgraph id: 2 3\n",
      "[INFO] 12:16:45 Started processing subregion 3 3, subgraph id: 3 3\n",
      "[INFO] 12:16:45 Started processing subregion 3 3, subgraph id: 4 3\n",
      "[INFO] 12:16:45 Started processing subregion 3 3, subgraph id: 5 3\n",
      "[INFO] 12:16:46 Loaded mask for subgraph : 3, 3\n",
      "[INFO] 12:16:46 Loaded mask for subgraph : 1, 3\n",
      "[INFO] 12:16:46 Loaded mask for subgraph : 2, 3\n",
      "[INFO] 12:16:46 Loaded mask for subgraph : 4, 3\n",
      "[INFO] 12:16:46 Loaded mask for subgraph : 5, 3\n",
      "[INFO] 12:16:46 Loaded mask for subgraph : 0, 3\n",
      "[INFO] 12:16:50 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_3.pt\n",
      "[INFO] 12:16:50 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_3.pt\n",
      "[INFO] 12:16:50 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_3.pt\n",
      "[INFO] 12:16:51 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_3.pt[INFO] 12:16:51 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_3.pt\n",
      "\n",
      "[INFO] 12:16:51 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_3.pt\n",
      "[INFO] 12:16:51 Generating subgraph 0 4 for Region 3 3\n",
      "[INFO] 12:16:51 Generating subgraph 1 4 for Region 3 3\n",
      "[INFO] 12:16:51 Generating subgraph 2 4 for Region 3 3\n",
      "[INFO] 12:16:51 Generating subgraph 3 4 for Region 3 3\n",
      "[INFO] 12:16:51 Generating subgraph 4 4 for Region 3 3\n",
      "[INFO] 12:16:51 Generating subgraph 5 4 for Region 3 3\n",
      "[INFO] 12:16:51 Started processing subregion 3 3, subgraph id: 0 4\n",
      "[INFO] 12:16:51 Started processing subregion 3 3, subgraph id: 1 4\n",
      "[INFO] 12:16:51 Started processing subregion 3 3, subgraph id: 2 4\n",
      "[INFO] 12:16:51 Started processing subregion 3 3, subgraph id: 3 4\n",
      "[INFO] 12:16:51 Started processing subregion 3 3, subgraph id: 4 4\n",
      "[INFO] 12:16:51 Started processing subregion 3 3, subgraph id: 5 4\n",
      "[INFO] 12:16:51 Loaded mask for subgraph : 4, 4\n",
      "[INFO] 12:16:51 Loaded mask for subgraph : 2, 4\n",
      "[INFO] 12:16:51 Loaded mask for subgraph : 0, 4\n",
      "[INFO] 12:16:51 Loaded mask for subgraph : 5, 4\n",
      "[INFO] 12:16:51 Loaded mask for subgraph : 3, 4\n",
      "[INFO] 12:16:51 Loaded mask for subgraph : 1, 4\n",
      "[INFO] 12:16:56 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_4.pt\n",
      "[INFO] 12:16:56 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_4.pt\n",
      "[INFO] 12:16:56 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_4.pt\n",
      "[INFO] 12:16:56 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_4.pt\n",
      "[INFO] 12:16:56 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_4.pt\n",
      "[INFO] 12:16:56 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_4.pt\n",
      "[INFO] 12:16:56 Generating subgraph 0 5 for Region 3 3\n",
      "[INFO] 12:16:56 Generating subgraph 1 5 for Region 3 3\n",
      "[INFO] 12:16:56 Generating subgraph 2 5 for Region 3 3\n",
      "[INFO] 12:16:56 Generating subgraph 3 5 for Region 3 3\n",
      "[INFO] 12:16:56 Generating subgraph 4 5 for Region 3 3\n",
      "[INFO] 12:16:56 Generating subgraph 5 5 for Region 3 3\n",
      "[INFO] 12:16:56 Started processing subregion 3 3, subgraph id: 0 5\n",
      "[INFO] 12:16:56 Started processing subregion 3 3, subgraph id: 1 5\n",
      "[INFO] 12:16:56 Started processing subregion 3 3, subgraph id: 2 5\n",
      "[INFO] 12:16:56 Started processing subregion 3 3, subgraph id: 3 5\n",
      "[INFO] 12:16:56 Started processing subregion 3 3, subgraph id: 4 5\n",
      "[INFO] 12:16:56 Started processing subregion 3 3, subgraph id: 5 5\n",
      "[INFO] 12:16:56 Loaded mask for subgraph : 1, 5\n",
      "[INFO] 12:16:56 Loaded mask for subgraph : 3, 5\n",
      "[INFO] 12:16:56 Loaded mask for subgraph : 2, 5\n",
      "[INFO] 12:16:56 Loaded mask for subgraph : 4, 5\n",
      "[INFO] 12:16:56 Loaded mask for subgraph : 0, 5\n",
      "[INFO] 12:16:56 Loaded mask for subgraph : 5, 5\n",
      "[INFO] 12:17:00 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_5.pt\n",
      "[INFO] 12:17:00 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_5.pt\n",
      "[INFO] 12:17:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_5.pt\n",
      "[INFO] 12:17:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_5.pt\n",
      "[INFO] 12:17:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_5.pt\n",
      "[INFO] 12:17:01 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_5.pt\n",
      "[INFO] 12:17:01 Generating subgraph 0 6 for Region 3 3\n",
      "[INFO] 12:17:01 Generating subgraph 1 6 for Region 3 3\n",
      "[INFO] 12:17:01 Generating subgraph 2 6 for Region 3 3\n",
      "[INFO] 12:17:01 Generating subgraph 3 6 for Region 3 3\n",
      "[INFO] 12:17:01 Generating subgraph 4 6 for Region 3 3\n",
      "[INFO] 12:17:01 Generating subgraph 5 6 for Region 3 3\n",
      "[INFO] 12:17:01 Started processing subregion 3 3, subgraph id: 0 6\n",
      "[INFO] 12:17:01 Started processing subregion 3 3, subgraph id: 1 6\n",
      "[INFO] 12:17:01 Started processing subregion 3 3, subgraph id: 2 6\n",
      "[INFO] 12:17:01 Started processing subregion 3 3, subgraph id: 3 6\n",
      "[INFO] 12:17:01 Started processing subregion 3 3, subgraph id: 4 6\n",
      "[INFO] 12:17:01 Started processing subregion 3 3, subgraph id: 5 6\n",
      "[INFO] 12:17:01 Loaded mask for subgraph : 0, 6\n",
      "[INFO] 12:17:01 Loaded mask for subgraph : 5, 6\n",
      "[INFO] 12:17:01 Loaded mask for subgraph : 1, 6\n",
      "[INFO] 12:17:01 Loaded mask for subgraph : 2, 6\n",
      "[INFO] 12:17:01 Loaded mask for subgraph : 3, 6\n",
      "[INFO] 12:17:01 Loaded mask for subgraph : 4, 6\n",
      "[INFO] 12:17:05 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_6.pt\n",
      "[INFO] 12:17:05 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_6.pt\n",
      "[INFO] 12:17:05 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_6.pt\n",
      "[INFO] 12:17:05 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_6.pt\n",
      "[INFO] 12:17:06 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_6.pt\n",
      "[INFO] 12:17:06 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_6.pt\n",
      "[INFO] 12:17:06 Generating subgraph 0 7 for Region 3 3\n",
      "[INFO] 12:17:06 Generating subgraph 1 7 for Region 3 3\n",
      "[INFO] 12:17:06 Generating subgraph 2 7 for Region 3 3\n",
      "[INFO] 12:17:06 Generating subgraph 3 7 for Region 3 3\n",
      "[INFO] 12:17:06 Generating subgraph 4 7 for Region 3 3\n",
      "[INFO] 12:17:06 Generating subgraph 5 7 for Region 3 3\n",
      "[INFO] 12:17:06 Started processing subregion 3 3, subgraph id: 0 7\n",
      "[INFO] 12:17:06 Started processing subregion 3 3, subgraph id: 1 7\n",
      "[INFO] 12:17:06 Started processing subregion 3 3, subgraph id: 2 7\n",
      "[INFO] 12:17:06 Started processing subregion 3 3, subgraph id: 3 7\n",
      "[INFO] 12:17:06 Started processing subregion 3 3, subgraph id: 4 7\n",
      "[INFO] 12:17:06 Started processing subregion 3 3, subgraph id: 5 7\n",
      "[INFO] 12:17:06 Loaded mask for subgraph : 2, 7\n",
      "[INFO] 12:17:06 Loaded mask for subgraph : 0, 7\n",
      "[INFO] 12:17:06 Loaded mask for subgraph : 5, 7\n",
      "[INFO] 12:17:06 Loaded mask for subgraph : 3, 7\n",
      "[INFO] 12:17:06 Loaded mask for subgraph : 4, 7\n",
      "[INFO] 12:17:06 Loaded mask for subgraph : 1, 7\n",
      "[INFO] 12:17:10 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_7.pt[INFO] 12:17:10 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_7.pt\n",
      "\n",
      "[INFO] 12:17:11 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_7.pt\n",
      "[INFO] 12:17:11 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_7.pt\n",
      "[INFO] 12:17:11 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_7.pt\n",
      "[INFO] 12:17:11 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_7.pt\n",
      "[INFO] 12:17:11 Generating subgraph 0 8 for Region 3 3\n",
      "[INFO] 12:17:11 Generating subgraph 1 8 for Region 3 3\n",
      "[INFO] 12:17:11 Generating subgraph 2 8 for Region 3 3\n",
      "[INFO] 12:17:11 Generating subgraph 3 8 for Region 3 3\n",
      "[INFO] 12:17:11 Generating subgraph 4 8 for Region 3 3\n",
      "[INFO] 12:17:11 Generating subgraph 5 8 for Region 3 3\n",
      "[INFO] 12:17:11 Started processing subregion 3 3, subgraph id: 0 8\n",
      "[INFO] 12:17:11 Started processing subregion 3 3, subgraph id: 1 8\n",
      "[INFO] 12:17:11 Started processing subregion 3 3, subgraph id: 2 8\n",
      "[INFO] 12:17:11 Started processing subregion 3 3, subgraph id: 3 8\n",
      "[INFO] 12:17:11 Started processing subregion 3 3, subgraph id: 4 8\n",
      "[INFO] 12:17:11 Started processing subregion 3 3, subgraph id: 5 8\n",
      "[INFO] 12:17:11 Loaded mask for subgraph : 0, 8\n",
      "[INFO] 12:17:11 Loaded mask for subgraph : 1, 8\n",
      "[INFO] 12:17:11 Loaded mask for subgraph : 4, 8\n",
      "[INFO] 12:17:11 Loaded mask for subgraph : 2, 8\n",
      "[INFO] 12:17:11 Loaded mask for subgraph : 3, 8\n",
      "[INFO] 12:17:11 Loaded mask for subgraph : 5, 8\n",
      "[INFO] 12:17:15 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_8.pt\n",
      "[INFO] 12:17:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_8.pt\n",
      "[INFO] 12:17:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_8.pt\n",
      "[INFO] 12:17:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_8.pt\n",
      "[INFO] 12:17:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_8.pt\n",
      "[INFO] 12:17:16 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_8.pt\n",
      "[INFO] 12:17:16 Generating subgraph 0 9 for Region 3 3\n",
      "[INFO] 12:17:16 Generating subgraph 1 9 for Region 3 3\n",
      "[INFO] 12:17:16 Generating subgraph 2 9 for Region 3 3\n",
      "[INFO] 12:17:16 Generating subgraph 3 9 for Region 3 3\n",
      "[INFO] 12:17:16 Generating subgraph 4 9 for Region 3 3\n",
      "[INFO] 12:17:16 Generating subgraph 5 9 for Region 3 3\n",
      "[INFO] 12:17:16 Started processing subregion 3 3, subgraph id: 0 9\n",
      "[INFO] 12:17:16 Started processing subregion 3 3, subgraph id: 1 9\n",
      "[INFO] 12:17:16 Started processing subregion 3 3, subgraph id: 2 9\n",
      "[INFO] 12:17:16 Started processing subregion 3 3, subgraph id: 3 9\n",
      "[INFO] 12:17:16 Started processing subregion 3 3, subgraph id: 4 9\n",
      "[INFO] 12:17:16 Started processing subregion 3 3, subgraph id: 5 9\n",
      "[INFO] 12:17:16 Loaded mask for subgraph : 4, 9\n",
      "[INFO] 12:17:16 Loaded mask for subgraph : 1, 9\n",
      "[INFO] 12:17:16 Loaded mask for subgraph : 3, 9\n",
      "[INFO] 12:17:16 Loaded mask for subgraph : 2, 9\n",
      "[INFO] 12:17:16 Loaded mask for subgraph : 0, 9\n",
      "[INFO] 12:17:16 Loaded mask for subgraph : 5, 9\n",
      "[INFO] 12:17:20 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_9.pt\n",
      "[INFO] 12:17:21 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_9.pt\n",
      "[INFO] 12:17:21 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_9.pt\n",
      "[INFO] 12:17:21 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_9.pt\n",
      "[INFO] 12:17:21 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_9.pt\n",
      "[INFO] 12:17:21 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_9.pt\n",
      "[INFO] 12:17:21 Generating subgraph 0 10 for Region 3 3\n",
      "[INFO] 12:17:21 Generating subgraph 1 10 for Region 3 3\n",
      "[INFO] 12:17:21 Generating subgraph 2 10 for Region 3 3\n",
      "[INFO] 12:17:21 Generating subgraph 3 10 for Region 3 3\n",
      "[INFO] 12:17:21 Generating subgraph 4 10 for Region 3 3\n",
      "[INFO] 12:17:21 Generating subgraph 5 10 for Region 3 3\n",
      "[INFO] 12:17:21 Started processing subregion 3 3, subgraph id: 0 10\n",
      "[INFO] 12:17:21 Started processing subregion 3 3, subgraph id: 1 10\n",
      "[INFO] 12:17:21 Started processing subregion 3 3, subgraph id: 2 10\n",
      "[INFO] 12:17:21 Started processing subregion 3 3, subgraph id: 3 10\n",
      "[INFO] 12:17:21 Started processing subregion 3 3, subgraph id: 4 10\n",
      "[INFO] 12:17:21 Started processing subregion 3 3, subgraph id: 5 10\n",
      "[INFO] 12:17:21 Loaded mask for subgraph : 3, 10\n",
      "[INFO] 12:17:21 Loaded mask for subgraph : 4, 10\n",
      "[INFO] 12:17:21 Loaded mask for subgraph : 0, 10\n",
      "[INFO] 12:17:21 Loaded mask for subgraph : 1, 10\n",
      "[INFO] 12:17:21 Loaded mask for subgraph : 5, 10\n",
      "[INFO] 12:17:21 Loaded mask for subgraph : 2, 10\n",
      "[INFO] 12:17:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_10.pt\n",
      "[INFO] 12:17:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_10.pt\n",
      "[INFO] 12:17:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_10.pt\n",
      "[INFO] 12:17:25 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_10.pt\n",
      "[INFO] 12:17:26 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_10.pt\n",
      "[INFO] 12:17:26 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_10.pt\n",
      "[INFO] 12:17:26 Generating subgraph 0 11 for Region 3 3\n",
      "[INFO] 12:17:26 Generating subgraph 1 11 for Region 3 3\n",
      "[INFO] 12:17:26 Generating subgraph 2 11 for Region 3 3\n",
      "[INFO] 12:17:26 Generating subgraph 3 11 for Region 3 3\n",
      "[INFO] 12:17:26 Generating subgraph 4 11 for Region 3 3\n",
      "[INFO] 12:17:26 Generating subgraph 5 11 for Region 3 3\n",
      "[INFO] 12:17:26 Started processing subregion 3 3, subgraph id: 0 11\n",
      "[INFO] 12:17:26 Started processing subregion 3 3, subgraph id: 1 11\n",
      "[INFO] 12:17:26 Started processing subregion 3 3, subgraph id: 2 11\n",
      "[INFO] 12:17:26 Started processing subregion 3 3, subgraph id: 3 11\n",
      "[INFO] 12:17:26 Started processing subregion 3 3, subgraph id: 4 11\n",
      "[INFO] 12:17:26 Started processing subregion 3 3, subgraph id: 5 11\n",
      "[INFO] 12:17:26 Loaded mask for subgraph : 5, 11\n",
      "[INFO] 12:17:26 Loaded mask for subgraph : 3, 11\n",
      "[INFO] 12:17:26 Loaded mask for subgraph : 2, 11\n",
      "[INFO] 12:17:26 Loaded mask for subgraph : 1, 11\n",
      "[INFO] 12:17:26 Loaded mask for subgraph : 0, 11\n",
      "[INFO] 12:17:26 Loaded mask for subgraph : 4, 11\n",
      "[INFO] 12:17:29 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_11.pt\n",
      "[INFO] 12:17:30 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_11.pt\n",
      "[INFO] 12:17:30 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_11.pt\n",
      "[INFO] 12:17:30 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_11.pt[INFO] 12:17:30 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_11.pt\n",
      "\n",
      "[INFO] 12:17:30 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_11.pt\n",
      "[INFO] 12:17:30 Generating subgraph 0 12 for Region 3 3\n",
      "[INFO] 12:17:30 Generating subgraph 1 12 for Region 3 3\n",
      "[INFO] 12:17:30 Generating subgraph 2 12 for Region 3 3\n",
      "[INFO] 12:17:30 Generating subgraph 3 12 for Region 3 3\n",
      "[INFO] 12:17:30 Generating subgraph 4 12 for Region 3 3\n",
      "[INFO] 12:17:30 Generating subgraph 5 12 for Region 3 3\n",
      "[INFO] 12:17:30 Started processing subregion 3 3, subgraph id: 0 12\n",
      "[INFO] 12:17:30 Started processing subregion 3 3, subgraph id: 1 12\n",
      "[INFO] 12:17:30 Started processing subregion 3 3, subgraph id: 2 12\n",
      "[INFO] 12:17:30 Started processing subregion 3 3, subgraph id: 3 12\n",
      "[INFO] 12:17:30 Started processing subregion 3 3, subgraph id: 4 12\n",
      "[INFO] 12:17:30 Started processing subregion 3 3, subgraph id: 5 12\n",
      "[INFO] 12:17:31 Loaded mask for subgraph : 4, 12\n",
      "[INFO] 12:17:31 Loaded mask for subgraph : 5, 12\n",
      "[INFO] 12:17:31 Loaded mask for subgraph : 0, 12\n",
      "[INFO] 12:17:31 Loaded mask for subgraph : 2, 12\n",
      "[INFO] 12:17:31 Loaded mask for subgraph : 3, 12\n",
      "[INFO] 12:17:31 Loaded mask for subgraph : 1, 12\n",
      "[INFO] 12:17:35 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_1_12.pt\n",
      "[INFO] 12:17:35 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_4_12.pt\n",
      "[INFO] 12:17:35 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_3_12.pt\n",
      "[INFO] 12:17:35 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_5_12.pt\n",
      "[INFO] 12:17:35 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_2_12.pt\n",
      "[INFO] 12:17:35 Saved subgraph to ./graphs/graphs_subregion_3_3\\subgraph_3_3_0_12.pt\n"
     ]
    }
   ],
   "source": [
    "# Running for 1 more iteration\n",
    "Part3(3, 3, 3, indices[3][3], updatedSubregions[3][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"./drive/MyDrive/ISRO_SuperResolution/models/{x}_{y}.pth\"))\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:25:58 Starting subgraph_3_3_0_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:27<00:00,  1.02it/s, 37.128582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:28:25 Trained model on subgraph_3_3_0_10.pt. Now saving predictions.\n",
      "[INFO] 12:28:26 Processed and deleted subgraph_3_3_0_10.pt\n",
      "[INFO] 12:28:26 Starting subgraph_3_3_0_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:45,  1.84s/it, 81.108696]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:29:35 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 80.336746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:31:12 Trained model on subgraph_3_3_0_11.pt. Now saving predictions.\n",
      "[INFO] 12:31:12 Processed and deleted subgraph_3_3_0_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:31:12 Starting subgraph_3_3_0_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:08<01:43,  1.88s/it, 92.557259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:32:21 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:42<00:00,  1.08s/it, 91.948456]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:33:55 Trained model on subgraph_3_3_0_12.pt. Now saving predictions.\n",
      "[INFO] 12:33:55 Processed and deleted subgraph_3_3_0_12.pt\n",
      "[INFO] 12:33:55 Starting subgraph_3_3_0_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:44,  1.84s/it, 18.433575] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:35:03 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 17.400835]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:36:40 Trained model on subgraph_3_3_0_2.pt. Now saving predictions.\n",
      "[INFO] 12:36:40 Processed and deleted subgraph_3_3_0_2.pt\n",
      "[INFO] 12:36:40 Starting subgraph_3_3_0_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:12<01:42,  1.86s/it, 24.430296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:37:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 22.822405]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:39:26 Trained model on subgraph_3_3_0_3.pt. Now saving predictions.\n",
      "[INFO] 12:39:26 Processed and deleted subgraph_3_3_0_3.pt\n",
      "[INFO] 12:39:26 Starting subgraph_3_3_0_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:07<01:45,  1.88s/it, 27.148699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:40:35 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:43<00:00,  1.09s/it, 26.603464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:42:10 Trained model on subgraph_3_3_0_4.pt. Now saving predictions.\n",
      "[INFO] 12:42:10 Processed and deleted subgraph_3_3_0_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:42:10 Starting subgraph_3_3_0_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:07<01:45,  1.86s/it, 47.619694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:43:18 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 46.734329]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:44:54 Trained model on subgraph_3_3_0_5.pt. Now saving predictions.\n",
      "[INFO] 12:44:54 Processed and deleted subgraph_3_3_0_5.pt\n",
      "[INFO] 12:44:54 Starting subgraph_3_3_0_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:07<01:47,  1.85s/it, 44.273678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:46:04 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 43.605671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:47:41 Trained model on subgraph_3_3_0_6.pt. Now saving predictions.\n",
      "[INFO] 12:47:41 Processed and deleted subgraph_3_3_0_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:47:41 Starting subgraph_3_3_0_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:10<01:41,  1.84s/it, 37.487701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:48:52 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:43<00:00,  1.09s/it, 36.876854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:50:25 Trained model on subgraph_3_3_0_7.pt. Now saving predictions.\n",
      "[INFO] 12:50:25 Processed and deleted subgraph_3_3_0_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:50:25 Starting subgraph_3_3_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:10<01:47,  1.91s/it, 32.034473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:51:37 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.11s/it, 31.368050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:53:11 Trained model on subgraph_3_3_0_8.pt. Now saving predictions.\n",
      "[INFO] 12:53:11 Processed and deleted subgraph_3_3_0_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:53:11 Starting subgraph_3_3_0_9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████    | 91/150 [01:08<01:46,  1.80s/it, 33.008125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:54:21 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.13s/it, 32.559597]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:56:00 Trained model on subgraph_3_3_0_9.pt. Now saving predictions.\n",
      "[INFO] 12:56:01 Processed and deleted subgraph_3_3_0_9.pt\n",
      "[INFO] 12:56:01 Starting subgraph_3_3_1_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:13<01:51,  1.98s/it, 28.561478] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:57:15 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 0 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:49<00:00,  1.13s/it, 27.969887]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12:58:51 Trained model on subgraph_3_3_1_0.pt. Now saving predictions.\n",
      "[INFO] 12:58:51 Processed and deleted subgraph_3_3_1_0.pt\n",
      "[INFO] 12:58:51 Starting subgraph_3_3_1_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:08<01:45,  1.86s/it, 25.924166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:00:01 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:45<00:00,  1.10s/it, 25.136372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:01:36 Trained model on subgraph_3_3_1_1.pt. Now saving predictions.\n",
      "[INFO] 13:01:36 Processed and deleted subgraph_3_3_1_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:01:36 Starting subgraph_3_3_1_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:46,  1.87s/it, 40.122513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:02:46 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:47<00:00,  1.11s/it, 39.091049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:04:23 Trained model on subgraph_3_3_1_10.pt. Now saving predictions.\n",
      "[INFO] 13:04:23 Processed and deleted subgraph_3_3_1_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:04:23 Starting subgraph_3_3_1_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  61%|██████▏   | 92/150 [01:10<01:49,  1.88s/it, 61.889923]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:05:35 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:49<00:00,  1.13s/it, 61.269753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:07:13 Trained model on subgraph_3_3_1_11.pt. Now saving predictions.\n",
      "[INFO] 13:07:13 Processed and deleted subgraph_3_3_1_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:07:13 Starting subgraph_3_3_1_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:06<02:00,  2.12s/it, 79.762337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:08:20 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 75.478363]\n",
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:09:58 Trained model on subgraph_3_3_1_12.pt. Now saving predictions.\n",
      "[INFO] 13:09:58 Processed and deleted subgraph_3_3_1_12.pt\n",
      "[INFO] 13:09:58 Starting subgraph_3_3_1_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 95/150 [01:10<01:41,  1.85s/it, 26.679958] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:11:09 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:44<00:00,  1.10s/it, 25.969944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:12:43 Trained model on subgraph_3_3_1_2.pt. Now saving predictions.\n",
      "[INFO] 13:12:43 Processed and deleted subgraph_3_3_1_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:12:44 Starting subgraph_3_3_1_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  62%|██████▏   | 93/150 [01:09<01:45,  1.86s/it, 35.308735]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:13:53 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:46<00:00,  1.11s/it, 34.721294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:15:30 Trained model on subgraph_3_3_1_3.pt. Now saving predictions.\n",
      "[INFO] 13:15:30 Processed and deleted subgraph_3_3_1_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:15:30 Starting subgraph_3_3_1_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  63%|██████▎   | 94/150 [01:11<02:01,  2.16s/it, 33.135036]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:16:41 Saved predictions to ./regions/ISRO_RegionData23/subregion_3_3.csv for subregion 1 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt): 100%|██████████| 150/150 [02:48<00:00,  1.12s/it, 32.602276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:18:19 Trained model on subgraph_3_3_1_4.pt. Now saving predictions.\n",
      "[INFO] 13:18:19 Processed and deleted subgraph_3_3_1_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manas Kumar Sinha\\AppData\\Local\\Temp\\ipykernel_7544\\1446110109.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13:18:19 Starting subgraph_3_3_1_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... (mask +nt):  14%|█▍        | 21/150 [00:05<00:33,  3.82it/s, 38.656193]"
     ]
    }
   ],
   "source": [
    "Part4(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load as np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.load('Part2Output.npz')['indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "        list([]), list([])],\n",
       "       [list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "        list([]), list([])],\n",
       "       [list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "        list([]), list([])],\n",
       "       [list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "        list([]), list([])],\n",
       "       [list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "        list([]), list([])],\n",
       "       [list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "        list([]), list([])],\n",
       "       [list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "        list([]), list([])],\n",
       "       [list([]), list([]), list([]), list([]), list([]), list([]),\n",
       "        list([]), list([])]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Part2('combined_output_test.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ISRO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
